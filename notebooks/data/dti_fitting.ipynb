{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf89def7-ce9f-418a-b045-77713889d840",
   "metadata": {},
   "source": [
    "# PITN - DWI Downsampling and DTI Fitting\n",
    "\n",
    "Code by:\n",
    "\n",
    "Tyler Spears - tas6hh@virginia.edu\n",
    "\n",
    "Dr. Tom Fletcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cad9c0a-d02b-4423-a50d-81edc55a5752",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports & Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ed4188-1637-47d3-8f80-22fb20a11f60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automatically re-import project-specific modules.\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# imports\n",
    "import collections\n",
    "import functools\n",
    "import io\n",
    "import math\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "import inspect\n",
    "import random\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "\n",
    "import ants\n",
    "import dipy\n",
    "import dipy.core\n",
    "import dipy.reconst\n",
    "import dipy.reconst.dti\n",
    "import dipy.segment.mask\n",
    "import dipy.viz\n",
    "import dipy.viz.regtools\n",
    "import dotenv\n",
    "\n",
    "# visualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import mpl_toolkits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import IPython\n",
    "\n",
    "# Try importing GPUtil for printing GPU specs.\n",
    "# May not be installed if using CPU only.\n",
    "try:\n",
    "    import GPUtil\n",
    "except ImportError:\n",
    "    warnings.warn(\"WARNING: Package GPUtil not found, cannot print GPU specs\")\n",
    "\n",
    "# Data management libraries.\n",
    "import nibabel as nib\n",
    "import natsort\n",
    "from natsort import natsorted\n",
    "import box\n",
    "from box import Box\n",
    "import pprint\n",
    "from pprint import pprint as ppr\n",
    "\n",
    "# Computation & ML libraries.\n",
    "import numpy as np\n",
    "import skimage\n",
    "import torch\n",
    "import torchio\n",
    "\n",
    "plt.rcParams.update({\"figure.autolayout\": True})\n",
    "plt.rcParams.update({\"figure.facecolor\": [1.0, 1.0, 1.0, 1.0]})\n",
    "\n",
    "# Set print options for ndarrays/tensors.\n",
    "np.set_printoptions(suppress=True, edgeitems=2, threshold=100, linewidth=88)\n",
    "torch.set_printoptions(\n",
    "    sci_mode=False, edgeitems=2, threshold=100, linewidth=88, profile=\"short\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a613c5-e196-4994-8542-bcf173491dd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update notebook's environment variables with direnv.\n",
    "# This requires the python-dotenv package, and direnv be installed on the system\n",
    "# This will not work on Windows.\n",
    "# NOTE: This is kind of hacky, and not necessarily safe. Be careful...\n",
    "# Libraries needed on the python side:\n",
    "# - os\n",
    "# - subprocess\n",
    "# - io\n",
    "# - dotenv\n",
    "\n",
    "# Form command to be run in direnv's context. This command will print out\n",
    "# all environment variables defined in the subprocess/sub-shell.\n",
    "command = f\"direnv exec {os.getcwd()} /usr/bin/env\"\n",
    "# Run command in a new subprocess.\n",
    "proc = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, cwd=os.getcwd())\n",
    "# Store and format the subprocess' output.\n",
    "proc_out = proc.communicate()[0].strip().decode(\"utf-8\")\n",
    "# Use python-dotenv to load the environment variables by using the output of\n",
    "# 'direnv exec ...' as a 'dummy' .env file.\n",
    "dotenv.load_dotenv(stream=io.StringIO(proc_out), override=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04807445-e10d-42c2-a0a0-9afcba5d4e87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Project-specific scripts\n",
    "# It's easier to import it this way rather than make an entirely new package, due to\n",
    "# conflicts with local packages and anaconda installations.\n",
    "# You made me do this, poor python package management!!\n",
    "if \"PROJECT_ROOT\" in os.environ:\n",
    "    lib_location = str(Path(os.environ[\"PROJECT_ROOT\"]).resolve())\n",
    "else:\n",
    "    lib_location = str(Path(\"../../\").resolve())\n",
    "if lib_location not in sys.path:\n",
    "    sys.path.insert(0, lib_location)\n",
    "import lib as pitn\n",
    "\n",
    "# Include the top-level lib module along with its submodules.\n",
    "%aimport lib\n",
    "# Grab all submodules of lib, not including modules outside of the package.\n",
    "includes = list(\n",
    "    filter(\n",
    "        lambda m: m.startswith(\"lib.\"),\n",
    "        map(lambda x: x[1].__name__, inspect.getmembers(pitn, inspect.ismodule)),\n",
    "    )\n",
    ")\n",
    "# Run aimport magic with constructed includes.\n",
    "ipy = IPython.get_ipython()\n",
    "ipy.run_line_magic(\"aimport\", \", \".join(includes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc364e8-5395-4fe9-9e7b-c1843048b264",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch setup\n",
    "# allow for CUDA usage, if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# keep device as the cpu\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fbd0f7-64be-4e10-81c0-448b6b0f084f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Specs Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5db7a8-78c8-491e-897b-63e35a98f72d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr cap\n",
    "# Capture output and save to log. Needs to be at the *very first* line of the cell.\n",
    "# Watermark\n",
    "%load_ext watermark\n",
    "%watermark --author \"Tyler Spears\" --updated --iso8601  --python --machine --iversions --githash\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    # GPU information\n",
    "    # Taken from\n",
    "    # <https://www.thepythoncode.com/article/get-hardware-system-information-python>.\n",
    "    # If GPUtil is not installed, skip this step.\n",
    "    try:\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        print(\"=\" * 50, \"GPU Specs\", \"=\" * 50)\n",
    "        list_gpus = []\n",
    "        for gpu in gpus:\n",
    "            # get the GPU id\n",
    "            gpu_id = gpu.id\n",
    "            # name of GPU\n",
    "            gpu_name = gpu.name\n",
    "            driver_version = gpu.driver\n",
    "            cuda_version = torch.version.cuda\n",
    "            # get total memory\n",
    "            gpu_total_memory = f\"{gpu.memoryTotal}MB\"\n",
    "            gpu_uuid = gpu.uuid\n",
    "            list_gpus.append(\n",
    "                (\n",
    "                    gpu_id,\n",
    "                    gpu_name,\n",
    "                    driver_version,\n",
    "                    cuda_version,\n",
    "                    gpu_total_memory,\n",
    "                    gpu_uuid,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            tabulate(\n",
    "                list_gpus,\n",
    "                headers=(\n",
    "                    \"id\",\n",
    "                    \"Name\",\n",
    "                    \"Driver Version\",\n",
    "                    \"CUDA Version\",\n",
    "                    \"Total Memory\",\n",
    "                    \"uuid\",\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    except NameError:\n",
    "        print(\"CUDA Version: \", torch.version.cuda)\n",
    "\n",
    "else:\n",
    "    print(\"CUDA not in use, falling back to CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb5c303-b842-4b41-ad8f-943f499e2681",
   "metadata": {
    "tags": [
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Tyler Spears\n",
      "\n",
      "Last updated: 2021-10-10T22:24:19.957916+00:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.8\n",
      "IPython version      : 7.23.1\n",
      "\n",
      "Compiler    : GCC 7.3.0\n",
      "OS          : Linux\n",
      "Release     : 5.4.0-88-generic\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: a0881885f8686eed0a7e92bc43c4517870ae4a19\n",
      "\n",
      "sys       : 3.8.8 (default, Feb 24 2021, 21:46:12) \n",
      "[GCC 7.3.0]\n",
      "numpy     : 1.20.2\n",
      "dipy      : 1.4.1\n",
      "GPUtil    : 1.4.0\n",
      "natsort   : 7.1.1\n",
      "nibabel   : 3.2.1\n",
      "monai     : 0.7.dev2138\n",
      "IPython   : 7.23.1\n",
      "skimage   : 0.18.1\n",
      "box       : 5.4.1\n",
      "ants      : 0.2.7\n",
      "matplotlib: 3.4.1\n",
      "json      : 2.0.9\n",
      "torch     : 1.9.0\n",
      "seaborn   : 0.11.1\n",
      "torchio   : 0.18.37\n",
      "\n",
      "================================================== GPU Specs ==================================================\n",
      "  id  Name       Driver Version      CUDA Version  Total Memory    uuid\n",
      "----  ---------  ----------------  --------------  --------------  ----------------------------------------\n",
      "   0  TITAN RTX  460.91.03                   11.1  24217.0MB       GPU-586d2016-71ef-ebb3-437b-6721a22191ec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cap is defined in an ipython magic command\n",
    "print(cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb83001-449d-4078-8740-2f68b80d44fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Variables & Definitions Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede33a5-37f0-47aa-ad96-b3fedb76ad1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up directories\n",
    "data_dir = pathlib.Path(os.environ[\"DATA_DIR\"])\n",
    "processed_data_dir = pathlib.Path(os.environ[\"WRITE_DATA_DIR\"])\n",
    "hcp_source_data_dir = data_dir / \"hcp\"\n",
    "oasis_source_data_dir = data_dir / \"oasis3\"\n",
    "assert hcp_source_data_dir.exists() and oasis_source_data_dir.exists()\n",
    "\n",
    "hcp_processed_data_dir = processed_data_dir / \"hcp/derivatives/mean-downsample\"\n",
    "oasis_processed_data_dir = processed_data_dir / \"oasis3/derivatives/mean-downsample\"\n",
    "assert hcp_processed_data_dir.exists() and oasis_processed_data_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b19d3bb-ed0f-4967-a072-cc981927847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict to keep track of experiment configuration parameters. Will not be logged to\n",
    "# tensorboard.\n",
    "params = Box(default_box=True)\n",
    "# 6 channels for the 6 DTI components\n",
    "params.channels = 6\n",
    "\n",
    "# Voxel sizes for the mean downsampling.\n",
    "# This is really just the HCP space size.\n",
    "params.source_vox_size = 1.25\n",
    "# OASIS3 DWI spacing.\n",
    "params.target_vox_size = 2.0\n",
    "params.downsample_factor = params.target_vox_size / params.source_vox_size\n",
    "# Include all b-values for DTI fitting.\n",
    "params.bval_range = (0, 4000)\n",
    "params.dti_fit_method = \"WLS\"\n",
    "\n",
    "# Percentile range of DWIs that will have voxel intensities clamped.\n",
    "# In other words, for each subject, for each DWI, any voxel\n",
    "# values <= the first quantile value will be clamped to that quantile, and any voxels\n",
    "# >= the second quantile value will be clamped to that quantile.\n",
    "params.clamp_percentiles = (0.01, 99.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f570d014-38a8-4090-a106-33c80ac2fdfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the output directory based on the target voxel size.\n",
    "oasis_processed_data_dir /= \"scale-orig\"\n",
    "assert oasis_processed_data_dir.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f3e220-16fa-4c56-90a0-8a813fc859b6",
   "metadata": {},
   "source": [
    "## DTI Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a1340d-016c-41db-9c89-5ade59b72e90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define pipeline\n",
    "# Set up the transformation pipeline.\n",
    "\n",
    "# Initial import and selection of DWIs.\n",
    "dwi_processing_transforms = torchio.Compose(\n",
    "    [\n",
    "        torchio.transforms.ToCanonical(include=(\"dwi\", \"mask\"), copy=False),\n",
    "        pitn.transforms.BValSelectionTransform(\n",
    "            bval_range=params.bval_range,\n",
    "            bval_key=\"bvals\",\n",
    "            bvec_key=\"bvecs\",\n",
    "            include=\"dwi\",\n",
    "            copy=False,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Percentile clipping of values in DWIs to remove outliers assumed to be noise.\n",
    "intensity_clip_transform = pitn.transforms.ClipPercentileTransformd(\n",
    "    \"dwi\",\n",
    "    lower=params.clamp_percentiles[0],\n",
    "    upper=params.clamp_percentiles[1],\n",
    "    only_nonzero=True,\n",
    "    channel_wise=True,\n",
    ")\n",
    "\n",
    "# Final pipeline for downsampling the DWIs and fitting to DTIs.\n",
    "downsample_dti_fitting_transforms = torchio.Compose(\n",
    "    [\n",
    "        pitn.transforms.FractionalMeanDownsampleTransform(\n",
    "            source_vox_size=params.source_vox_size,\n",
    "            target_vox_size=params.target_vox_size,\n",
    "            include=(\"dwi\", \"mask\"),\n",
    "            keep={\"dwi\": \"fr_dwi\", \"mask\": \"fr_mask\"},\n",
    "            copy=False,\n",
    "        ),\n",
    "        pitn.transforms.RenameImageTransform(\n",
    "            {\"dwi\": \"lr_dwi\", \"mask\": \"lr_mask\"}, copy=False\n",
    "        ),\n",
    "        pitn.transforms.FitDTITransform(\n",
    "            \"bvals\",\n",
    "            \"bvecs\",\n",
    "            \"fr_mask\",\n",
    "            fit_method=params.dti_fit_method,\n",
    "            include=(\"fr_dwi\"),\n",
    "            #             cache_dir=\"./.cache\",\n",
    "            copy=False,\n",
    "        ),\n",
    "        pitn.transforms.FitDTITransform(\n",
    "            \"bvals\",\n",
    "            \"bvecs\",\n",
    "            \"lr_mask\",\n",
    "            fit_method=params.dti_fit_method,\n",
    "            include=(\"lr_dwi\"),\n",
    "            #             cache_dir=\"./.cache\",\n",
    "            copy=False,\n",
    "        ),\n",
    "        pitn.transforms.RenameImageTransform(\n",
    "            {\"fr_dwi\": \"fr_dti\", \"lr_dwi\": \"lr_dti\"}, copy=False\n",
    "        ),\n",
    "        pitn.transforms.ImageToDictTransform(include=(\"lr_dti\", \"lr_mask\"), copy=False),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c176dc-f5bf-4232-bdbc-0bd1587dd3a2",
   "metadata": {},
   "source": [
    "### HCP - DWI Downsampling & DTI Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85c6ba6-26c0-4bb6-8bd8-d3d746c5b435",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load and process DWIs, downsample DWIs, and fit DTIs to both full resolution and low-\n",
    "# resolution DTIs.\n",
    "hcp_subj_data: dict = dict()\n",
    "\n",
    "# HCP has error-corrected DWIs and a matching mask. So, grab the DWI and the mask\n",
    "# from the same source directory.\n",
    "source_dwi_dir = \"T1w/Diffusion\"\n",
    "source_dwi_filename = \"data.nii.gz\"\n",
    "source_bval_filename = \"bvals\"\n",
    "source_bvec_filename = \"bvecs\"\n",
    "source_mask_dir = source_dwi_dir\n",
    "mask_filename = \"nodif_brain_mask.nii.gz\"\n",
    "\n",
    "fr_output_dir = hcp_processed_data_dir / \"scale-orig\"\n",
    "lr_output_dir = hcp_processed_data_dir / f\"scale-{params.target_vox_size:.2f}mm\"\n",
    "\n",
    "# HCP subj directories are only numbers, no 'sub-' or code in front.\n",
    "for subj_dir in hcp_source_data_dir.glob(\"[0-9]*\"):\n",
    "    subj_id = subj_dir.name\n",
    "    # Output filenames, stored in a Box for convenience.\n",
    "    filenames = Box(default_box=True)\n",
    "    filenames.fr.dti = f\"sub-{subj_id}_scale-orig_dti.nii.gz\"\n",
    "    filenames.fr.mask = f\"sub-{subj_id}_scale-orig_mask.nii.gz\"\n",
    "    filenames.lr.dti = f\"sub-{subj_id}_scale-{params.target_vox_size:.2f}mm_dti.nii.gz\"\n",
    "    filenames.lr.mask = (\n",
    "        f\"sub-{subj_id}_scale-{params.target_vox_size:.2f}mm_mask.nii.gz\"\n",
    "    )\n",
    "    # Set the subject-specific data source directory.\n",
    "    subj_source_dir = subj_dir / source_dwi_dir\n",
    "\n",
    "    bvals = torch.as_tensor(\n",
    "        np.loadtxt(subj_source_dir / source_bval_filename).astype(int)\n",
    "    )\n",
    "    bvecs = torch.as_tensor(np.loadtxt(subj_source_dir / source_bvec_filename))\n",
    "    # Reshape to be N x 3\n",
    "    if bvecs.shape[0] == 3:\n",
    "        bvecs = bvecs.T\n",
    "\n",
    "    dwi = torchio.ScalarImage(\n",
    "        subj_source_dir / source_dwi_filename,\n",
    "        type=torchio.INTENSITY,\n",
    "        bvals=bvals,\n",
    "        bvecs=bvecs,\n",
    "        reader=pitn.io.nifti_reader,\n",
    "        channels_last=True,\n",
    "    )\n",
    "\n",
    "    subj_mask_dir = subj_dir / source_mask_dir\n",
    "    brain_mask = torchio.LabelMap(\n",
    "        subj_mask_dir / mask_filename,\n",
    "        type=torchio.LABEL,\n",
    "        channels_last=False,\n",
    "    )\n",
    "    brain_mask.set_data(brain_mask.data.bool())\n",
    "    \n",
    "    # Create the initial Subject object with the DWI and the corresponding mask.\n",
    "    subj_dict = torchio.Subject(subj_id=subj_id, dwi=dwi, mask=brain_mask)\n",
    "    \n",
    "    # Perform some light processing on the DWI.\n",
    "    dwi_preproc = dwi_processing_transforms(subj_dict)\n",
    "    # Clip extreme values on a per-channel basis of the masked DWI.\n",
    "    clipped_dwi_dict = intensity_clip_transform(\n",
    "        {\"dwi\": dwi_preproc[\"dwi\"].tensor * dwi_preproc[\"mask\"].tensor}\n",
    "    )\n",
    "    dwi_preproc[\"dwi\"].set_data(clipped_dwi_dict[\"dwi\"])\n",
    "    \n",
    "    # Downsample the DWIs, and fit both the full-resolution and low-resolution DWIs to\n",
    "    # DTIs.\n",
    "    preproc_subj = downsample_dti_fitting_transforms(dwi_preproc)\n",
    "\n",
    "    # Save out all images to files.\n",
    "    header = nib.load(subj_source_dir / source_dwi_filename).header\n",
    "    # FR images.\n",
    "    subj_fr_output_dir = fr_output_dir / f\"sub-{subj_id}/dti\"\n",
    "    subj_fr_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    dti_img = nib.Nifti1Image(\n",
    "        preproc_subj[\"fr_dti\"][\"data\"].cpu().numpy(),\n",
    "        affine=preproc_subj[\"fr_dti\"][\"affine\"],\n",
    "        header=header,\n",
    "    )\n",
    "    mask_img = nib.Nifti1Image(\n",
    "        preproc_subj[\"fr_mask\"][\"data\"].cpu().numpy(),\n",
    "        affine=preproc_subj[\"fr_mask\"][\"affine\"],\n",
    "        header=header,\n",
    "    )\n",
    "\n",
    "    nib.save(dti_img, subj_fr_output_dir / filenames.fr.dti)\n",
    "    nib.save(mask_img, subj_fr_output_dir / filenames.fr.mask)\n",
    "    # Save some of the pipeline description\n",
    "    fr_pipeline_dir = subj_fr_output_dir / \"pipeline\"\n",
    "    fr_pipeline_dir.mkdir(parents=True, exist_ok=True)\n",
    "    fr_pipe_file = fr_pipeline_dir / \"description.txt\"\n",
    "    with open(fr_pipe_file, \"a+\") as f:\n",
    "        f.write(\"Pipeline Description\\n\")\n",
    "        f.write(f\"Voxel spacing: {params.source_vox_size}\\n\")\n",
    "        f.write(f\"dipy version {dipy.__version__}\\n\")\n",
    "        f.write(f\"Params {params.to_dict()}\\n\")\n",
    "        f.write(\n",
    "            \"Pipeline functions:\\n\"\n",
    "            + f\"{(dwi_processing_transforms, intensity_clip_transform, downsample_dti_fitting_transforms)}\\n\"\n",
    "        )\n",
    "\n",
    "    # LR images.\n",
    "    subj_lr_output_dir = lr_output_dir / f\"sub-{subj_id}/dti\"\n",
    "    subj_lr_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    dti_img = nib.Nifti1Image(\n",
    "        preproc_subj[\"lr_dti\"][\"data\"].cpu().numpy(),\n",
    "        affine=preproc_subj[\"lr_dti\"][\"affine\"],\n",
    "        header=header,\n",
    "    )\n",
    "    mask_img = nib.Nifti1Image(\n",
    "        preproc_subj[\"lr_mask\"][\"data\"].cpu().numpy(),\n",
    "        affine=preproc_subj[\"lr_mask\"][\"affine\"],\n",
    "        header=header,\n",
    "    )\n",
    "\n",
    "    nib.save(dti_img, subj_lr_output_dir / filenames.lr.dti)\n",
    "    nib.save(mask_img, subj_lr_output_dir / filenames.lr.mask)\n",
    "    # Save some of the pipeline description\"\n",
    "    lr_pipeline_dir = subj_lr_output_dir / \"pipeline\"\n",
    "    lr_pipeline_dir.mkdir(parents=True, exist_ok=True)\n",
    "    lr_pipe_file = lr_pipeline_dir / \"description.txt\"\n",
    "    with open(lr_pipe_file, \"a+\") as f:\n",
    "        f.write(\"Pipeline Description\\n\")\n",
    "        f.write(f\"Original voxel spacing: {params.source_vox_size}\\n\")\n",
    "        f.write(f\"Downscaled voxel spacing: {params.target_vox_size}\\n\")\n",
    "        f.write(f\"dipy version {dipy.__version__}\\n\")\n",
    "        f.write(f\"Params {params.to_dict()}\\n\")\n",
    "        f.write(\n",
    "            \"Pipeline functions:\\n\"\n",
    "            + f\"{(dwi_processing_transforms, intensity_clip_transform, downsample_dti_fitting_transforms)}\\n\"\n",
    "        )\n",
    "\n",
    "    # Optionally save into a dict for later processing.\n",
    "    #     hcp_subj_data[subj_id] = preproc_subj\n",
    "\n",
    "    print(\"=\" * 20)\n",
    "\n",
    "print(\"===Data Loaded & Transformed===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bc8c9d-8ee0-46be-8be1-0c9a6291c0de",
   "metadata": {
    "tags": []
   },
   "source": [
    "### OASIS3 - DTI Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a833498a-d7cc-436a-a9dd-1ed1fd6758f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pipeline for (only) fitting DTIs, no downscaling.\n",
    "dti_fitting_transforms = torchio.Compose(\n",
    "    [\n",
    "        pitn.transforms.FitDTITransform(\n",
    "            \"bvals\",\n",
    "            \"bvecs\",\n",
    "            \"mask\",\n",
    "            fit_method=params.dti_fit_method,\n",
    "            include=(\"dwi\"),\n",
    "            #             cache_dir=\"./.cache\",\n",
    "            copy=False,\n",
    "        ),\n",
    "        pitn.transforms.RenameImageTransform({\"dwi\": \"dti\", \"dwi\": \"dti\"}, copy=False),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7f1ea0-0f84-47f0-b1a5-686bfcc4b149",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load and process DWIs, downsample DWIs, and fit DTIs to both full resolution and low-\n",
    "# resolution DTIs.\n",
    "oasis_subj_data: dict = dict()\n",
    "\n",
    "# OASIS source data files are stored in a BIDS-like structure with\n",
    "# `sub-OAS3[subj_number]/ses-[session_id]/[image_type]`\n",
    "for subj_dir in oasis_source_data_dir.glob(\"sub-OAS[0-9]*\"):\n",
    "    \n",
    "    # Individual scans can be broken out by 1) subject id, 2) session id, and optionally\n",
    "    # 3) run number.\n",
    "    subj_id = subj_dir.name\n",
    "    sessions = list(subj_dir.glob(\"ses-*\"))\n",
    "    sessions = natsorted(sessions)\n",
    "    # Just take the first available session.\n",
    "    session_id = sessions[0].name\n",
    "    # Find the available run numbers, if any.\n",
    "    runs = (subj_dir / session_id / \"dwi\").glob(\"*run-*\")\n",
    "    runs = set(map(lambda s: re.search(r\"run-[0-9]+\", s.name)[0], runs))\n",
    "    runs = natsorted(list(runs))\n",
    "    # If more than one run exists, pick the first one (usually \"01\"). Otherwise, set the\n",
    "    # id to be an empty string for later.\n",
    "    if len(runs) == 0:\n",
    "        run_id = \"\"\n",
    "    else:\n",
    "        run_id = runs[0]\n",
    "    # Construct the freesurfer-specific subject id, which is a combination of subject\n",
    "    # id and session id.\n",
    "    freesurfer_id = f'{subj_id.replace(\"sub-\", \"\")}_MR_{session_id.replace(\"ses-\", \"\")}'\n",
    "\n",
    "    # The subject's DWI directory\n",
    "    subj_dwi_dir = subj_dir / session_id / \"dwi\"\n",
    "    read_files = Box(default_box=True)\n",
    "    read_files.dwi = list(\n",
    "        subj_dwi_dir.glob(f\"*{str(run_id) + '*' if run_id else run_id}dwi*.nii.gz\")\n",
    "    )[0]\n",
    "    # All DWI-related files have the same prefix name, with different file types.\n",
    "    base_name = read_files.dwi.name.replace(\"\".join(read_files.dwi.suffixes), \"\")\n",
    "    read_files.bvals = subj_dwi_dir / (base_name + \".bval\")\n",
    "    read_files.bvecs = subj_dwi_dir / (base_name + \".bvec\")\n",
    "\n",
    "    # Grab the mask generated from the previous T1 freesurfer processing; this already\n",
    "    # aligns with the DWIs, but will need to be resampled into the DWI space.\n",
    "    read_files.mask = list(\n",
    "        (oasis_processed_data_dir / f\"sub-{freesurfer_id}\" / \"mask\").glob(\n",
    "            \"*mask*.nii.gz\"\n",
    "        )\n",
    "    )[0]\n",
    "\n",
    "    bvals = torch.as_tensor(np.loadtxt(read_files.bvals).astype(int))\n",
    "    bvecs = torch.as_tensor(np.loadtxt(read_files.bvecs))\n",
    "    # Reshape to be N x 3\n",
    "    if bvecs.shape[0] == 3:\n",
    "        bvecs = bvecs.T\n",
    "\n",
    "    dwi = torchio.ScalarImage(\n",
    "        read_files.dwi,\n",
    "        type=torchio.INTENSITY,\n",
    "        bvals=bvals,\n",
    "        bvecs=bvecs,\n",
    "        reader=pitn.io.nifti_reader,\n",
    "        channels_last=True,\n",
    "    )\n",
    "\n",
    "    subj_mask_dir = subj_dir / source_mask_dir\n",
    "    brain_mask = torchio.LabelMap(\n",
    "        read_files.mask,\n",
    "        type=torchio.LABEL,\n",
    "        channels_last=False,\n",
    "    )\n",
    "    brain_mask.set_data(brain_mask.data.bool())\n",
    "    # The mask is taken from the freesurfer processing of the T1w image, which is in a\n",
    "    # higher spatial resolution than the DWIs. So, need to resample the mask to the\n",
    "    # DWI spacing. Typically we don't like resampling, but on the mask only should be\n",
    "    # fine.\n",
    "    resampler = torchio.transforms.Resample(dwi)\n",
    "    brain_mask = resampler(brain_mask)\n",
    "    # Slightly dilate the mask to account for registration errors between T1 and DWIs.\n",
    "    np_mask = brain_mask.tensor[0].numpy().astype(bool)\n",
    "    np_mask = skimage.morphology.binary_dilation(np_mask, skimage.morphology.ball(4))\n",
    "    brain_mask.set_data(\n",
    "        torch.from_numpy(np_mask).bool()[\n",
    "            None,\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Create the initial Subject object to pass to the processing pipeline.\n",
    "    subj_dict = torchio.Subject(subj_id=subj_id, dwi=dwi, mask=brain_mask)\n",
    "    \n",
    "    # Perform initial, light processing of DWIs.\n",
    "    dwi_preproc = dwi_processing_transforms(subj_dict)\n",
    "    # Clip extreme values in the DWIs, seen as noise.\n",
    "    clipped_dwi_dict = intensity_clip_transform(\n",
    "        {\"dwi\": dwi_preproc[\"dwi\"].tensor * dwi_preproc[\"mask\"].tensor}\n",
    "    )\n",
    "    dwi_preproc[\"dwi\"].set_data(clipped_dwi_dict[\"dwi\"])\n",
    "    # Fit the DWIs to DTIs.\n",
    "    preproc_subj = dti_fitting_transforms(dwi_preproc)\n",
    "\n",
    "    # Save out DTIs to a file.\n",
    "    header = nib.load(read_files.dwi).header\n",
    "    # Both DTI and the mask files will go in the same directory, to indicate that this\n",
    "    # mask is specific to the DTI space.\n",
    "    subj_output_dir = oasis_processed_data_dir / f\"sub-{freesurfer_id}/dti\"\n",
    "    subj_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    write_dti_file = subj_output_dir / f\"sub-{freesurfer_id}_scale-orig_dti.nii.gz\"\n",
    "    write_mask_file = subj_output_dir / f\"sub-{freesurfer_id}_scale-orig_mask.nii.gz\"\n",
    "\n",
    "    dti_img = nib.Nifti1Image(\n",
    "        preproc_subj[\"dti\"][\"data\"].cpu().numpy(),\n",
    "        affine=preproc_subj[\"dti\"][\"affine\"],\n",
    "        header=header,\n",
    "    )\n",
    "    mask_img = nib.Nifti1Image(\n",
    "        preproc_subj[\"mask\"][\"data\"].cpu().numpy(),\n",
    "        affine=preproc_subj[\"mask\"][\"affine\"],\n",
    "        header=header,\n",
    "    )\n",
    "\n",
    "    nib.save(dti_img, write_dti_file)\n",
    "    nib.save(mask_img, write_mask_file)\n",
    "    # Save some of the pipeline description\n",
    "    pipeline_dir = subj_output_dir / \"pipeline\"\n",
    "    pipeline_dir.mkdir(parents=True, exist_ok=True)\n",
    "    pipe_file = pipeline_dir / \"description.txt\"\n",
    "    with open(pipe_file, \"a+\") as f:\n",
    "        f.write(\"Pipeline Description\\n\")\n",
    "        f.write(f\"Voxel spacing: {params.target_vox_size}\\n\")\n",
    "        f.write(f\"Params {params.to_dict()}\\n\")\n",
    "        f.write(\n",
    "            \"Pipeline functions:\\n\"\n",
    "            + f\"{(dwi_processing_transforms, intensity_clip_transform, dti_fitting_transforms)}\\n\"\n",
    "        )\n",
    "        f.write(f\"dipy version {dipy.__version__}\\n\")\n",
    "        f.write(f\"nibabel version {nib.__version__}\\n\")\n",
    "    # Optionally save into a dict for later processing.\n",
    "    #     oasis_subj_data[subj_id] = preproc_subj\n",
    "\n",
    "    print(\"=\" * 20)\n",
    "\n",
    "print(\"===Data Loaded & Transformed===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ad087c-33d5-4f00-8f0e-5159b0bd7d2b",
   "metadata": {},
   "source": [
    "### UVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf90f239-0fa2-419d-a3d9-2b0a8db3cdcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda-pitn]",
   "language": "python",
   "name": "conda-env-miniconda-pitn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
