{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13edb204-c964-4525-80d8-0fdd2730df3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pain in the Net - Laplacian Pyramid Translation Network (LPTN)\n",
    "Application of Laplacian Pyramid Translation Network (LPTN) to domain adaptation of diffusion MRI.\n",
    "\n",
    "\n",
    "Code by:\n",
    "\n",
    "Tyler Spears - tas6hh@virginia.edu\n",
    "\n",
    "Dr. Tom Fletcher\n",
    "\n",
    "---\n",
    "\n",
    "Based on the following work(s):\n",
    "\n",
    "* `J. Liang, H. Zeng, and L. Zhang, “High-Resolution Photorealistic Image Translation in Real-Time: A Laplacian Pyramid Translation Network,” 2021, pp. 9392–9400. Accessed: Aug. 26, 2021. [Online]. Available: https://openaccess.thecvf.com/content/CVPR2021/html/Liang_High-Resolution_Photorealistic_Image_Translation_in_Real-Time_A_Laplacian_Pyramid_Translation_CVPR_2021_paper.html\n",
    "`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da03c5c0-aea8-4c1c-98ca-bf001e59dde2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports & Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6f91f0-eca2-4990-9214-f34aa16b4d05",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bbe9cc-5b3f-4204-870d-4a5bc5f28aab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automatically re-import project-specific modules.\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# imports\n",
    "import collections\n",
    "import functools\n",
    "import io\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "import copy\n",
    "import pdb\n",
    "import inspect\n",
    "import random\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import typing\n",
    "import zipfile\n",
    "\n",
    "import ants\n",
    "import dipy\n",
    "import dipy.core\n",
    "import dipy.reconst\n",
    "import dipy.reconst.dti\n",
    "import dipy.segment.mask\n",
    "import dipy.viz\n",
    "import dipy.viz.regtools\n",
    "import dotenv\n",
    "\n",
    "# visualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import mpl_toolkits\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import IPython\n",
    "\n",
    "# Try importing GPUtil for printing GPU specs.\n",
    "# May not be installed if using CPU only.\n",
    "try:\n",
    "    import GPUtil\n",
    "except ImportError:\n",
    "    warnings.warn(\"WARNING: Package GPUtil not found, cannot print GPU specs\")\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, Markdown\n",
    "import ipyplot\n",
    "\n",
    "# Data management libraries.\n",
    "import nibabel as nib\n",
    "import natsort\n",
    "from natsort import natsorted\n",
    "import addict\n",
    "from addict import Addict\n",
    "import box\n",
    "from box import Box\n",
    "import pprint\n",
    "from pprint import pprint as ppr\n",
    "\n",
    "# Computation & ML libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchio\n",
    "import pytorch_lightning as pl\n",
    "import monai\n",
    "\n",
    "import skimage\n",
    "import skimage.feature\n",
    "import skimage.filters\n",
    "import skimage.measure\n",
    "import scipy\n",
    "\n",
    "%aimport pitn\n",
    "import pitn\n",
    "\n",
    "\n",
    "plt.rcParams.update({\"figure.autolayout\": True})\n",
    "plt.rcParams.update({\"figure.facecolor\": [1.0, 1.0, 1.0, 1.0]})\n",
    "\n",
    "# Set print options for ndarrays/tensors.\n",
    "np.set_printoptions(suppress=True, edgeitems=2, threshold=100, linewidth=88)\n",
    "torch.set_printoptions(\n",
    "    sci_mode=False, edgeitems=2, threshold=100, linewidth=88, profile=\"short\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b8326b-55e1-4195-a85f-7627f591f1f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update notebook's environment variables with direnv.\n",
    "# This requires the python-dotenv package, and direnv be installed on the system\n",
    "# This will not work on Windows.\n",
    "# NOTE: This is kind of hacky, and not necessarily safe. Be careful...\n",
    "# Libraries needed on the python side:\n",
    "# - os\n",
    "# - subprocess\n",
    "# - io\n",
    "# - dotenv\n",
    "\n",
    "# Form command to be run in direnv's context. This command will print out\n",
    "# all environment variables defined in the subprocess/sub-shell.\n",
    "command = f\"direnv exec {os.getcwd()} /usr/bin/env\"\n",
    "# Run command in a new subprocess.\n",
    "proc = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, cwd=os.getcwd())\n",
    "# Store and format the subprocess' output.\n",
    "proc_out = proc.communicate()[0].strip().decode(\"utf-8\")\n",
    "# Use python-dotenv to load the environment variables by using the output of\n",
    "# 'direnv exec ...' as a 'dummy' .env file.\n",
    "dotenv.load_dotenv(stream=io.StringIO(proc_out), override=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a706635-b544-4f41-886b-d6997dda27e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch setup\n",
    "# allow for CUDA usage, if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# keep device as the cpu\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498af6cf-4263-4415-b6f3-fd32abab4bcb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Specs Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff2952b-b4a3-4241-9322-e317c32cb2db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr cap\n",
    "# Capture output and save to log. Needs to be at the *very first* line of the cell.\n",
    "# Watermark\n",
    "%load_ext watermark\n",
    "%watermark --author \"Tyler Spears\" --updated --iso8601  --python --machine --iversions --githash\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    # GPU information\n",
    "    # Taken from\n",
    "    # <https://www.thepythoncode.com/article/get-hardware-system-information-python>.\n",
    "    # If GPUtil is not installed, skip this step.\n",
    "    try:\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        print(\"=\" * 50, \"GPU Specs\", \"=\" * 50)\n",
    "        list_gpus = []\n",
    "        for gpu in gpus:\n",
    "            # get the GPU id\n",
    "            gpu_id = gpu.id\n",
    "            # name of GPU\n",
    "            gpu_name = gpu.name\n",
    "            driver_version = gpu.driver\n",
    "            cuda_version = torch.version.cuda\n",
    "            # get total memory\n",
    "            gpu_total_memory = f\"{gpu.memoryTotal}MB\"\n",
    "            gpu_uuid = gpu.uuid\n",
    "            list_gpus.append(\n",
    "                (\n",
    "                    gpu_id,\n",
    "                    gpu_name,\n",
    "                    driver_version,\n",
    "                    cuda_version,\n",
    "                    gpu_total_memory,\n",
    "                    gpu_uuid,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            tabulate(\n",
    "                list_gpus,\n",
    "                headers=(\n",
    "                    \"id\",\n",
    "                    \"Name\",\n",
    "                    \"Driver Version\",\n",
    "                    \"CUDA Version\",\n",
    "                    \"Total Memory\",\n",
    "                    \"uuid\",\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    except NameError:\n",
    "        print(\"CUDA Version: \", torch.version.cuda)\n",
    "\n",
    "else:\n",
    "    print(\"CUDA not in use, falling back to CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae38f0a-8614-4e0d-a40a-bee5a3585baa",
   "metadata": {
    "tags": [
     "keep_output"
    ]
   },
   "outputs": [],
   "source": [
    "# cap is defined in an ipython magic command\n",
    "print(cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09447c83-298e-444a-91ca-aa46058eb956",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Variables & Definitions Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc41112b-67dc-4fa3-828e-48a2c88dfb49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up directories\n",
    "data_dir = pathlib.Path(os.environ[\"DATA_DIR\"])\n",
    "\n",
    "# Directories that contain processed DTI data. The `*_processed_data_dir` should hold\n",
    "# the directory containing all subjects indexed by the eventual `possible_ids`.\n",
    "processed_data_dir = pathlib.Path(os.environ[\"WRITE_DATA_DIR\"])\n",
    "hcp_processed_data_dir = (\n",
    "    processed_data_dir / \"hcp/derivatives/mean-downsample/scale-2.00mm\"\n",
    ")\n",
    "clinic_processed_data_dir = (\n",
    "    processed_data_dir / \"oasis3/derivatives/mean-downsample/scale-orig\"\n",
    ")\n",
    "assert hcp_processed_data_dir.exists() and clinic_processed_data_dir.exists()\n",
    "results_dir = pathlib.Path(os.environ[\"RESULTS_DIR\"])\n",
    "assert results_dir.exists()\n",
    "tmp_results_dir = pathlib.Path(os.environ[\"TMP_RESULTS_DIR\"])\n",
    "assert tmp_results_dir.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7a7233-48c1-439d-af3f-62f628af527c",
   "metadata": {},
   "source": [
    "### Experiment Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fdc6f9-42fc-4ce6-905e-c938b89edb0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tensorboard experiment logging setup.\n",
    "EXPERIMENT_NAME = \"test_oasis3_unproc_250_epochs\"\n",
    "\n",
    "ts = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "# Break ISO format because many programs don't like having colons ':' in a filename.\n",
    "ts = ts.replace(\":\", \"_\")\n",
    "experiment_name = ts + \"__\" + EXPERIMENT_NAME\n",
    "run_name = experiment_name\n",
    "print(experiment_name)\n",
    "# experiment_results_dir = results_dir / experiment_name\n",
    "\n",
    "# Create temporary directory for results directory, in case experiment does not finish.\n",
    "tmp_dirs = list(filter(lambda s: not str(s).startswith(\".\"), tmp_results_dir.glob(\"*\")))\n",
    "\n",
    "# Only keep up to N tmp results.\n",
    "n_tmp_to_keep = 3\n",
    "if len(tmp_dirs) > (n_tmp_to_keep - 1):\n",
    "    print(f\"More than {n_tmp_to_keep} temporary results, culling to the most recent\")\n",
    "    tmps_to_delete = natsorted([str(tmp_dir) for tmp_dir in tmp_dirs])[\n",
    "        : -(n_tmp_to_keep - 1)\n",
    "    ]\n",
    "    for tmp_dir in tmps_to_delete:\n",
    "        shutil.rmtree(tmp_dir)\n",
    "        print(\"Deleted temporary results directory \", tmp_dir)\n",
    "\n",
    "experiment_results_dir = tmp_results_dir / experiment_name\n",
    "# Final target directory, to be made when experiment is complete.\n",
    "final_experiment_results_dir = results_dir / experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe6460c-da7e-4391-980a-344508313cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pass this object into the pytorchlightning Trainer object, for easier logging within\n",
    "# the training/testing loops.\n",
    "pl_logger = pl.loggers.TensorBoardLogger(\n",
    "    tmp_results_dir,\n",
    "    name=experiment_name,\n",
    "    version=\"\",\n",
    "    log_graph=False,\n",
    "    default_hp_metric=False,\n",
    ")\n",
    "# Use the lower-level logger for logging histograms, images, etc.\n",
    "logger = pl_logger.experiment\n",
    "\n",
    "# Create a separate txt file to log streams of events & info besides parameters & results.\n",
    "log_txt_file = Path(logger.log_dir) / \"log.txt\"\n",
    "with open(log_txt_file, \"a+\") as f:\n",
    "    f.write(f\"Experiment Name: {experiment_name}\\n\")\n",
    "    f.write(f\"Timestamp: {ts}\\n\")\n",
    "    # cap is defined in an ipython magic command\n",
    "    f.write(f\"Environment and Hardware Info:\\n {cap}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82e4788-bced-42d5-b341-3652317f2427",
   "metadata": {},
   "source": [
    "### Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb06aa61-9f96-4b68-92c9-4a54bf351870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = Box(default_box=True)\n",
    "\n",
    "# Data params.\n",
    "params.num_channels = 6\n",
    "params.hcp.num_subjects = 14\n",
    "params.clinic.num_subjects = 9\n",
    "params.clamp_percentiles = (0.01, 99.99)\n",
    "# params.data_scale_range = None\n",
    "# Scale input data by the valid values of each channel of the DTI.\n",
    "# I.e., Dx,x in [0, 1], Dx,y in [-1, 1], Dy,y in [0, 1], Dy,z in [-1, 1], etc.\n",
    "params.data_scale_range = ((0, -1, 0, -1, -1, 0), (1, 1, 1, 1, 1, 1))\n",
    "\n",
    "# Network params.\n",
    "params.num_laplace_high_freq = 3\n",
    "params.discriminator_downscale_factors = [1, 2, 4]\n",
    "params.lambda_adversary_loss = 10\n",
    "params.lambda_reconst_loss_weight = 50\n",
    "params.use_grad_penalty = False\n",
    "params.lambda_grad_penalty = 100\n",
    "# Set the init function to None to change to pytorch default initialization.\n",
    "# params.net_init.f = None\n",
    "params.net_init.mean = 0.0\n",
    "params.net_init.std = 0.02\n",
    "\n",
    "# Adam optimizer kwargs for each network.\n",
    "params.optim.gen_kwargs.lr = 2e-4\n",
    "params.optim.gen_kwargs.betas = (0.5, 0.99)\n",
    "params.optim.discriminator_kwargs.lr = 2e-4\n",
    "params.optim.discriminator_kwargs.betas = (0.5, 0.99)\n",
    "\n",
    "# Training, validation, & testing params\n",
    "# Patch size must be a factor of 2**num_laplace_high_freq\n",
    "params.train.patch_size = (32, 32, 32)\n",
    "params.batch_size = 32\n",
    "params.samples_per_subj_per_epoch = 1000\n",
    "params.max_epochs = 250\n",
    "params.train.hcp_num_subjects = 13\n",
    "params.val.hcp_num_subjects = 1\n",
    "\n",
    "# Create these assert statements because having an invalid number of train/val subjects\n",
    "# may not be caught in the loading below and cause a silent runtime error.\n",
    "assert params.train.hcp_num_subjects <= params.hcp.num_subjects\n",
    "assert params.val.hcp_num_subjects <= params.hcp.num_subjects\n",
    "\n",
    "with open(log_txt_file, \"a+\") as f:\n",
    "    f.write(pprint.pformat(params.to_dict()) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c39e21-8cf5-4f2e-b715-5bf8d310d653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optional weight & bias initialization of conv layers.\n",
    "@torch.no_grad()\n",
    "def conv_init_normal(m, mean, std):\n",
    "    if isinstance(m, (torch.nn.Conv3d, torch.nn.ConvTranspose3d)):\n",
    "        torch.nn.init.normal_(m.weight, mean=mean, std=std)\n",
    "        torch.nn.init.normal_(m.bias, mean=mean, std=std)\n",
    "\n",
    "\n",
    "if params.net_init.to_dict():\n",
    "    f = functools.partial(\n",
    "        conv_init_normal, mean=params.net_init.mean, std=params.net_init.std\n",
    "    )\n",
    "    params.net_init.f = f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1009ab0f-71cd-4692-8722-e6fa947366d1",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ea665-dc8b-45b8-be6f-b9cc3c5f62b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transformation pipeline.\n",
    "# The input to the laplacian pyramid must be divisible by 2 for the number of high-\n",
    "# frequency levels in the pyramid.\n",
    "laplace_pyramid_divisible_by_shape = 2**params.num_laplace_high_freq\n",
    "\n",
    "pre_process_pipeline = monai.transforms.Compose(\n",
    "    [\n",
    "        monai.transforms.CropForegroundd([\"dti\", \"mask\"], source_key=\"mask\", margin=3),\n",
    "        monai.transforms.DivisiblePadd(\n",
    "            [\"dti\", \"mask\"], laplace_pyramid_divisible_by_shape\n",
    "        ),\n",
    "        # Data are already clipped by percentile (see the pipeline.txt file in each\n",
    "        # subj directory for details).\n",
    "        #         pitn.transforms.ClipPercentileTransformd(\n",
    "        #             \"dti\",\n",
    "        #             params.clamp_percentiles[0],\n",
    "        #             params.clamp_percentiles[1],\n",
    "        #             nonzero=True,\n",
    "        #             channel_wise=True,\n",
    "        #         ),\n",
    "        monai.transforms.ToTensord(\"dti\", dtype=torch.float),\n",
    "        monai.transforms.ToTensord(\"mask\", dtype=torch.bool),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03506296-ec92-4083-93c1-83308f181847",
   "metadata": {},
   "source": [
    "### Load and Pre-Process HCP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac5a22-bd1a-41e3-ac99-3d4cdb1ee14c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find data directories for each subject.\n",
    "hcp_subj_dirs: dict = dict()\n",
    "\n",
    "possible_ids = [\n",
    "    \"sub-397154\",\n",
    "    \"sub-224022\",\n",
    "    \"sub-140117\",\n",
    "    \"sub-751348\",\n",
    "    \"sub-894774\",\n",
    "    \"sub-156637\",\n",
    "    \"sub-227432\",\n",
    "    \"sub-303624\",\n",
    "    \"sub-185947\",\n",
    "    \"sub-810439\",\n",
    "    \"sub-753251\",\n",
    "    \"sub-644246\",\n",
    "    \"sub-141422\",\n",
    "    \"sub-135528\",\n",
    "    \"sub-103010\",\n",
    "    \"sub-700634\",\n",
    "]\n",
    "\n",
    "## Sub-set the chosen participants for dev and debugging!\n",
    "selected_ids = random.sample(possible_ids, params.hcp.num_subjects)\n",
    "if params.hcp.num_subjects < len(possible_ids):\n",
    "    warnings.warn(\n",
    "        \"WARNING: Sub-selecting participants for dev and debugging. \"\n",
    "        + f\"Subj IDs selected: {selected_ids}\"\n",
    "    )\n",
    "# ### A nested warning! For debugging only.\n",
    "# warnings.warn(\"WARNING: Mixing training and testing subjects\")\n",
    "# selected_ids.append(selected_ids[0])\n",
    "# ###\n",
    "##\n",
    "\n",
    "selected_ids = natsorted(selected_ids)\n",
    "\n",
    "for subj_id in selected_ids:\n",
    "    hcp_subj_dirs[subj_id] = hcp_processed_data_dir / f\"{subj_id}\"\n",
    "    assert hcp_subj_dirs[subj_id].exists()\n",
    "ppr(hcp_subj_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f5408d-5934-44cc-9aba-01df3461e7ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log to file and experiment.\n",
    "with open(log_txt_file, \"a+\") as f:\n",
    "    f.write(f\"Selected HCP Subjects: {selected_ids}\\n\")\n",
    "\n",
    "logger.add_text(\"hcp_subjs\", pprint.pformat(selected_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f30dbd-f4d2-4144-9ba1-6dbf9097b441",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data loading and processing loop.\n",
    "hcp_subj_data = list()\n",
    "# Data reader object for NIFTI files.\n",
    "nib_reader = monai.data.NibabelReader(as_closest_canonical=False)\n",
    "\n",
    "# Directory prefixes for each image to be read.\n",
    "dti_file_prefix = \"dti\"\n",
    "# Mask was saved alongside the DTI nifti file.\n",
    "mask_file_prefix = dti_file_prefix\n",
    "\n",
    "for subj_id, subj_dir in hcp_subj_dirs.items():\n",
    "\n",
    "    subj_data = dict()\n",
    "    subj_data[\"subj_id\"] = subj_id\n",
    "\n",
    "    # Load the DTIs\n",
    "    img_dir = subj_dir / dti_file_prefix\n",
    "    img_filename = list(img_dir.glob(f\"{subj_id}*dti.nii.gz\"))\n",
    "    # Make sure the glob pattern only matches one file.\n",
    "    assert len(img_filename) == 1\n",
    "    img_filename = img_filename[0]\n",
    "    nib_img = nib_reader.read(img_filename)\n",
    "    img, metadata = nib_reader.get_data(nib_img)\n",
    "    subj_data[\"dti\"] = img\n",
    "    # The default metadata key name for monai.\n",
    "    subj_data[\"dti_meta_dict\"] = metadata\n",
    "\n",
    "    # Load masks\n",
    "    img_dir = subj_dir / mask_file_prefix\n",
    "    img_filename = list(img_dir.glob(f\"{subj_id}*mask.nii.gz\"))\n",
    "    # Make sure the glob pattern only matches one file.\n",
    "    assert len(img_filename) == 1\n",
    "    img_filename = img_filename[0]\n",
    "    nib_img = nib_reader.read(img_filename)\n",
    "    img, metadata = nib_reader.get_data(nib_img)\n",
    "    subj_data[\"mask\"] = img\n",
    "    # The default metadata key name for monai.\n",
    "    subj_data[\"mask_meta_dict\"] = metadata\n",
    "\n",
    "    # Pre-process subject DTIs.\n",
    "    subj_data = pre_process_pipeline(subj_data)\n",
    "\n",
    "    # Perform scaling of input data?\n",
    "    if params.data_scale_range is not None:\n",
    "        scaler = pitn.data.norm.DTIMinMaxScaler(\n",
    "            params.data_scale_range[0],\n",
    "            params.data_scale_range[1],\n",
    "            dim=(1, 2, 3),\n",
    "            channel_size=params.num_channels,\n",
    "        )\n",
    "        scaled = scaler.scale(subj_data[\"dti\"] * subj_data[\"mask\"], stateful=True)\n",
    "        subj_data[\"dti\"] = scaled * subj_data[\"mask\"]\n",
    "        subj_data[\"scaler\"] = scaler\n",
    "\n",
    "    hcp_subj_data.append(subj_data)\n",
    "\n",
    "# Create dataset with all HCP subjects included.\n",
    "hcp_subj_dataset = monai.data.Dataset(hcp_subj_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f3d4d9-333b-464e-be59-485d8de80582",
   "metadata": {},
   "source": [
    "### Load & Pre-Process Clinical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4cda4b-7289-4b78-bb67-88e2c3d3d67d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # UVA data loading.\n",
    "# # Find data directories for each subject.\n",
    "# clinic_subj_dirs: dict = dict()\n",
    "\n",
    "# possible_ids = [\"001\"]\n",
    "\n",
    "# ## Sub-set the chosen participants for dev and debugging!\n",
    "# selected_ids = random.sample(possible_ids, params.clinic.num_subjects)\n",
    "# if params.clinic.num_subjects < len(possible_ids):\n",
    "#     warnings.warn(\n",
    "#         \"WARNING: Sub-selecting participants for dev and debugging. \"\n",
    "#         + f\"Subj IDs selected: {selected_ids}\"\n",
    "#     )\n",
    "# # ### A nested warning! For debugging only.\n",
    "# # warnings.warn(\"WARNING: Mixing training and testing subjects\")\n",
    "# # selected_ids.append(selected_ids[0])\n",
    "# # ###\n",
    "# ##\n",
    "\n",
    "# selected_ids = natsorted(selected_ids)\n",
    "\n",
    "# for subj_id in selected_ids:\n",
    "#     clinic_subj_dirs[subj_id] = (\n",
    "#         clinic_processed_data_dir / f\"derivatives/diffusion/sub-{subj_id}/ses-01\"\n",
    "#     )\n",
    "#     assert clinic_subj_dirs[subj_id].exists()\n",
    "# ppr(clinic_subj_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8061c3-dc01-49e2-a6c5-ecff830766de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OASIS3 Dataset\n",
    "\n",
    "# Find data directories for each subject.\n",
    "clinic_subj_dirs: dict = dict()\n",
    "\n",
    "possible_ids = [\n",
    "    \"sub-OAS30188_MR_d3844\",\n",
    "    \"sub-OAS30375_MR_d5792\",\n",
    "    \"sub-OAS30558_MR_d2148\",\n",
    "    \"sub-OAS30643_MR_d0280\",\n",
    "    \"sub-OAS30685_MR_d0032\",\n",
    "    \"sub-OAS30762_MR_d0043\",\n",
    "    \"sub-OAS30770_MR_d1201\",\n",
    "    \"sub-OAS30944_MR_d0089\",\n",
    "    \"sub-OAS31018_MR_d0041\",\n",
    "    \"sub-OAS31157_MR_d4924\",\n",
    "]\n",
    "\n",
    "## Sub-set the chosen participants for dev and debugging!\n",
    "selected_ids = random.sample(possible_ids, params.clinic.num_subjects)\n",
    "if params.clinic.num_subjects < len(possible_ids):\n",
    "    warnings.warn(\n",
    "        \"WARNING: Sub-selecting participants for dev and debugging. \"\n",
    "        + f\"Subj IDs selected: {selected_ids}\"\n",
    "    )\n",
    "# ### A nested warning! For debugging only.\n",
    "# warnings.warn(\"WARNING: Mixing training and testing subjects\")\n",
    "# selected_ids.append(selected_ids[0])\n",
    "# ###\n",
    "##\n",
    "\n",
    "selected_ids = natsorted(selected_ids)\n",
    "\n",
    "for subj_id in selected_ids:\n",
    "    clinic_subj_dirs[subj_id] = clinic_processed_data_dir / f\"{subj_id}\"\n",
    "    assert clinic_subj_dirs[subj_id].exists()\n",
    "ppr(clinic_subj_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166fa58d-b619-4a17-8440-43f15f2f5c50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log to file and experiment.\n",
    "with open(log_txt_file, \"a+\") as f:\n",
    "    f.write(f\"Selected Clinic-Scanned Subjects: {selected_ids}\\n\")\n",
    "\n",
    "logger.add_text(\"clinic_data_subjs\", pprint.pformat(selected_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed602676-361b-49cb-af88-c7976f653e13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data loading and processing loop.\n",
    "clinic_subj_data = list()\n",
    "# Data reader object for NIFTI files.\n",
    "nib_reader = monai.data.NibabelReader(as_closest_canonical=False)\n",
    "\n",
    "# Directory prefixes for each image to be read.\n",
    "dti_file_prefix = \"dti\"\n",
    "# Mask is specific to the DTI, so it is located alongside the DTI nifti file.\n",
    "mask_file_prefix = dti_file_prefix\n",
    "\n",
    "for subj_id, subj_dir in clinic_subj_dirs.items():\n",
    "    subj_data = dict()\n",
    "    subj_data[\"subj_id\"] = subj_id\n",
    "\n",
    "    # Load the DTIs\n",
    "    img_dir = subj_dir / dti_file_prefix\n",
    "    img_filename = list(img_dir.glob(f\"{subj_id}*dti.nii.gz\"))\n",
    "    # Make sure the glob pattern only matches one file.\n",
    "    assert len(img_filename) == 1\n",
    "    img_filename = img_filename[0]\n",
    "    nib_img = nib_reader.read(img_filename)\n",
    "    img, metadata = nib_reader.get_data(nib_img)\n",
    "    subj_data[\"dti\"] = img\n",
    "    # The default metadata key name for monai.\n",
    "    subj_data[\"dti_meta_dict\"] = metadata\n",
    "\n",
    "    # Load masks\n",
    "    img_dir = subj_dir / mask_file_prefix\n",
    "    img_filename = list(img_dir.glob(f\"{subj_id}*mask.nii.gz\"))\n",
    "    # Make sure the glob pattern only matches one file.\n",
    "    assert len(img_filename) == 1\n",
    "    img_filename = img_filename[0]\n",
    "    nib_img = nib_reader.read(img_filename)\n",
    "    img, metadata = nib_reader.get_data(nib_img)\n",
    "    subj_data[\"mask\"] = img\n",
    "    # The default metadata key name for monai.\n",
    "    subj_data[\"mask_meta_dict\"] = metadata\n",
    "\n",
    "    # Pre-process subject DTIs.\n",
    "    subj_data = pre_process_pipeline(subj_data)\n",
    "\n",
    "    # Perform scaling of input data?\n",
    "    if params.data_scale_range is not None:\n",
    "        scaler = pitn.data.norm.DTIMinMaxScaler(\n",
    "            params.data_scale_range[0],\n",
    "            params.data_scale_range[1],\n",
    "            dim=(1, 2, 3),\n",
    "            channel_size=params.num_channels,\n",
    "        )\n",
    "        scaled = scaler.scale(subj_data[\"dti\"] * subj_data[\"mask\"], stateful=True)\n",
    "        subj_data[\"dti\"] = scaled * subj_data[\"mask\"]\n",
    "        subj_data[\"scaler\"] = scaler\n",
    "\n",
    "    clinic_subj_data.append(subj_data)\n",
    "\n",
    "# Create dataset with all \"clinical quality\" subjects included.\n",
    "clinic_subj_dataset = monai.data.Dataset(clinic_subj_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0181623e-5e4d-4253-a51b-9591eecba91a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup of Training Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f3429-086e-4cc6-8da5-d06b2e27ed4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Designate HCP subjects for training, validation, and testing.\n",
    "hcp_ids = [s[\"subj_id\"] for s in hcp_subj_data]\n",
    "random.shuffle(hcp_ids)\n",
    "hcp_train_ids = hcp_ids[: params.train.hcp_num_subjects]\n",
    "hcp_val_ids = hcp_ids[: params.val.hcp_num_subjects]\n",
    "\n",
    "# Designate clinic subject IDs for training.\n",
    "clinic_ids = [s[\"subj_id\"] for s in clinic_subj_data]\n",
    "random.shuffle(clinic_ids)\n",
    "# Just select all clinic IDs.\n",
    "clinic_train_ids = clinic_ids[: params.clinic.num_subjects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0b36e9-963c-4c66-a99a-33ffee3c7fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up dataset and data loading objects.\n",
    "# ! The samplers created here will cause the source domain patches and the target domain\n",
    "# patches to *not* be aligned in any way; this is intentional for unpaired I2I.\n",
    "\n",
    "# Set up HCP scan data.\n",
    "# Training set.\n",
    "source_patch_ds = list()\n",
    "for subj_dict in filter(lambda s: s[\"subj_id\"] in hcp_train_ids, hcp_subj_data):\n",
    "    source_patch_ds.append(\n",
    "        pitn.data.MaskFilteredPatchDataset3d(\n",
    "            subj_dict[\"dti\"], mask=subj_dict[\"mask\"], patch_size=params.train.patch_size\n",
    "        )\n",
    "    )\n",
    "\n",
    "source_train_dataset = torch.utils.data.ConcatDataset(source_patch_ds)\n",
    "source_train_sampler = pitn.samplers.ConcatDatasetBalancedRandomSampler(\n",
    "    source_train_dataset.datasets,\n",
    "    max_samples_per_dataset=params.samples_per_subj_per_epoch,\n",
    ")\n",
    "\n",
    "source_train_loader = monai.data.DataLoader(\n",
    "    source_train_dataset,\n",
    "    sampler=source_train_sampler,\n",
    "    batch_size=params.batch_size,\n",
    "    pin_memory=True,\n",
    "    num_workers=7,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "# Validation set.\n",
    "source_vol_ds = list()\n",
    "for subj_dict in filter(lambda s: s[\"subj_id\"] in hcp_val_ids, hcp_subj_data):\n",
    "    source_vol_ds.append(\n",
    "        subj_dict[\"dti\"][\n",
    "            None,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "source_val_dataset = torch.utils.data.ConcatDataset(source_vol_ds)\n",
    "\n",
    "source_val_loader = monai.data.DataLoader(\n",
    "    source_val_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    #     persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d4f8c-1e37-413c-b298-e9c1a94b3f58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up clinic scan data.\n",
    "target_patch_ds = list()\n",
    "for subj_dict in filter(lambda s: s[\"subj_id\"] in clinic_train_ids, clinic_subj_data):\n",
    "    target_patch_ds.append(\n",
    "        pitn.data.MaskFilteredPatchDataset3d(\n",
    "            subj_dict[\"dti\"], mask=subj_dict[\"mask\"], patch_size=params.train.patch_size\n",
    "        )\n",
    "    )\n",
    "\n",
    "target_train_dataset = torch.utils.data.ConcatDataset(target_patch_ds)\n",
    "\n",
    "# Calculate the number of clinic samples per subject to match the total length of the\n",
    "# source domain dataset.\n",
    "num_clinic_samples_per_img = np.floor(\n",
    "    len(source_train_dataset.datasets)\n",
    "    * params.samples_per_subj_per_epoch\n",
    "    / len(target_train_dataset.datasets)\n",
    ").astype(int)\n",
    "\n",
    "target_train_sampler = pitn.samplers.ConcatDatasetBalancedRandomSampler(\n",
    "    target_train_dataset.datasets,\n",
    "    max_samples_per_dataset=num_clinic_samples_per_img,\n",
    ")\n",
    "\n",
    "target_train_loader = monai.data.DataLoader(\n",
    "    target_train_dataset,\n",
    "    sampler=target_train_sampler,\n",
    "    batch_size=params.batch_size,\n",
    "    pin_memory=True,\n",
    "    num_workers=7,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b36bd2-f1f6-4b24-b67a-8f5a9ceb3281",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49101a65-8de8-4e51-b935-0950ce3c5393",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClinicMatchGAN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_channels: int,\n",
    "        gen_num_high_freq: int = 3,\n",
    "        discriminator_downsample_factors=[1, 2, 4],\n",
    "        lambda_adversary_loss: float = 1,\n",
    "        lambda_grad_penalty: float = 1,\n",
    "        lambda_reconst_loss_weight=1,\n",
    "        gen_optim_kwargs=dict(),\n",
    "        discriminator_optim_kwargs=dict(),\n",
    "        weight_init_fn=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        if self.hparams.lambda_grad_penalty is None:\n",
    "            self.hparams.use_grad_penalty = False\n",
    "        else:\n",
    "            self.hparams.use_grad_penalty = True\n",
    "        self.generator = pitn.nn.gan.generative.LPTN(\n",
    "            num_channels, num_high_freq_levels=self.hparams.gen_num_high_freq\n",
    "        )\n",
    "\n",
    "        self.discriminator = pitn.nn.gan.adversarial.MultiDiscriminator(\n",
    "            num_channels, self.hparams.discriminator_downsample_factors\n",
    "        )\n",
    "\n",
    "        if weight_init_fn is not None:\n",
    "            self.generator = self.generator.apply(weight_init_fn)\n",
    "            self.discriminator = self.discriminator.apply(weight_init_fn)\n",
    "\n",
    "        self.val_psnr_metric = monai.metrics.PSNRMetric(max_val=1.0)\n",
    "        self.val_viz_slice = None\n",
    "        self.val_viz_range = None\n",
    "\n",
    "        self.plain_log = Box(default_box=True, loss_gen=dict(), loss_discrim=dict())\n",
    "        self.plain_log.discrim_preds.real = dict()\n",
    "        self.plain_log.discrim_preds.fake = dict()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.generator(x)\n",
    "\n",
    "    def reconstruction_loss(self, y_source, y_pred):\n",
    "        return F.mse_loss(y_source, y_pred, reduction=\"mean\")\n",
    "\n",
    "    def ls_adversarial_loss(self, sample, label: int):\n",
    "\n",
    "        sample_pred = self.discriminator(sample)\n",
    "        loss = F.mse_loss(\n",
    "            sample_pred, torch.ones_like(sample_pred) * label, reduction=\"mean\"\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def grad_penalty(\n",
    "        self,\n",
    "        real_samples: torch.Tensor,\n",
    "        fake_samples: torch.Tensor,\n",
    "    ):\n",
    "\n",
    "        batch_size = real_samples.shape[0]\n",
    "        avg_weight_rand = torch.rand(batch_size, *((1,) * (real_samples.ndim - 1))).to(\n",
    "            real_samples\n",
    "        )\n",
    "        # For each sample in the batch, find a randomly-weighted linear interpolation\n",
    "        # between the real and generated/fake samples.\n",
    "        weighted_interpolate = (avg_weight_rand * real_samples) + (\n",
    "            (1 - avg_weight_rand) * fake_samples\n",
    "        )\n",
    "        # Need to require grad for the gradient calculation.\n",
    "        weighted_interp_samples = weighted_interpolate.requires_grad_(True)\n",
    "        pred_interp_samples = self.discriminator(weighted_interp_samples)\n",
    "\n",
    "        grad = torch.autograd.grad(\n",
    "            outputs=pred_interp_samples,\n",
    "            inputs=weighted_interp_samples,\n",
    "            grad_outputs=torch.ones_like(pred_interp_samples),\n",
    "            create_graph=True,\n",
    "            only_inputs=True,\n",
    "            retain_graph=True,\n",
    "        )[0]\n",
    "\n",
    "        grad = grad.view(batch_size, -1)\n",
    "        # Calculate L2 norm manually so a small epsilon can be used to avoid NaNs.\n",
    "        eps = 1e-7\n",
    "        penalty = torch.mean((torch.sqrt(torch.sum((grad**2), dim=1) + eps) - 1) ** 2)\n",
    "\n",
    "        return penalty\n",
    "\n",
    "    def ls_gan_grad_penalty(self, real_samples, noise_scale: float, k=1):\n",
    "        \"\"\"Implements another form of grad penalty from Kodali, et. al., 2017, used in\n",
    "        Mao, et. al., 2018 (2nd LS-GAN paper).\n",
    "        \"\"\"\n",
    "        batch_size = real_samples.shape[0]\n",
    "\n",
    "        # Technically, the original paper specified the noise as a multi-variate Gaussian\n",
    "        # with a diagonal covariance matrix filled with the same value. So, the\n",
    "        # 'c' value in that formulation would scale up the *variance*, while the\n",
    "        # equivalent 1D Normal distribution here specifies the *standard deviation*.\n",
    "        # It probably doesn't matter.\n",
    "        noise_dist = torch.distributions.Normal(0.0, noise_scale)\n",
    "        noise = noise_dist.sample(real_samples.shape).to(real_samples)\n",
    "        noisy_samples = real_samples + noise\n",
    "        # Need to require grad for the gradient calculation.\n",
    "        noisy_samples = noisy_samples.requires_grad_(True)\n",
    "\n",
    "        pred_samples = self.discriminator(noisy_samples)\n",
    "\n",
    "        grad = torch.autograd.grad(\n",
    "            outputs=pred_samples,\n",
    "            inputs=noisy_samples,\n",
    "            grad_outputs=torch.ones_like(pred_samples),\n",
    "            create_graph=True,\n",
    "            only_inputs=True,\n",
    "            retain_graph=True,\n",
    "        )[0]\n",
    "\n",
    "        grad = grad.view(batch_size, -1)\n",
    "        # Calculate L2 norm manually so a small epsilon can be used to avoid NaNs.\n",
    "        eps = 1e-7\n",
    "        penalty = torch.mean((torch.sqrt(torch.sum((grad**2), dim=1) + eps) - k) ** 2)\n",
    "\n",
    "        return penalty\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "\n",
    "        source_samples = batch[\"source\"]\n",
    "        target_samples = batch[\"target\"]\n",
    "\n",
    "        # Optimizer index decides whether this step updates the generator or discriminator.\n",
    "        # Update generator network.\n",
    "        if optimizer_idx == self._GENERATOR_OPTIMIZER_IDX:\n",
    "            translated_samples = self.generator(source_samples)\n",
    "\n",
    "            l_g_reconstruct = self.reconstruction_loss(\n",
    "                source_samples, translated_samples\n",
    "            )\n",
    "            l_g_reconstruct *= self.hparams.lambda_reconst_loss_weight\n",
    "            self.log(\n",
    "                \"train_loss_terms/gen_reconstruct\",\n",
    "                l_g_reconstruct.detach(),\n",
    "            )\n",
    "\n",
    "            l_g_adversarial = self.ls_adversarial_loss(\n",
    "                translated_samples,\n",
    "                label=0,\n",
    "            )\n",
    "            l_g_adversarial *= self.hparams.lambda_adversary_loss * 1 / 2\n",
    "            self.log(\n",
    "                \"train_loss_terms/gen_adversarial\",\n",
    "                l_g_adversarial.detach(),\n",
    "            )\n",
    "\n",
    "            # Combine terms into final loss.\n",
    "            loss_gen = l_g_reconstruct + l_g_adversarial\n",
    "            self.log(\"train/gen_loss\", loss_gen.detach())\n",
    "            # Log loss and set up return dictionary.\n",
    "            self.plain_log.loss_gen[self.global_step] = float(\n",
    "                loss_gen.detach().cpu().item()\n",
    "            )\n",
    "\n",
    "            tqdm_dict = {\"loss_gen\": loss_gen.detach()}\n",
    "            output = collections.OrderedDict(\n",
    "                {\"loss\": loss_gen, \"progress_bar\": tqdm_dict, \"log\": tqdm_dict}\n",
    "            )\n",
    "\n",
    "        ### Update discriminator network.\n",
    "        elif optimizer_idx == self._DISCRIMINATOR_OPTIMIZER_IDX:\n",
    "\n",
    "            # Real images.\n",
    "            loss_real = self.ls_adversarial_loss(target_samples, label=1) / 2\n",
    "            self.log(\"train_loss_terms/discrim_real_loss\", loss_real.detach())\n",
    "\n",
    "            # Translated (i.e., fake) images\n",
    "            # We aren't updating the generator weights here, so there's no need to\n",
    "            # keep track of the generator's gradients.\n",
    "            with torch.no_grad():\n",
    "                translated_samples = self.generator(source_samples)\n",
    "            loss_fake = self.ls_adversarial_loss(translated_samples, label=-1) / 2\n",
    "            self.log(\"train_loss_terms/discrim_fake_loss\", loss_fake.detach())\n",
    "\n",
    "            # Noise scaling found by taking `~ 0.1176 x abs diff between max and min`\n",
    "            # (of values of the input tensors, here the samples from the target domain).\n",
    "            if self.hparams.use_grad_penalty:\n",
    "                grad_penalty = self.ls_gan_grad_penalty(\n",
    "                    target_samples, noise_scale=0.2352\n",
    "                )\n",
    "                grad_penalty *= self.hparams.lambda_grad_penalty\n",
    "                self.log(\n",
    "                    \"train_loss_terms/discrim_grad_penalty\",\n",
    "                    grad_penalty.detach(),\n",
    "                )\n",
    "            else:\n",
    "                grad_penalty = torch.zeros_like(loss_fake)\n",
    "\n",
    "            # Combine terms into final loss value.\n",
    "            loss_discrim = loss_fake + loss_real + grad_penalty\n",
    "            self.log(\"train/discrim_loss\", loss_discrim.detach())\n",
    "\n",
    "            # Record loss and set up return dictionary.\n",
    "            self.plain_log.loss_discrim[self.global_step] = float(\n",
    "                loss_discrim.detach().cpu().item()\n",
    "            )\n",
    "            tqdm_dict = {\"loss_discrim\": loss_discrim.detach()}\n",
    "            output = collections.OrderedDict(\n",
    "                {\"loss\": loss_discrim, \"progress_bar\": tqdm_dict, \"log\": tqdm_dict}\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise RuntimeError(f\"ERROR: Invalid optimizer index {optimizer_idx}\")\n",
    "        # Record discriminator predictions for later plotting.\n",
    "        if self.global_step % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                real_preds = self.discriminator(target_samples)\n",
    "                fake_preds = self.discriminator(translated_samples)\n",
    "                self.plain_log.discrim_preds.real[self.global_step] = torch.clone(\n",
    "                    real_preds.detach().cpu()\n",
    "                )\n",
    "                self.plain_log.discrim_preds.fake[self.global_step] = torch.clone(\n",
    "                    fake_preds.detach().cpu()\n",
    "                )\n",
    "\n",
    "        return output\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        source_sample = batch\n",
    "        source_translate = self.generator(source_sample)\n",
    "        reconstruction_loss = (\n",
    "            self.hparams.lambda_reconst_loss_weight\n",
    "            * self.reconstruction_loss(source_sample, source_translate)\n",
    "        )\n",
    "        adv_loss = (\n",
    "            self.hparams.lambda_adversary_loss\n",
    "            * self.ls_adversarial_loss(source_translate, label=0)\n",
    "            / 2\n",
    "        )\n",
    "\n",
    "        self.log(\"val/reconstruct_loss\", reconstruction_loss.detach())\n",
    "        self.log(\"val/adversarial_loss\", adv_loss.detach())\n",
    "\n",
    "        psnr_loss = self.val_psnr_metric(y_pred=source_translate, y=source_sample)\n",
    "        self.log(\"val/psnr\", psnr_loss.detach())\n",
    "\n",
    "        # Only plot subject translation if batch size of the validation step is 1.\n",
    "        if source_sample.shape[0] == 1:\n",
    "            plot_vol = source_translate[0].cpu()\n",
    "            plot_vol = torch.clip(\n",
    "                plot_vol,\n",
    "                *tuple(torch.quantile(plot_vol, q=torch.as_tensor([0.001, 0.999]))),\n",
    "            )\n",
    "            plot_vol = monai.transforms.utils.rescale_array(\n",
    "                plot_vol, minv=0.0, maxv=255.0\n",
    "            )\n",
    "            monai.visualize.img2tensorboard.add_animated_gif(\n",
    "                image_tensor=plot_vol,\n",
    "                writer=self.logger.experiment,\n",
    "                tag=\"val_subj\",\n",
    "                max_out=1,\n",
    "                scale_factor=1.0,\n",
    "                global_step=self.global_step,\n",
    "            )\n",
    "\n",
    "            # Log a slice of the source, translated, and the abs. error.\n",
    "            fig = plt.figure(dpi=100)\n",
    "\n",
    "            if self.val_viz_slice is None:\n",
    "                self.val_viz_slice = (\n",
    "                    slice(None),\n",
    "                    slice(None),\n",
    "                    (source_translate.shape[-1] // 2) + 2,\n",
    "                )\n",
    "            dtis_to_plot = [\n",
    "                source_sample[0, 0].cpu().numpy(),\n",
    "                source_translate[0, 0].cpu().numpy(),\n",
    "                torch.abs(source_sample[0, 0] - source_translate[0, 0]).cpu().numpy(),\n",
    "            ]\n",
    "            dtis_to_plot = list(map(lambda v: v[self.val_viz_slice], dtis_to_plot))\n",
    "\n",
    "            if self.val_viz_range is None:\n",
    "                vmin = np.min(np.stack(dtis_to_plot))\n",
    "                vmax = np.max(np.stack(dtis_to_plot))\n",
    "                self.val_viz_range = (vmin, vmax)\n",
    "            else:\n",
    "                vmin, vmax = self.val_viz_range\n",
    "            cmap = \"gray\"\n",
    "            grid = mpl_toolkits.axes_grid1.ImageGrid(\n",
    "                fig,\n",
    "                111,\n",
    "                nrows_ncols=(1, 3),\n",
    "                axes_pad=0.1,\n",
    "                share_all=True,\n",
    "                cbar_mode=\"single\",\n",
    "                cbar_location=\"bottom\",\n",
    "                cbar_pad=0.1,\n",
    "            )\n",
    "\n",
    "            map_names = [\"Source\", \"Translated\", \"Abs. Error\"]\n",
    "            for ax, label, dti in zip(grid, map_names, dtis_to_plot):\n",
    "                im = ax.imshow(\n",
    "                    np.rot90(dti), interpolation=None, cmap=cmap, vmin=vmin, vmax=vmax\n",
    "                )\n",
    "                ax.set_xlabel(label)\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                ax.set_xticklabels([])\n",
    "                ax.set_yticklabels([])\n",
    "\n",
    "            grid.cbar_axes[0].colorbar(im)\n",
    "\n",
    "            self.logger.experiment.add_figure(\"val_slice\", fig, self.global_step)\n",
    "\n",
    "        return psnr_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        opt_gen = torch.optim.Adam(\n",
    "            self.generator.parameters(), **self.hparams.gen_optim_kwargs\n",
    "        )\n",
    "        opt_discriminator = torch.optim.Adam(\n",
    "            self.discriminator.parameters(), **self.hparams.discriminator_optim_kwargs\n",
    "        )\n",
    "\n",
    "        self._GENERATOR_OPTIMIZER_IDX = 0\n",
    "        self._DISCRIMINATOR_OPTIMIZER_IDX = 1\n",
    "        return [opt_gen, opt_discriminator], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448d46aa-c344-4b15-8e08-2fdf62124a99",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c31863-63bc-40ae-b62b-62844d75f61a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "train_start_timestamp = datetime.datetime.now().replace(microsecond=0)\n",
    "# Explicitly set whether or not to use grad penalty.\n",
    "lambda_grad_penalty = params.lambda_grad_penalty if params.use_grad_penalty else None\n",
    "\n",
    "# Instantiate model.\n",
    "model = ClinicMatchGAN(\n",
    "    params.num_channels,\n",
    "    gen_num_high_freq=params.num_laplace_high_freq,\n",
    "    discriminator_downsample_factors=params.discriminator_downscale_factors,\n",
    "    lambda_adversary_loss=params.lambda_adversary_loss,\n",
    "    lambda_grad_penalty=lambda_grad_penalty,\n",
    "    lambda_reconst_loss_weight=params.lambda_reconst_loss_weight,\n",
    "    gen_optim_kwargs=params.optim.gen_kwargs,\n",
    "    discriminator_optim_kwargs=params.optim.discriminator_kwargs,\n",
    "    weight_init_fn=params.net_init.f,\n",
    ")\n",
    "\n",
    "with open(log_txt_file, \"a+\") as f:\n",
    "    f.write(f\"Model overview: {model}\\n\")\n",
    "\n",
    "# Create trainer object.\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_epochs=params.max_epochs,\n",
    "    logger=pl_logger,\n",
    "    multiple_trainloader_mode=\"max_size_cycle\",\n",
    "    log_every_n_steps=min([50, len(source_train_loader), len(target_train_loader)]),\n",
    "    check_val_every_n_epoch=3,\n",
    "    #     progress_bar_refresh_rate=10,\n",
    "    terminate_on_nan=True,\n",
    ")\n",
    "\n",
    "# Many warnings are produced here, so it's better for my sanity (and worse in every other\n",
    "# way) to just filter and ignore them...\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    # with torch.autograd.detect_anomaly():\n",
    "    trainer.fit(\n",
    "        model,\n",
    "        train_dataloaders={\n",
    "            \"source\": source_train_loader,\n",
    "            \"target\": target_train_loader,\n",
    "        },\n",
    "        val_dataloaders=source_val_loader,\n",
    "    )\n",
    "\n",
    "train_duration = datetime.datetime.now().replace(microsecond=0) - train_start_timestamp\n",
    "print(f\"Train duration: {train_duration}\")\n",
    "with open(log_txt_file, \"a+\") as f:\n",
    "    f.write(\"\\n\")\n",
    "    f.write(f\"Training time: {train_duration}\\n\")\n",
    "    f.write(\n",
    "        f\"\\t{train_duration.days} Days, \"\n",
    "        + f\"{train_duration.seconds // 3600} Hours,\"\n",
    "        + f\"{(train_duration.seconds // 60) % 60} Minutes,\"\n",
    "        + f'{train_duration.seconds % 60} Seconds\"\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0c93e3-c8a0-4d37-ac76-1af537ecafc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save out trained model\n",
    "trainer.save_checkpoint(str(experiment_results_dir / \"model.ckpt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfd39dc-e76d-440b-8d0f-88ba04abea34",
   "metadata": {},
   "source": [
    "## Result Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95235ef-2354-411d-96dd-59eeeb5997c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "enable_fig_save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f37a3-c7b6-4033-bae2-04fbc6f11501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up visualization objects.\n",
    "viz_data = Box(default_box=True)\n",
    "\n",
    "# Find a common size for all volumes\n",
    "spatial_shapes = list()\n",
    "for subj in itertools.chain(hcp_subj_dataset, clinic_subj_dataset):\n",
    "    spatial_shapes.append(tuple(subj[\"dti\"].shape[-3:]))\n",
    "target_spatial_shape = tuple(np.max(np.asarray(spatial_shapes), axis=0))\n",
    "padder = monai.transforms.SpatialPad(\n",
    "    torch.Size(target_spatial_shape), method=\"symmetric\", mode=\"replicate\"\n",
    ")\n",
    "cropper = monai.transforms.CenterSpatialCrop(torch.Size(target_spatial_shape))\n",
    "\n",
    "# Designate static image from clinical data as the target for registration.\n",
    "clinic_viz_subj_idx = 0\n",
    "# Register the b0 images, then apply the transformation on each DTI channel.\n",
    "static_arr = clinic_subj_dataset[clinic_viz_subj_idx][\"dti\"].cpu().numpy()\n",
    "static_arr = cropper(padder(static_arr))[0]\n",
    "regist_static_img = ants.from_numpy(static_arr, spacing=(2.0, 2.0, 2.0))\n",
    "regist_static_img.set_origin(ants.get_center_of_mass(regist_static_img))\n",
    "# The transform must stay rigid, no scaling or shearing. The translation network is almost\n",
    "# certainly sensitive to feature scale.\n",
    "ants_regist_kwargs = {\"type_of_transform\": \"DenseRigid\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbc014c-dad4-4056-b322-17f5d92a3ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    # Grab HCP data for viz.\n",
    "    for subj in hcp_subj_dataset:\n",
    "        data = Box(default_box=True)\n",
    "\n",
    "        dti = subj[\"dti\"].cpu().numpy()\n",
    "\n",
    "        # Pad and crop DTIs to be the same shape.\n",
    "        dti = np.asarray(cropper(padder(dti)))\n",
    "        # Perform registration to the selected clinical volume.\n",
    "        ants_dti_xx = ants.from_numpy(dti[0], spacing=(2, 2, 2))\n",
    "        dti_center = ants.get_center_of_mass(ants_dti_xx)\n",
    "        ants_dti_xx.set_origin(dti_center)\n",
    "        xx_regist = ants.registration(\n",
    "            regist_static_img, ants_dti_xx, **ants_regist_kwargs\n",
    "        )\n",
    "\n",
    "        dti_registered = list()\n",
    "        for i_ch in range(len(dti)):\n",
    "            dti_chan = dti[i_ch]\n",
    "            dti_chan = ants.from_numpy(dti_chan, spacing=(2, 2, 2), origin=dti_center)\n",
    "            dti_chan = ants.apply_transforms(\n",
    "                regist_static_img, dti_chan, xx_regist[\"fwdtransforms\"]\n",
    "            )\n",
    "            dti_chan = dti_chan.numpy()\n",
    "            dti_registered.append(dti_chan)\n",
    "\n",
    "        dti = torch.from_numpy(np.stack(dti_registered))\n",
    "\n",
    "        data.dti = subj[\"scaler\"].descale(dti.cpu()).numpy()\n",
    "\n",
    "        mask = cropper(padder(subj[\"mask\"].float()))\n",
    "        ants_mask = ants.from_numpy(mask[0], spacing=(2, 2, 2), origin=dti_center)\n",
    "        ants_mask = ants.apply_transforms(\n",
    "            regist_static_img, ants_mask, xx_regist[\"fwdtransforms\"]\n",
    "        )\n",
    "        mask = ants_mask.numpy().astype(bool)\n",
    "        data.mask = mask\n",
    "\n",
    "        translated = model.generator(\n",
    "            dti.to(model.device)[\n",
    "                None,\n",
    "            ]\n",
    "        )[0].cpu()\n",
    "        translated = subj[\"scaler\"].descale(translated).numpy()\n",
    "        data.translated = translated\n",
    "\n",
    "        # Store discriminator prediction.\n",
    "        pred_class = model.discriminator(\n",
    "            dti.to(model.device)[\n",
    "                None,\n",
    "            ]\n",
    "        )[0]\n",
    "        data.pred_class = pred_class.cpu().numpy()\n",
    "\n",
    "        viz_data.hcp[str(subj[\"subj_id\"])] = data\n",
    "\n",
    "    # Grab clinic data for viz.\n",
    "    for subj in clinic_subj_dataset:\n",
    "\n",
    "        dti = subj[\"dti\"]\n",
    "        # Pad and crop DTIs to be the same shape.\n",
    "        dti = torch.from_numpy(cropper(padder(dti)))\n",
    "        data.dti = subj[\"scaler\"].descale(dti.cpu()).numpy()\n",
    "\n",
    "        data.mask = cropper(padder(subj[\"mask\"].float())).astype(bool)\n",
    "\n",
    "        # Store discriminator prediction.\n",
    "        pred_class = model.discriminator(\n",
    "            dti.to(model.device)[\n",
    "                None,\n",
    "            ]\n",
    "        )[0]\n",
    "        data.pred_class = pred_class.cpu().numpy()\n",
    "\n",
    "        viz_data.clinic[str(subj[\"subj_id\"])] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca39a4-4446-4125-8c46-14a753f5bbb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hcp_viz_subj_idx = 1\n",
    "hcp_viz_subj_id = list(viz_data.hcp.keys())[hcp_viz_subj_idx]\n",
    "hcp_viz_subj = viz_data.hcp[hcp_viz_subj_id]\n",
    "\n",
    "clinic_viz_subj_id = list(viz_data.clinic.keys())[clinic_viz_subj_idx]\n",
    "clinic_viz_subj = viz_data.clinic[clinic_viz_subj_id]\n",
    "\n",
    "# 6-channel slice for visualization.\n",
    "# Grab from roughly the center, offset by a few mms.\n",
    "viz_slice = (\n",
    "    slice(None),\n",
    "    (hcp_viz_subj.dti.shape[1] // 2) + 4,\n",
    "    slice(None),\n",
    "    slice(None),\n",
    ")\n",
    "\n",
    "\n",
    "def abs_error_map(y, y_pred):\n",
    "\n",
    "    y = torch.as_tensor(y)\n",
    "    y_pred = torch.as_tensor(y_pred)\n",
    "\n",
    "    error = torch.abs(y - y_pred)\n",
    "\n",
    "    return error.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c8aa7-13eb-4ea2-955c-0b9d00781218",
   "metadata": {},
   "source": [
    "### DTI Comparisons - All Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d310259e-9157-4ecb-a171-2a37f611fc24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "channel_names = [\n",
    "    r\"$D_{x,x}$\",\n",
    "    r\"$D_{x,y}$\",\n",
    "    r\"$D_{y,y}$\",\n",
    "    r\"$D_{x,z}$\",\n",
    "    r\"$D_{y,z}$\",\n",
    "    r\"$D_{z,z}$\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ddc56-9f60-4a16-8e71-d934ac0144db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display all 6 DTIs for the following:\n",
    "# Source domain\n",
    "# Translated\n",
    "# Target domain\n",
    "# Absolute error between source and translated\n",
    "\n",
    "\n",
    "cmap = \"gray\"\n",
    "\n",
    "row_names = [\n",
    "    \"Source HCP\",\n",
    "    \"Translated\",\n",
    "    \"Target Clinic\",\n",
    "    \"Abs Error\\nSource-Translated\",\n",
    "]\n",
    "\n",
    "dti_rows = [\n",
    "    hcp_viz_subj.dti,\n",
    "    hcp_viz_subj.translated,\n",
    "    clinic_viz_subj.dti,\n",
    "    abs_error_map(hcp_viz_subj.dti, hcp_viz_subj.translated),\n",
    "]\n",
    "\n",
    "dti_rows = list(map(lambda a: a[viz_slice], dti_rows))\n",
    "\n",
    "nrows = len(dti_rows)\n",
    "ncols = len(channel_names)\n",
    "\n",
    "# Don't take the absolute max and min values, as there exist some extreme (e.g., > 3\n",
    "# orders of magnitude) outliers. Instead, take some percente quantile.\n",
    "# Reshape and concatenate the dtis in order to compute the quantiles of images with\n",
    "# different shapes (e.g., the low-res input patch).\n",
    "max_dti = np.quantile(\n",
    "    np.concatenate([di.reshape(6, -1) for di in dti_rows], axis=1), 1.0\n",
    ")\n",
    "min_dti = np.quantile(\n",
    "    np.concatenate([di.reshape(6, -1) for di in dti_rows], axis=1), 0.0\n",
    ")\n",
    "\n",
    "# nrows = len(dtis)\n",
    "# ncols = len(channel_names)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 5), dpi=160)\n",
    "\n",
    "grid = mpl.gridspec.GridSpec(\n",
    "    nrows,\n",
    "    ncols,\n",
    "    figure=fig,\n",
    "    hspace=0.05,\n",
    "    wspace=0.05,\n",
    ")\n",
    "axs = list()\n",
    "max_subplot_height = 0\n",
    "for i_row in range(nrows):\n",
    "    dti = dti_rows[i_row]\n",
    "\n",
    "    for j_col in range(ncols):\n",
    "        ax = fig.add_subplot(grid[i_row, j_col])\n",
    "        ax.imshow(\n",
    "            np.rot90(dti[j_col]),\n",
    "            cmap=cmap,\n",
    "            interpolation=None,\n",
    "            vmin=min_dti,\n",
    "            vmax=max_dti,\n",
    "        )\n",
    "        if ax.get_subplotspec().is_first_col():\n",
    "            ax.set_ylabel(row_names[i_row], size=\"xx-small\")\n",
    "        if ax.get_subplotspec().is_last_row():\n",
    "            ax.set_xlabel(channel_names[j_col])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        # Update highest subplot to put the `suptitle` later on.\n",
    "        max_subplot_height = max(\n",
    "            max_subplot_height, ax.get_position(original=False).get_points()[1, 1]\n",
    "        )\n",
    "        axs.append(ax)\n",
    "\n",
    "color_norm = mpl.colors.Normalize(vmin=min_dti, vmax=max_dti)\n",
    "fig.colorbar(\n",
    "    mpl.cm.ScalarMappable(norm=color_norm, cmap=cmap),\n",
    "    ax=axs,\n",
    "    location=\"right\",\n",
    "    fraction=0.1,\n",
    "    pad=0.03,\n",
    ")\n",
    "plt.suptitle(\n",
    "    \"DTI and Abs. Error, Normalized over All Images\",\n",
    "    y=max_subplot_height + 0.015,\n",
    "    verticalalignment=\"bottom\",\n",
    ")\n",
    "if enable_fig_save:\n",
    "    plt.savefig(experiment_results_dir / \"DTI_Channel_w_Abs_Err.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ea833f-8a7a-4d4d-9d54-53fbb83ea80f",
   "metadata": {},
   "source": [
    "## Diffusion Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333f3942-cbf5-4506-84e0-b94cc3392ca7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=3, sharey=True, dpi=160)  # , figsize=(2, 6))\n",
    "\n",
    "diffusion_viz_slice = (slice(None), slice(None), (target_spatial_shape[-1] // 2) + 8)\n",
    "\n",
    "map_names = [\"Source\", \"Translated\", \"Target\"]\n",
    "for ax, label, dti, mask in zip(\n",
    "    axs,\n",
    "    map_names,\n",
    "    [hcp_viz_subj.dti, hcp_viz_subj.translated, clinic_viz_subj.dti],\n",
    "    [hcp_viz_subj.mask, hcp_viz_subj.mask, clinic_viz_subj.mask],\n",
    "):\n",
    "    diff_dir_map = pitn.viz.direction_map(dti * mask)\n",
    "    # Set channels last for matplotlib\n",
    "    diff_dir_map = diff_dir_map.transpose(1, 2, 3, 0)\n",
    "\n",
    "    ax.imshow(np.rot90(diff_dir_map[diffusion_viz_slice]), interpolation=None)\n",
    "    ax.set_xlabel(label)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "if enable_fig_save:\n",
    "    plt.savefig(experiment_results_dir / \"Diff_Color_Maps.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8391ae69-af3d-422d-89c3-c76febe9d74d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74158e75-a06b-49f6-87fa-92fe13b70db4",
   "metadata": {},
   "source": [
    "## End Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218ba5bc-3518-468f-95c3-0e2a094d94eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pl_logger.experiment.flush()\n",
    "# Close tensorboard logger.\n",
    "# Don't finalize if the experiment was for debugging.\n",
    "if \"debug\" not in EXPERIMENT_NAME.casefold():\n",
    "    pl_logger.finalize(\"success\")\n",
    "    # Experiment is complete, move the results directory to its final location.\n",
    "    if experiment_results_dir != final_experiment_results_dir:\n",
    "        print(\"Moving out of tmp location\")\n",
    "        experiment_results_dir = experiment_results_dir.rename(\n",
    "            final_experiment_results_dir\n",
    "        )\n",
    "        log_txt_file = experiment_results_dir / log_txt_file.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c074eb0-56aa-473b-97ea-7809f154571d",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda-pitn]",
   "language": "python",
   "name": "conda-env-miniconda-pitn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
