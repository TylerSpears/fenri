{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13edb204-c964-4525-80d8-0fdd2730df3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pain in the Net - LPTN Gan on T1w Images\n",
    "Application of Laplacian Pyramid Translation Network (LPTN) to domain adaptation of diffusion MRI.\n",
    "\n",
    "\n",
    "Code by:\n",
    "\n",
    "Tyler Spears - tas6hh@virginia.edu\n",
    "\n",
    "Dr. Tom Fletcher\n",
    "\n",
    "---\n",
    "\n",
    "Based on the following work(s):\n",
    "\n",
    "* `J. Liang, H. Zeng, and L. Zhang, “High-Resolution Photorealistic Image Translation in Real-Time: A Laplacian Pyramid Translation Network,” 2021, pp. 9392–9400. Accessed: Aug. 26, 2021. [Online]. Available: https://openaccess.thecvf.com/content/CVPR2021/html/Liang_High-Resolution_Photorealistic_Image_Translation_in_Real-Time_A_Laplacian_Pyramid_Translation_CVPR_2021_paper.html\n",
    "`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da03c5c0-aea8-4c1c-98ca-bf001e59dde2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports & Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6f91f0-eca2-4990-9214-f34aa16b4d05",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bbe9cc-5b3f-4204-870d-4a5bc5f28aab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automatically re-import project-specific modules.\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# imports\n",
    "import collections\n",
    "import functools\n",
    "import io\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "import copy\n",
    "import pdb\n",
    "import inspect\n",
    "import random\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import typing\n",
    "import zipfile\n",
    "\n",
    "import ants\n",
    "import dipy\n",
    "import dipy.core\n",
    "import dipy.reconst\n",
    "import dipy.reconst.dti\n",
    "import dipy.segment.mask\n",
    "import dipy.viz\n",
    "import dipy.viz.regtools\n",
    "import dotenv\n",
    "\n",
    "# visualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import mpl_toolkits\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import IPython\n",
    "\n",
    "# Try importing GPUtil for printing GPU specs.\n",
    "# May not be installed if using CPU only.\n",
    "try:\n",
    "    import GPUtil\n",
    "except ImportError:\n",
    "    warnings.warn(\"WARNING: Package GPUtil not found, cannot print GPU specs\")\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, Markdown\n",
    "import ipyplot\n",
    "\n",
    "# Data management libraries.\n",
    "import nibabel as nib\n",
    "import natsort\n",
    "from natsort import natsorted\n",
    "import addict\n",
    "from addict import Addict\n",
    "import box\n",
    "from box import Box\n",
    "import pprint\n",
    "from pprint import pprint as ppr\n",
    "\n",
    "# Computation & ML libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchio\n",
    "import pytorch_lightning as pl\n",
    "import monai\n",
    "\n",
    "import skimage\n",
    "import skimage.feature\n",
    "import skimage.filters\n",
    "import skimage.measure\n",
    "import scipy\n",
    "\n",
    "plt.rcParams.update({\"figure.autolayout\": True})\n",
    "plt.rcParams.update({\"figure.facecolor\": [1.0, 1.0, 1.0, 1.0]})\n",
    "\n",
    "# Set print options for ndarrays/tensors.\n",
    "np.set_printoptions(suppress=True, edgeitems=2, threshold=100, linewidth=88)\n",
    "torch.set_printoptions(\n",
    "    sci_mode=False, edgeitems=2, threshold=100, linewidth=88, profile=\"short\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b8326b-55e1-4195-a85f-7627f591f1f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update notebook's environment variables with direnv.\n",
    "# This requires the python-dotenv package, and direnv be installed on the system\n",
    "# This will not work on Windows.\n",
    "# NOTE: This is kind of hacky, and not necessarily safe. Be careful...\n",
    "# Libraries needed on the python side:\n",
    "# - os\n",
    "# - subprocess\n",
    "# - io\n",
    "# - dotenv\n",
    "\n",
    "# Form command to be run in direnv's context. This command will print out\n",
    "# all environment variables defined in the subprocess/sub-shell.\n",
    "command = f\"direnv exec {os.getcwd()} /usr/bin/env\"\n",
    "# Run command in a new subprocess.\n",
    "proc = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, cwd=os.getcwd())\n",
    "# Store and format the subprocess' output.\n",
    "proc_out = proc.communicate()[0].strip().decode(\"utf-8\")\n",
    "# Use python-dotenv to load the environment variables by using the output of\n",
    "# 'direnv exec ...' as a 'dummy' .env file.\n",
    "dotenv.load_dotenv(stream=io.StringIO(proc_out), override=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8449ea-9648-4517-8aab-7e85308436de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Project-specific scripts\n",
    "# It's easier to import it this way rather than make an entirely new package, due to\n",
    "# conflicts with local packages and anaconda installations.\n",
    "# You made me do this, poor python package management!!\n",
    "if \"PROJECT_ROOT\" in os.environ:\n",
    "    lib_location = str(Path(os.environ[\"PROJECT_ROOT\"]).resolve())\n",
    "else:\n",
    "    lib_location = str(Path(\"../../\").resolve())\n",
    "if lib_location not in sys.path:\n",
    "    sys.path.insert(0, lib_location)\n",
    "import lib as pitn\n",
    "\n",
    "# Include the top-level lib module along with its submodules.\n",
    "%aimport lib\n",
    "# Grab all submodules of lib, not including modules outside of the package.\n",
    "includes = list(\n",
    "    filter(\n",
    "        lambda m: m.startswith(\"lib.\"),\n",
    "        map(lambda x: x[1].__name__, inspect.getmembers(pitn, inspect.ismodule)),\n",
    "    )\n",
    ")\n",
    "# Run aimport magic with constructed includes.\n",
    "ipy = IPython.get_ipython()\n",
    "ipy.run_line_magic(\"aimport\", \", \".join(includes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a706635-b544-4f41-886b-d6997dda27e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch setup\n",
    "# allow for CUDA usage, if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# keep device as the cpu\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498af6cf-4263-4415-b6f3-fd32abab4bcb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Specs Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff2952b-b4a3-4241-9322-e317c32cb2db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr cap\n",
    "# Capture output and save to log. Needs to be at the *very first* line of the cell.\n",
    "# Watermark\n",
    "%load_ext watermark\n",
    "%watermark --author \"Tyler Spears\" --updated --iso8601  --python --machine --iversions --githash\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    # GPU information\n",
    "    # Taken from\n",
    "    # <https://www.thepythoncode.com/article/get-hardware-system-information-python>.\n",
    "    # If GPUtil is not installed, skip this step.\n",
    "    try:\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        print(\"=\" * 50, \"GPU Specs\", \"=\" * 50)\n",
    "        list_gpus = []\n",
    "        for gpu in gpus:\n",
    "            # get the GPU id\n",
    "            gpu_id = gpu.id\n",
    "            # name of GPU\n",
    "            gpu_name = gpu.name\n",
    "            driver_version = gpu.driver\n",
    "            cuda_version = torch.version.cuda\n",
    "            # get total memory\n",
    "            gpu_total_memory = f\"{gpu.memoryTotal}MB\"\n",
    "            gpu_uuid = gpu.uuid\n",
    "            list_gpus.append(\n",
    "                (\n",
    "                    gpu_id,\n",
    "                    gpu_name,\n",
    "                    driver_version,\n",
    "                    cuda_version,\n",
    "                    gpu_total_memory,\n",
    "                    gpu_uuid,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            tabulate(\n",
    "                list_gpus,\n",
    "                headers=(\n",
    "                    \"id\",\n",
    "                    \"Name\",\n",
    "                    \"Driver Version\",\n",
    "                    \"CUDA Version\",\n",
    "                    \"Total Memory\",\n",
    "                    \"uuid\",\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    except NameError:\n",
    "        print(\"CUDA Version: \", torch.version.cuda)\n",
    "\n",
    "else:\n",
    "    print(\"CUDA not in use, falling back to CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae38f0a-8614-4e0d-a40a-bee5a3585baa",
   "metadata": {
    "tags": [
     "keep_output"
    ]
   },
   "outputs": [],
   "source": [
    "# cap is defined in an ipython magic command\n",
    "print(cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09447c83-298e-444a-91ca-aa46058eb956",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Variables & Definitions Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc41112b-67dc-4fa3-828e-48a2c88dfb49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up directories\n",
    "data_dir = pathlib.Path(os.environ[\"DATA_DIR\"])\n",
    "\n",
    "processed_data_dir = pathlib.Path(os.environ[\"WRITE_DATA_DIR\"])\n",
    "hcp_processed_data_dir = (\n",
    "    processed_data_dir / \"hcp/derivatives/mean-downsample/scale-1.00mm\"\n",
    ")\n",
    "clinic_processed_data_dir = (\n",
    "    processed_data_dir / \"oasis3/derivatives/mean-downsample/scale-orig\"\n",
    ")\n",
    "assert hcp_processed_data_dir.exists() and clinic_processed_data_dir.exists()\n",
    "results_dir = pathlib.Path(os.environ[\"RESULTS_DIR\"])\n",
    "assert results_dir.exists()\n",
    "tmp_results_dir = pathlib.Path(os.environ[\"TMP_RESULTS_DIR\"])\n",
    "assert tmp_results_dir.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7a7233-48c1-439d-af3f-62f628af527c",
   "metadata": {},
   "source": [
    "### Experiment Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fdc6f9-42fc-4ce6-905e-c938b89edb0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tensorboard experiment logging setup.\n",
    "EXPERIMENT_NAME = \"debug_t1w_64_cube_patches\"\n",
    "\n",
    "ts = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "# Break ISO format because many programs don't like having colons ':' in a filename.\n",
    "ts = ts.replace(\":\", \"_\")\n",
    "experiment_name = ts + \"__\" + EXPERIMENT_NAME\n",
    "run_name = experiment_name\n",
    "print(experiment_name)\n",
    "# experiment_results_dir = results_dir / experiment_name\n",
    "\n",
    "# Create temporary directory for results directory, in case experiment does not finish.\n",
    "tmp_dirs = list(filter(lambda s: not str(s).startswith(\".\"), tmp_results_dir.glob(\"*\")))\n",
    "\n",
    "# Only keep up to N tmp results.\n",
    "n_tmp_to_keep = 3\n",
    "if len(tmp_dirs) > (n_tmp_to_keep - 1):\n",
    "    print(f\"More than {n_tmp_to_keep} temporary results, culling to the most recent\")\n",
    "    tmps_to_delete = natsorted([str(tmp_dir) for tmp_dir in tmp_dirs])[\n",
    "        : -(n_tmp_to_keep - 1)\n",
    "    ]\n",
    "    for tmp_dir in tmps_to_delete:\n",
    "        shutil.rmtree(tmp_dir)\n",
    "        print(\"Deleted temporary results directory \", tmp_dir)\n",
    "\n",
    "experiment_results_dir = tmp_results_dir / experiment_name\n",
    "# Final target directory, to be made when experiment is complete.\n",
    "final_experiment_results_dir = results_dir / experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe6460c-da7e-4391-980a-344508313cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pass this object into the pytorchlightning Trainer object, for easier logging within\n",
    "# the training/testing loops.\n",
    "pl_logger = pl.loggers.TensorBoardLogger(\n",
    "    tmp_results_dir,\n",
    "    name=experiment_name,\n",
    "    version=\"\",\n",
    "    log_graph=False,\n",
    "    default_hp_metric=False,\n",
    ")\n",
    "# Use the lower-level logger for logging histograms, images, etc.\n",
    "logger = pl_logger.experiment\n",
    "\n",
    "# Create a separate txt file to log streams of events & info besides parameters & results.\n",
    "log_txt_file = Path(logger.log_dir) / \"log.txt\"\n",
    "with open(log_txt_file, \"a+\") as f:\n",
    "    f.write(f\"Experiment Name: {experiment_name}\\n\")\n",
    "    f.write(f\"Timestamp: {ts}\\n\")\n",
    "    # cap is defined in an ipython magic command\n",
    "    f.write(f\"Environment and Hardware Info:\\n {cap}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82e4788-bced-42d5-b341-3652317f2427",
   "metadata": {},
   "source": [
    "### Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb06aa61-9f96-4b68-92c9-4a54bf351870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = Box(default_box=True)\n",
    "\n",
    "# Data params.\n",
    "params.num_channels = 1\n",
    "params.hcp.num_subjects = 13\n",
    "params.clinic.num_subjects = 9\n",
    "params.clamp_percentiles = (0.05, 95.0)\n",
    "# params.data_scale_range = None\n",
    "# Scale input data by the valid values of each channel of the vol.\n",
    "# I.e., Dx,x in [0, 1], Dx,y in [-1, 1], Dy,y in [0, 1], Dy,z in [-1, 1], etc.\n",
    "params.data_scale_range = ((-1), (1))\n",
    "\n",
    "# Network params.\n",
    "params.num_laplace_high_freq = 3\n",
    "params.discriminator_downscale_factors = [1, 2, 4]\n",
    "params.lambda_adversary_loss = 1\n",
    "params.lambda_reconst_loss_weight = 10\n",
    "params.use_grad_penalty = False\n",
    "params.lambda_grad_penalty = 100\n",
    "# Set the init function to None to change to pytorch default initialization.\n",
    "# params.net_init.f = None\n",
    "params.net_init.mean = 0.0\n",
    "params.net_init.std = 0.02\n",
    "\n",
    "# Adam optimizer kwargs for each network.\n",
    "params.optim.gen_kwargs.lr = 2e-4\n",
    "params.optim.gen_kwargs.betas = (0.5, 0.99)\n",
    "params.optim.discriminator_kwargs.lr = 2e-4\n",
    "params.optim.discriminator_kwargs.betas = (0.5, 0.99)\n",
    "\n",
    "# Training, validation, & testing params\n",
    "# Patch size must be a factor of 2**num_laplace_high_freq\n",
    "params.train.patch_size = (64, 64, 64)\n",
    "params.batch_size = 8\n",
    "params.samples_per_subj_per_epoch = 300\n",
    "params.max_epochs = 50\n",
    "params.train.hcp_num_subjects = 12\n",
    "params.val.hcp_num_subjects = 1\n",
    "# Data augmentation parameters\n",
    "# params.train.aug.input_noise_dist = None\n",
    "params.train.aug.input_noise_dist = torch.distributions.normal.Normal(0, 0.05)\n",
    "# params.train.aug.label_noise_dist = None\n",
    "params.train.aug.label_noise_dist = torch.distributions.uniform.Uniform(-0.3, 0.3)\n",
    "\n",
    "# Create these assert statements because having an invalid number of train/val subjects\n",
    "# may not be caught in the loading below and cause a silent runtime error.\n",
    "assert params.train.hcp_num_subjects <= params.hcp.num_subjects\n",
    "assert params.val.hcp_num_subjects <= params.hcp.num_subjects\n",
    "\n",
    "with open(log_txt_file, \"a+\") as f:\n",
    "    f.write(pprint.pformat(params) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c39e21-8cf5-4f2e-b715-5bf8d310d653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optional weight & bias initialization of conv layers.\n",
    "@torch.no_grad()\n",
    "def conv_init_normal(m, mean, std):\n",
    "    if isinstance(m, (torch.nn.Conv3d, torch.nn.ConvTranspose3d)):\n",
    "        torch.nn.init.normal_(m.weight, mean=mean, std=std)\n",
    "        torch.nn.init.normal_(m.bias, mean=mean, std=std)\n",
    "\n",
    "\n",
    "if \"mean\" in params.net_init and \"std\" in params.net_init:\n",
    "    f = functools.partial(\n",
    "        conv_init_normal, mean=params.net_init.mean, std=params.net_init.std\n",
    "    )\n",
    "    params.net_init.f = f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1009ab0f-71cd-4692-8722-e6fa947366d1",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ea665-dc8b-45b8-be6f-b9cc3c5f62b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transformation pipeline.\n",
    "# The input to the laplacian pyramid must be divisible by 2 for the number of high-\n",
    "# frequency levels in the pyramid.\n",
    "laplace_pyramid_divisible_by_shape = 2**params.num_laplace_high_freq\n",
    "\n",
    "pre_process_pipeline = monai.transforms.Compose(\n",
    "    [\n",
    "        monai.transforms.CropForegroundd([\"t1w\", \"mask\"], source_key=\"mask\", margin=3),\n",
    "        monai.transforms.DivisiblePadd(\n",
    "            [\"t1w\", \"mask\"], laplace_pyramid_divisible_by_shape\n",
    "        ),\n",
    "        monai.transforms.ToTensord(\"t1w\", dtype=torch.float),\n",
    "        monai.transforms.ToTensord(\"mask\", dtype=torch.float),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03506296-ec92-4083-93c1-83308f181847",
   "metadata": {},
   "source": [
    "### Load and Pre-Process HCP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac5a22-bd1a-41e3-ac99-3d4cdb1ee14c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find data directories for each subject.\n",
    "hcp_subj_dirs: dict = dict()\n",
    "\n",
    "possible_ids = [\n",
    "    \"sub-397154\",\n",
    "    \"sub-224022\",\n",
    "    \"sub-140117\",\n",
    "    \"sub-751348\",\n",
    "    \"sub-894774\",\n",
    "    \"sub-156637\",\n",
    "    \"sub-227432\",\n",
    "    \"sub-303624\",\n",
    "    \"sub-185947\",\n",
    "    \"sub-810439\",\n",
    "    \"sub-753251\",\n",
    "    \"sub-644246\",\n",
    "    \"sub-141422\",\n",
    "    \"sub-135528\",\n",
    "    \"sub-103010\",\n",
    "    \"sub-700634\",\n",
    "]\n",
    "\n",
    "## Sub-set the chosen participants for dev and debugging!\n",
    "selected_ids = random.sample(possible_ids, params.hcp.num_subjects)\n",
    "if params.hcp.num_subjects < len(possible_ids):\n",
    "    warnings.warn(\n",
    "        \"WARNING: Sub-selecting participants for dev and debugging. \"\n",
    "        + f\"Subj IDs selected: {selected_ids}\"\n",
    "    )\n",
    "# ### A nested warning! For debugging only.\n",
    "# warnings.warn(\"WARNING: Mixing training and testing subjects\")\n",
    "# selected_ids.append(selected_ids[0])\n",
    "# ###\n",
    "##\n",
    "\n",
    "selected_ids = natsorted(selected_ids)\n",
    "\n",
    "for subj_id in selected_ids:\n",
    "    hcp_subj_dirs[subj_id] = hcp_processed_data_dir / f\"{subj_id}\"\n",
    "    assert hcp_subj_dirs[subj_id].exists()\n",
    "ppr(hcp_subj_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f5408d-5934-44cc-9aba-01df3461e7ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log to file and experiment.\n",
    "with open(log_txt_file, \"a+\") as f:\n",
    "    f.write(f\"Selected HCP Subjects: {selected_ids}\\n\")\n",
    "\n",
    "logger.add_text(\"hcp_subjs\", pprint.pformat(selected_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f30dbd-f4d2-4144-9ba1-6dbf9097b441",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data loading and processing loop.\n",
    "hcp_subj_data = list()\n",
    "# Data reader object for NIFTI files.\n",
    "nib_reader = monai.data.NibabelReader(as_closest_canonical=True)\n",
    "\n",
    "# Directory prefixes for each image to be read.\n",
    "t1w_file_prefix = \"anat\"\n",
    "mask_file_prefix = \"mask\"\n",
    "\n",
    "for subj_id, subj_dir in hcp_subj_dirs.items():\n",
    "    subj_data = dict()\n",
    "    subj_data[\"subj_id\"] = subj_id\n",
    "\n",
    "    # Load the T1s\n",
    "    img_dir = subj_dir / t1w_file_prefix\n",
    "    img_filename = list(img_dir.glob(f\"{subj_id}*T1w*.nii.gz\"))\n",
    "    # Make sure the glob pattern only matches one file.\n",
    "    assert len(img_filename) == 1\n",
    "    img_filename = img_filename[0]\n",
    "    nib_img = nib_reader.read(img_filename)\n",
    "    img, metadata = nib_reader.get_data(nib_img)\n",
    "    # Add channel dimension if not found.\n",
    "    if len(img.shape) == 3:\n",
    "        img = img[\n",
    "            None,\n",
    "        ]\n",
    "    subj_data[\"t1w\"] = img\n",
    "    # The default metadata key name for monai.\n",
    "    subj_data[\"t1w_meta_dict\"] = metadata\n",
    "\n",
    "    # Load masks\n",
    "    img_dir = subj_dir / mask_file_prefix\n",
    "    img_filename = list(img_dir.glob(f\"{subj_id}*mask*.nii.gz\"))\n",
    "    # Make sure the glob pattern only matches one file.\n",
    "    assert len(img_filename) == 1\n",
    "    img_filename = img_filename[0]\n",
    "    nib_img = nib_reader.read(img_filename)\n",
    "    img, metadata = nib_reader.get_data(nib_img)\n",
    "    if len(img.shape) == 3:\n",
    "        img = img[\n",
    "            None,\n",
    "        ]\n",
    "    subj_data[\"mask\"] = img\n",
    "    # The default metadata key name for monai.\n",
    "    subj_data[\"mask_meta_dict\"] = metadata\n",
    "\n",
    "    # Pre-process subject vols.\n",
    "    subj_data = pre_process_pipeline(subj_data)\n",
    "\n",
    "    # Perform scaling of input data?\n",
    "    if params.data_scale_range is not None:\n",
    "        scaler = pitn.data.norm.DTIMinMaxScaler(\n",
    "            params.data_scale_range[0],\n",
    "            params.data_scale_range[1],\n",
    "            quantile_low=params.clamp_percentiles[0] / 100,\n",
    "            quantile_high=params.clamp_percentiles[1] / 100,\n",
    "            dim=(1, 2, 3),\n",
    "            channel_size=params.num_channels,\n",
    "            clip=True,\n",
    "        )\n",
    "        scaled = scaler.scale(subj_data[\"t1w\"] * subj_data[\"mask\"], stateful=True)\n",
    "        subj_data[\"t1w\"] = scaled * subj_data[\"mask\"]\n",
    "        subj_data[\"scaler\"] = scaler\n",
    "\n",
    "    hcp_subj_data.append(subj_data)\n",
    "\n",
    "# Create dataset with all HCP subjects included.\n",
    "hcp_subj_dataset = monai.data.Dataset(hcp_subj_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f3d4d9-333b-464e-be59-485d8de80582",
   "metadata": {},
   "source": [
    "### Load & Pre-Process Clinical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4cda4b-7289-4b78-bb67-88e2c3d3d67d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find data directories for each subject.\n",
    "clinic_subj_dirs: dict = dict()\n",
    "\n",
    "possible_ids = [\n",
    "    \"sub-OAS30188_MR_d3844\",\n",
    "    \"sub-OAS30375_MR_d5792\",\n",
    "    \"sub-OAS30558_MR_d2148\",\n",
    "    \"sub-OAS30643_MR_d0280\",\n",
    "    \"sub-OAS30685_MR_d0032\",\n",
    "    \"sub-OAS30762_MR_d0043\",\n",
    "    \"sub-OAS30770_MR_d1201\",\n",
    "    \"sub-OAS30944_MR_d0089\",\n",
    "    \"sub-OAS31018_MR_d0041\",\n",
    "    \"sub-OAS31157_MR_d4924\",\n",
    "]\n",
    "\n",
    "## Sub-set the chosen participants for dev and debugging!\n",
    "selected_ids = random.sample(possible_ids, params.clinic.num_subjects)\n",
    "if params.clinic.num_subjects < len(possible_ids):\n",
    "    warnings.warn(\n",
    "        \"WARNING: Sub-selecting participants for dev and debugging. \"\n",
    "        + f\"Subj IDs selected: {selected_ids}\"\n",
    "    )\n",
    "# ### A nested warning! For debugging only.\n",
    "# warnings.warn(\"WARNING: Mixing training and testing subjects\")\n",
    "# selected_ids.append(selected_ids[0])\n",
    "# ###\n",
    "##\n",
    "\n",
    "selected_ids = natsorted(selected_ids)\n",
    "\n",
    "for subj_id in selected_ids:\n",
    "    clinic_subj_dirs[subj_id] = clinic_processed_data_dir / f\"{subj_id}\"\n",
    "    assert clinic_subj_dirs[subj_id].exists()\n",
    "ppr(clinic_subj_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8061c3-dc01-49e2-a6c5-ecff830766de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OASIS3 Dataset\n",
    "\n",
    "# # Find data directories for each subject.\n",
    "# clinic_subj_dirs: dict = dict()\n",
    "\n",
    "# possible_ids = [\n",
    "#     \"OAS30052\",\n",
    "#     \"OAS30058\",\n",
    "#     \"OAS30132\",\n",
    "#     \"OAS30176\",\n",
    "#     \"OAS30180\",\n",
    "#     \"OAS30203\",\n",
    "#     \"OAS30233\",\n",
    "#     \"OAS30257\",\n",
    "#     \"OAS30261\",\n",
    "#     \"OAS30719\",\n",
    "#     \"OAS30733\",\n",
    "#     \"OAS30748\",\n",
    "#     \"OAS30959\",\n",
    "#     \"OAS31084\",\n",
    "#     \"OAS31088\",\n",
    "#     \"OAS31111\",\n",
    "# ]\n",
    "\n",
    "# ## Sub-set the chosen participants for dev and debugging!\n",
    "# selected_ids = random.sample(possible_ids, params.clinic.num_subjects)\n",
    "# if params.clinic.num_subjects < len(possible_ids):\n",
    "#     warnings.warn(\n",
    "#         \"WARNING: Sub-selecting participants for dev and debugging. \"\n",
    "#         + f\"Subj IDs selected: {selected_ids}\"\n",
    "#     )\n",
    "# # ### A nested warning! For debugging only.\n",
    "# # warnings.warn(\"WARNING: Mixing training and testing subjects\")\n",
    "# # selected_ids.append(selected_ids[0])\n",
    "# # ###\n",
    "# ##\n",
    "\n",
    "# selected_ids = natsorted(selected_ids)\n",
    "\n",
    "# for subj_id in selected_ids:\n",
    "#     clinic_subj_dirs[subj_id] = (\n",
    "#         clinic_processed_data_dir / f\"derivatives/diffusion/sub-{subj_id}/ses-01\"\n",
    "#     )\n",
    "#     assert clinic_subj_dirs[subj_id].exists()\n",
    "# ppr(clinic_subj_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166fa58d-b619-4a17-8440-43f15f2f5c50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log to file and experiment.\n",
    "with open(log_txt_file, \"a+\") as f:\n",
    "    f.write(f\"Selected Clinically-Scanned Subjects: {selected_ids}\\n\")\n",
    "\n",
    "logger.add_text(\"clinic_data_subjs\", pprint.pformat(selected_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed602676-361b-49cb-af88-c7976f653e13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data loading and processing loop.\n",
    "clinic_subj_data = list()\n",
    "# Data reader object for NIFTI files.\n",
    "nib_reader = monai.data.NibabelReader(as_closest_canonical=True)\n",
    "\n",
    "# Directory prefixes for each image to be read.\n",
    "t1w_file_prefix = \"anat\"\n",
    "mask_file_prefix = \"mask\"\n",
    "\n",
    "for subj_id, subj_dir in clinic_subj_dirs.items():\n",
    "    subj_data = dict()\n",
    "    subj_data[\"subj_id\"] = subj_id\n",
    "\n",
    "    # Load the T1s\n",
    "    img_dir = subj_dir / t1w_file_prefix\n",
    "    img_filename = list(img_dir.glob(f\"{subj_id}*T1w*.nii.gz\"))\n",
    "    # Make sure the glob pattern only matches one file.\n",
    "    assert len(img_filename) == 1\n",
    "    img_filename = img_filename[0]\n",
    "    nib_img = nib_reader.read(img_filename)\n",
    "    img, metadata = nib_reader.get_data(nib_img)\n",
    "    # Add channel dimension if not found.\n",
    "    if len(img.shape) == 3:\n",
    "        img = img[\n",
    "            None,\n",
    "        ]\n",
    "    subj_data[\"t1w\"] = img\n",
    "    # The default metadata key name for monai.\n",
    "    subj_data[\"t1w_meta_dict\"] = metadata\n",
    "\n",
    "    # Load masks\n",
    "    img_dir = subj_dir / mask_file_prefix\n",
    "    img_filename = list(img_dir.glob(f\"{subj_id}*mask*.nii.gz\"))\n",
    "    # Make sure the glob pattern only matches one file.\n",
    "    assert len(img_filename) == 1\n",
    "    img_filename = img_filename[0]\n",
    "    nib_img = nib_reader.read(img_filename)\n",
    "    img, metadata = nib_reader.get_data(nib_img)\n",
    "    if len(img.shape) == 3:\n",
    "        img = img[\n",
    "            None,\n",
    "        ]\n",
    "    subj_data[\"mask\"] = img\n",
    "    # The default metadata key name for monai.\n",
    "    subj_data[\"mask_meta_dict\"] = metadata\n",
    "\n",
    "    # Pre-process subject vols.\n",
    "    subj_data = pre_process_pipeline(subj_data)\n",
    "\n",
    "    # Perform scaling of input data?\n",
    "    if params.data_scale_range is not None:\n",
    "        scaler = pitn.data.norm.DTIMinMaxScaler(\n",
    "            params.data_scale_range[0],\n",
    "            params.data_scale_range[1],\n",
    "            quantile_low=params.clamp_percentiles[0] / 100,\n",
    "            quantile_high=params.clamp_percentiles[1] / 100,\n",
    "            dim=(1, 2, 3),\n",
    "            channel_size=params.num_channels,\n",
    "            clip=True,\n",
    "        )\n",
    "        scaled = scaler.scale(subj_data[\"t1w\"] * subj_data[\"mask\"], stateful=True)\n",
    "        subj_data[\"t1w\"] = scaled * subj_data[\"mask\"]\n",
    "        subj_data[\"scaler\"] = scaler\n",
    "\n",
    "    clinic_subj_data.append(subj_data)\n",
    "\n",
    "# Create dataset with all \"clinical quality\" subjects included.\n",
    "clinic_subj_dataset = monai.data.Dataset(clinic_subj_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0181623e-5e4d-4253-a51b-9591eecba91a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup of Training Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f3429-086e-4cc6-8da5-d06b2e27ed4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Designate HCP subjects for training, validation, and testing.\n",
    "hcp_ids = [s[\"subj_id\"] for s in hcp_subj_data]\n",
    "random.shuffle(hcp_ids)\n",
    "hcp_train_ids = hcp_ids[: params.train.hcp_num_subjects]\n",
    "hcp_val_ids = hcp_ids[: params.val.hcp_num_subjects]\n",
    "\n",
    "# Designate clinic subject IDs for training.\n",
    "clinic_ids = [s[\"subj_id\"] for s in clinic_subj_data]\n",
    "random.shuffle(clinic_ids)\n",
    "# Just select all clinic IDs.\n",
    "clinic_train_ids = clinic_ids[: params.clinic.num_subjects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0b36e9-963c-4c66-a99a-33ffee3c7fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up dataset and data loading objects.\n",
    "# ! The samplers created here will cause the source domain patches and the target domain\n",
    "# patches to *not* be aligned in any way; this is intentional for unpaired I2I.\n",
    "\n",
    "# Set up HCP scan data.\n",
    "# Training set.\n",
    "source_patch_ds = list()\n",
    "for subj_dict in filter(lambda s: s[\"subj_id\"] in hcp_train_ids, hcp_subj_data):\n",
    "    source_patch_ds.append(\n",
    "        pitn.data.MaskFilteredPatchDataset3d(\n",
    "            subj_dict[\"t1w\"], mask=subj_dict[\"mask\"], patch_size=params.train.patch_size\n",
    "        )\n",
    "    )\n",
    "\n",
    "source_train_dataset = torch.utils.data.ConcatDataset(source_patch_ds)\n",
    "source_train_sampler = pitn.samplers.ConcatDatasetBalancedRandomSampler(\n",
    "    source_train_dataset.datasets,\n",
    "    max_samples_per_dataset=params.samples_per_subj_per_epoch,\n",
    ")\n",
    "\n",
    "source_train_loader = monai.data.DataLoader(\n",
    "    source_train_dataset,\n",
    "    sampler=source_train_sampler,\n",
    "    batch_size=params.batch_size,\n",
    "    pin_memory=True,\n",
    "    num_workers=7,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "# Validation set.\n",
    "source_vol_ds = list()\n",
    "for subj_dict in filter(lambda s: s[\"subj_id\"] in hcp_val_ids, hcp_subj_data):\n",
    "    source_vol_ds.append(\n",
    "        subj_dict[\"t1w\"][\n",
    "            None,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "source_val_dataset = torch.utils.data.ConcatDataset(source_vol_ds)\n",
    "\n",
    "source_val_loader = monai.data.DataLoader(\n",
    "    source_val_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    #     persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d4f8c-1e37-413c-b298-e9c1a94b3f58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up clinic scan data.\n",
    "target_patch_ds = list()\n",
    "for subj_dict in filter(lambda s: s[\"subj_id\"] in clinic_train_ids, clinic_subj_data):\n",
    "    target_patch_ds.append(\n",
    "        pitn.data.MaskFilteredPatchDataset3d(\n",
    "            subj_dict[\"t1w\"], mask=subj_dict[\"mask\"], patch_size=params.train.patch_size\n",
    "        )\n",
    "    )\n",
    "\n",
    "target_train_dataset = torch.utils.data.ConcatDataset(target_patch_ds)\n",
    "\n",
    "# Calculate the number of clinic samples per subject to match the total length of the\n",
    "# source domain dataset.\n",
    "num_clinic_samples_per_img = np.floor(\n",
    "    len(source_train_dataset.datasets)\n",
    "    * params.samples_per_subj_per_epoch\n",
    "    / len(target_train_dataset.datasets)\n",
    ").astype(int)\n",
    "\n",
    "target_train_sampler = pitn.samplers.ConcatDatasetBalancedRandomSampler(\n",
    "    target_train_dataset.datasets,\n",
    "    max_samples_per_dataset=num_clinic_samples_per_img,\n",
    ")\n",
    "\n",
    "target_train_loader = monai.data.DataLoader(\n",
    "    target_train_dataset,\n",
    "    sampler=target_train_sampler,\n",
    "    batch_size=params.batch_size,\n",
    "    pin_memory=True,\n",
    "    num_workers=7,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b36bd2-f1f6-4b24-b67a-8f5a9ceb3281",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49101a65-8de8-4e51-b935-0950ce3c5393",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClinicMatchGAN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_channels: int,\n",
    "        gen_num_high_freq: int = 3,\n",
    "        discriminator_downsample_factors=[1, 2, 4],\n",
    "        lambda_adversary_loss: float = 1,\n",
    "        lambda_grad_penalty: float = 1,\n",
    "        lambda_reconst_loss_weight=1,\n",
    "        gen_optim_kwargs=dict(),\n",
    "        discriminator_optim_kwargs=dict(),\n",
    "        weight_init_fn=None,\n",
    "        input_noise_dist=None,\n",
    "        label_noise_dist=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        if self.hparams.lambda_grad_penalty is None:\n",
    "            self.hparams.use_grad_penalty = False\n",
    "        else:\n",
    "            self.hparams.use_grad_penalty = True\n",
    "        self.generator = pitn.nn.gan.generative.LPTN(\n",
    "            num_channels, num_high_freq_levels=self.hparams.gen_num_high_freq\n",
    "        )\n",
    "\n",
    "        self.discriminator = pitn.nn.gan.adversarial.MultiDiscriminator(\n",
    "            num_channels, self.hparams.discriminator_downsample_factors\n",
    "        )\n",
    "\n",
    "        if weight_init_fn is not None:\n",
    "            self.generator = self.generator.apply(weight_init_fn)\n",
    "            self.discriminator = self.discriminator.apply(weight_init_fn)\n",
    "\n",
    "        self.val_psnr_metric = monai.metrics.PSNRMetric(max_val=1.0)\n",
    "        self.val_viz_slice = None\n",
    "        self.val_viz_range = None\n",
    "\n",
    "        self.plain_log = Box(default_box=True, loss_gen=dict(), loss_discrim=dict())\n",
    "        self.plain_log.discrim_preds.real = dict()\n",
    "        self.plain_log.discrim_preds.fake = dict()\n",
    "        self.plain_log.val.discrim_preds.fake = dict()\n",
    "\n",
    "    def forward(self, x, input_noise=True):\n",
    "        if input_noise:\n",
    "            if self.hparams.input_noise_dist is not None:\n",
    "                x = x + self.hparams.input_noise_dist.sample(x.shape).to(x)\n",
    "        return self.generator(x)\n",
    "\n",
    "    def reconstruction_loss(self, y_source, y_pred):\n",
    "        return F.mse_loss(y_source, y_pred, reduction=\"mean\")\n",
    "\n",
    "    def ls_adversarial_loss(self, sample, label: int, label_noise_dist=None):\n",
    "\n",
    "        sample_pred = self.discriminator(sample)\n",
    "        label_t = torch.ones_like(sample_pred) * label\n",
    "        if label_noise_dist is not None:\n",
    "            label_t = label_t + label_noise_dist.sample(label_t.shape).to(label_t)\n",
    "        loss = F.mse_loss(sample_pred, label_t, reduction=\"mean\")\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def grad_penalty(\n",
    "        self,\n",
    "        real_samples: torch.Tensor,\n",
    "        fake_samples: torch.Tensor,\n",
    "    ):\n",
    "\n",
    "        batch_size = real_samples.shape[0]\n",
    "        avg_weight_rand = torch.rand(batch_size, *((1,) * (real_samples.ndim - 1))).to(\n",
    "            real_samples\n",
    "        )\n",
    "        # For each sample in the batch, find a randomly-weighted linear interpolation\n",
    "        # between the real and generated/fake samples.\n",
    "        weighted_interpolate = (avg_weight_rand * real_samples) + (\n",
    "            (1 - avg_weight_rand) * fake_samples\n",
    "        )\n",
    "        # Need to require grad for the gradient calculation.\n",
    "        weighted_interp_samples = weighted_interpolate.requires_grad_(True)\n",
    "        pred_interp_samples = self.discriminator(weighted_interp_samples)\n",
    "\n",
    "        grad = torch.autograd.grad(\n",
    "            outputs=pred_interp_samples,\n",
    "            inputs=weighted_interp_samples,\n",
    "            grad_outputs=torch.ones_like(pred_interp_samples),\n",
    "            create_graph=True,\n",
    "            only_inputs=True,\n",
    "            retain_graph=True,\n",
    "        )[0]\n",
    "\n",
    "        grad = grad.view(batch_size, -1)\n",
    "        # Calculate L2 norm manually so a small epsilon can be used to avoid NaNs.\n",
    "        eps = 1e-7\n",
    "        penalty = torch.mean((torch.sqrt(torch.sum((grad**2), dim=1) + eps) - 1) ** 2)\n",
    "\n",
    "        return penalty\n",
    "\n",
    "    def ls_gan_grad_penalty(self, real_samples, noise_scale: float, k=1):\n",
    "        \"\"\"Implements another form of grad penalty from Kodali, et. al., 2017, used in\n",
    "        Mao, et. al., 2018 (2nd LS-GAN paper).\n",
    "        \"\"\"\n",
    "        batch_size = real_samples.shape[0]\n",
    "\n",
    "        # Technically, the original paper specified the noise as a multi-variate Gaussian\n",
    "        # with a diagonal covariance matrix filled with the same value. So, the\n",
    "        # 'c' value in that formulation would scale up the *variance*, while the\n",
    "        # equivalent 1D Normal distribution here specifies the *standard deviation*.\n",
    "        # It probably doesn't matter.\n",
    "        noise_dist = torch.distributions.Normal(0.0, noise_scale)\n",
    "        noise = noise_dist.sample(real_samples.shape).to(real_samples)\n",
    "        noisy_samples = real_samples + noise\n",
    "        # Need to require grad for the gradient calculation.\n",
    "        noisy_samples = noisy_samples.requires_grad_(True)\n",
    "\n",
    "        pred_samples = self.discriminator(noisy_samples)\n",
    "\n",
    "        grad = torch.autograd.grad(\n",
    "            outputs=pred_samples,\n",
    "            inputs=noisy_samples,\n",
    "            grad_outputs=torch.ones_like(pred_samples),\n",
    "            create_graph=True,\n",
    "            only_inputs=True,\n",
    "            retain_graph=True,\n",
    "        )[0]\n",
    "\n",
    "        grad = grad.view(batch_size, -1)\n",
    "        # Calculate L2 norm manually so a small epsilon can be used to avoid NaNs.\n",
    "        eps = 1e-7\n",
    "        penalty = torch.mean((torch.sqrt(torch.sum((grad**2), dim=1) + eps) - k) ** 2)\n",
    "\n",
    "        return penalty\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "\n",
    "        source_samples = batch[\"source\"]\n",
    "        target_samples = batch[\"target\"]\n",
    "        if self.hparams.input_noise_dist is not None:\n",
    "            source_samples = source_samples + self.hparams.input_noise_dist.sample(\n",
    "                source_samples.shape\n",
    "            ).to(source_samples)\n",
    "        # Optimizer index decides whether this step updates the generator or discriminator.\n",
    "        # Update generator network.\n",
    "        if optimizer_idx == self._GENERATOR_OPTIMIZER_IDX:\n",
    "\n",
    "            translated_samples = self.generator(source_samples)\n",
    "\n",
    "            l_g_reconstruct = self.reconstruction_loss(\n",
    "                source_samples, translated_samples\n",
    "            )\n",
    "            l_g_reconstruct *= self.hparams.lambda_reconst_loss_weight\n",
    "            self.log(\n",
    "                \"train_loss_terms/gen_reconstruct\",\n",
    "                l_g_reconstruct.detach(),\n",
    "            )\n",
    "\n",
    "            l_g_adversarial = self.ls_adversarial_loss(\n",
    "                translated_samples,\n",
    "                label=0,\n",
    "                label_noise_dist=self.hparams.label_noise_dist,\n",
    "            )\n",
    "            l_g_adversarial *= self.hparams.lambda_adversary_loss * 1 / 2\n",
    "            self.log(\n",
    "                \"train_loss_terms/gen_adversarial\",\n",
    "                l_g_adversarial.detach(),\n",
    "            )\n",
    "\n",
    "            # Combine terms into final loss.\n",
    "            loss_gen = l_g_reconstruct + l_g_adversarial\n",
    "            self.log(\"train/gen_loss\", loss_gen.detach())\n",
    "            # Log loss and set up return dictionary.\n",
    "            self.plain_log.loss_gen[self.global_step] = float(\n",
    "                loss_gen.detach().cpu().item()\n",
    "            )\n",
    "\n",
    "            tqdm_dict = {\"loss_gen\": loss_gen.detach()}\n",
    "            output = collections.OrderedDict(\n",
    "                {\"loss\": loss_gen, \"progress_bar\": tqdm_dict, \"log\": tqdm_dict}\n",
    "            )\n",
    "\n",
    "        ### Update discriminator network.\n",
    "        elif optimizer_idx == self._DISCRIMINATOR_OPTIMIZER_IDX:\n",
    "\n",
    "            # Real images.\n",
    "            loss_real = (\n",
    "                self.ls_adversarial_loss(\n",
    "                    target_samples,\n",
    "                    label=1,\n",
    "                    label_noise_dist=self.hparams.label_noise_dist,\n",
    "                )\n",
    "                / 2\n",
    "            )\n",
    "            self.log(\"train_loss_terms/discrim_real_loss\", loss_real.detach())\n",
    "\n",
    "            # Translated (i.e., fake) images\n",
    "            # We aren't updating the generator weights here, so there's no need to\n",
    "            # keep track of the generator's gradients.\n",
    "            with torch.no_grad():\n",
    "                translated_samples = self.generator(source_samples)\n",
    "            loss_fake = (\n",
    "                self.ls_adversarial_loss(\n",
    "                    translated_samples,\n",
    "                    label=-1,\n",
    "                    label_noise_dist=self.hparams.label_noise_dist,\n",
    "                )\n",
    "                / 2\n",
    "            )\n",
    "            self.log(\"train_loss_terms/discrim_fake_loss\", loss_fake.detach())\n",
    "\n",
    "            # Noise scaling found by taking `~ 0.1176 x abs diff between max and min`\n",
    "            # (of values of the input tensors, here the samples from the target domain).\n",
    "            if self.hparams.use_grad_penalty:\n",
    "                grad_penalty = self.ls_gan_grad_penalty(\n",
    "                    target_samples, noise_scale=0.2352\n",
    "                )\n",
    "                grad_penalty *= self.hparams.lambda_grad_penalty\n",
    "                self.log(\n",
    "                    \"train_loss_terms/discrim_grad_penalty\",\n",
    "                    grad_penalty.detach(),\n",
    "                )\n",
    "            else:\n",
    "                grad_penalty = torch.zeros_like(loss_fake)\n",
    "\n",
    "            # Combine terms into final loss value.\n",
    "            loss_discrim = loss_fake + loss_real + grad_penalty\n",
    "            self.log(\"train/discrim_loss\", loss_discrim.detach())\n",
    "\n",
    "            # Record loss and set up return dictionary.\n",
    "            self.plain_log.loss_discrim[self.global_step] = float(\n",
    "                loss_discrim.detach().cpu().item()\n",
    "            )\n",
    "            tqdm_dict = {\"loss_discrim\": loss_discrim.detach()}\n",
    "            output = collections.OrderedDict(\n",
    "                {\"loss\": loss_discrim, \"progress_bar\": tqdm_dict, \"log\": tqdm_dict}\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise RuntimeError(f\"ERROR: Invalid optimizer index {optimizer_idx}\")\n",
    "        # Record discriminator predictions for later plotting.\n",
    "        if self.global_step % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                real_preds = self.discriminator(target_samples)\n",
    "                fake_preds = self.discriminator(translated_samples)\n",
    "                self.plain_log.discrim_preds.real[self.global_step] = torch.clone(\n",
    "                    real_preds.detach().cpu()\n",
    "                )\n",
    "                self.plain_log.discrim_preds.fake[self.global_step] = torch.clone(\n",
    "                    fake_preds.detach().cpu()\n",
    "                )\n",
    "        return output\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        source_sample = batch\n",
    "        if self.hparams.input_noise_dist is not None:\n",
    "            source_sample = source_sample + self.hparams.input_noise_dist.sample(\n",
    "                source_sample.shape\n",
    "            ).to(source_sample)\n",
    "        source_translate = self.generator(source_sample)\n",
    "        reconstruction_loss = (\n",
    "            self.hparams.lambda_reconst_loss_weight\n",
    "            * self.reconstruction_loss(source_sample, source_translate)\n",
    "        )\n",
    "        adv_loss = (\n",
    "            self.hparams.lambda_adversary_loss\n",
    "            * self.ls_adversarial_loss(source_translate, label=0)\n",
    "            / 2\n",
    "        )\n",
    "\n",
    "        self.log(\"val/reconstruct_loss\", reconstruction_loss.detach())\n",
    "        self.log(\"val/adversarial_loss\", adv_loss.detach())\n",
    "\n",
    "        psnr_loss = self.val_psnr_metric(y_pred=source_translate, y=source_sample)\n",
    "        self.log(\"val/psnr\", psnr_loss.detach())\n",
    "\n",
    "        # Only plot subject translation if batch size of the validation step is 1.\n",
    "        if source_sample.shape[0] == 1:\n",
    "            # Save the discriminator's prediction over the entire translated volume.\n",
    "            d_pred = self.discriminator(source_translate)\n",
    "            # Should only have 1 set of predictions over all spatial scales.\n",
    "            d_pred = d_pred.flatten().detach().cpu()\n",
    "            self.plain_log.val.discrim_preds.fake[self.global_step] = d_pred\n",
    "\n",
    "            plot_vol = source_translate[0].cpu()\n",
    "            plot_vol = torch.clip(\n",
    "                plot_vol,\n",
    "                *tuple(torch.quantile(plot_vol, q=torch.as_tensor([0.001, 0.999]))),\n",
    "            )\n",
    "            plot_vol = monai.transforms.utils.rescale_array(\n",
    "                plot_vol, minv=0.0, maxv=255.0\n",
    "            )\n",
    "            monai.visualize.img2tensorboard.add_animated_gif(\n",
    "                image_tensor=plot_vol,\n",
    "                writer=self.logger.experiment,\n",
    "                tag=\"val_subj\",\n",
    "                max_out=1,\n",
    "                scale_factor=1.0,\n",
    "                global_step=self.global_step,\n",
    "            )\n",
    "\n",
    "            # Log a slice of the source, translated, and the abs. error.\n",
    "            fig = plt.figure(dpi=100)\n",
    "\n",
    "            if self.val_viz_slice is None:\n",
    "                self.val_viz_slice = (\n",
    "                    slice(None),\n",
    "                    slice(None),\n",
    "                    (source_translate.shape[-1] // 2) + 2,\n",
    "                )\n",
    "            vol_to_plot = [\n",
    "                source_sample[0, 0].cpu().numpy(),\n",
    "                source_translate[0, 0].cpu().numpy(),\n",
    "                torch.abs(source_sample[0, 0] - source_translate[0, 0]).cpu().numpy(),\n",
    "            ]\n",
    "            vol_to_plot = list(map(lambda v: v[self.val_viz_slice], vol_to_plot))\n",
    "\n",
    "            if self.val_viz_range is None:\n",
    "                vmin = np.min(np.stack(vol_to_plot))\n",
    "                vmax = np.max(np.stack(vol_to_plot))\n",
    "                self.val_viz_range = (vmin, vmax)\n",
    "            else:\n",
    "                vmin, vmax = self.val_viz_range\n",
    "            cmap = \"gray\"\n",
    "            grid = mpl_toolkits.axes_grid1.ImageGrid(\n",
    "                fig,\n",
    "                111,\n",
    "                nrows_ncols=(1, 3),\n",
    "                axes_pad=0.1,\n",
    "                share_all=True,\n",
    "                cbar_mode=\"single\",\n",
    "                cbar_location=\"bottom\",\n",
    "                cbar_pad=0.1,\n",
    "            )\n",
    "\n",
    "            map_names = [\"Source\", \"Translated\", \"Abs. Error\"]\n",
    "            for ax, label, vol in zip(grid, map_names, vol_to_plot):\n",
    "                im = ax.imshow(\n",
    "                    np.rot90(vol), interpolation=None, cmap=cmap, vmin=vmin, vmax=vmax\n",
    "                )\n",
    "                ax.set_xlabel(label)\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                ax.set_xticklabels([])\n",
    "                ax.set_yticklabels([])\n",
    "\n",
    "            grid.cbar_axes[0].colorbar(im)\n",
    "\n",
    "            self.logger.experiment.add_figure(\"val_slice\", fig, self.global_step)\n",
    "\n",
    "        return psnr_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        opt_gen = torch.optim.Adam(\n",
    "            self.generator.parameters(), **self.hparams.gen_optim_kwargs\n",
    "        )\n",
    "        opt_discriminator = torch.optim.Adam(\n",
    "            self.discriminator.parameters(), **self.hparams.discriminator_optim_kwargs\n",
    "        )\n",
    "\n",
    "        self._GENERATOR_OPTIMIZER_IDX = 0\n",
    "        self._DISCRIMINATOR_OPTIMIZER_IDX = 1\n",
    "        return [opt_gen, opt_discriminator], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448d46aa-c344-4b15-8e08-2fdf62124a99",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c31863-63bc-40ae-b62b-62844d75f61a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "train_start_timestamp = datetime.datetime.now().replace(microsecond=0)\n",
    "# Explicitly set whether or not to use grad penalty.\n",
    "lambda_grad_penalty = params.lambda_grad_penalty if params.use_grad_penalty else None\n",
    "\n",
    "# Instantiate model.\n",
    "model = ClinicMatchGAN(\n",
    "    params.num_channels,\n",
    "    gen_num_high_freq=params.num_laplace_high_freq,\n",
    "    discriminator_downsample_factors=params.discriminator_downscale_factors,\n",
    "    lambda_adversary_loss=params.lambda_adversary_loss,\n",
    "    lambda_grad_penalty=lambda_grad_penalty,\n",
    "    lambda_reconst_loss_weight=params.lambda_reconst_loss_weight,\n",
    "    gen_optim_kwargs=params.optim.gen_kwargs,\n",
    "    discriminator_optim_kwargs=params.optim.discriminator_kwargs,\n",
    "    weight_init_fn=params.net_init.f,\n",
    "    input_noise_dist=params.train.aug.input_noise_dist,\n",
    "    label_noise_dist=params.train.aug.label_noise_dist,\n",
    ")\n",
    "\n",
    "with open(log_txt_file, \"a+\") as f:\n",
    "    f.write(f\"Model overview: {model}\\n\")\n",
    "\n",
    "# Create trainer object.\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_epochs=params.max_epochs,\n",
    "    logger=pl_logger,\n",
    "    multiple_trainloader_mode=\"max_size_cycle\",\n",
    "    log_every_n_steps=min([50, len(source_train_loader), len(target_train_loader)]),\n",
    "    check_val_every_n_epoch=3,\n",
    "    #     progress_bar_refresh_rate=10,\n",
    "    terminate_on_nan=True,\n",
    ")\n",
    "\n",
    "# Many warnings are produced here, so it's better for my sanity (and worse in every other\n",
    "# way) to just filter and ignore them...\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    # with torch.autograd.detect_anomaly():\n",
    "    trainer.fit(\n",
    "        model,\n",
    "        train_dataloaders={\n",
    "            \"source\": source_train_loader,\n",
    "            \"target\": target_train_loader,\n",
    "        },\n",
    "        val_dataloaders=source_val_loader,\n",
    "    )\n",
    "\n",
    "train_duration = datetime.datetime.now().replace(microsecond=0) - train_start_timestamp\n",
    "print(f\"Train duration: {train_duration}\")\n",
    "with open(log_txt_file, \"a+\") as f:\n",
    "    f.write(\"\\n\")\n",
    "    f.write(f\"Training time: {train_duration}\\n\")\n",
    "    f.write(\n",
    "        f\"\\t{train_duration.days} Days, \"\n",
    "        + f\"{train_duration.seconds // 3600} Hours,\"\n",
    "        + f\"{(train_duration.seconds // 60) % 60} Minutes,\"\n",
    "        + f'{train_duration.seconds % 60} Seconds\"\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0c93e3-c8a0-4d37-ac76-1af537ecafc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save out trained model\n",
    "trainer.save_checkpoint(str(experiment_results_dir / \"model.ckpt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfd39dc-e76d-440b-8d0f-88ba04abea34",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Result Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1350e9f2-b3b5-4913-bbdb-8373bc3ef6b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "enable_fig_save = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762c98bf-0d43-408e-89b5-7711738274cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Metrics During Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1560bfd2-3254-4f70-90d1-46a3cdd1cf43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predictions over real data, with a target value of 1\n",
    "plt.figure(dpi=120)\n",
    "\n",
    "real_discrim_steps = list(model.plain_log.discrim_preds.real.keys())\n",
    "real_discrim_steps = np.asarray(real_discrim_steps)\n",
    "real_discrim_preds = list(model.plain_log.discrim_preds.real.values())\n",
    "real_discrim_preds = torch.stack(real_discrim_preds, dim=0).cpu()\n",
    "real_discrim_preds = torch.mean(real_discrim_preds, dim=1).numpy()\n",
    "\n",
    "plt.plot(\n",
    "    real_discrim_steps,\n",
    "    real_discrim_preds,\n",
    "    ls=\"-\",\n",
    "    label=[f\"Real = 1, Scale {f}\" for f in params.discriminator_downscale_factors],\n",
    "    lw=0.9,\n",
    "    alpha=0.85,\n",
    ")\n",
    "\n",
    "fake_discrim_steps = list(model.plain_log.discrim_preds.fake.keys())\n",
    "fake_discrim_steps = np.asarray(fake_discrim_steps)\n",
    "fake_discrim_preds = list(model.plain_log.discrim_preds.fake.values())\n",
    "fake_discrim_preds = torch.stack(fake_discrim_preds, dim=0).cpu()\n",
    "fake_discrim_preds = torch.mean(fake_discrim_preds, dim=1).numpy()\n",
    "\n",
    "plt.plot(\n",
    "    fake_discrim_steps,\n",
    "    fake_discrim_preds,\n",
    "    label=[f\"Fakes = -1, Scale {f}\" for f in params.discriminator_downscale_factors],\n",
    "    lw=0.9,\n",
    "    alpha=0.85,\n",
    ")\n",
    "plt.title(\"Prediction of Discriminator during Training\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Mean (Over Batch) Discriminator Prediction\")\n",
    "plt.legend(ncol=2)\n",
    "plt.hlines(\n",
    "    [-1, 1],\n",
    "    -1,\n",
    "    max(fake_discrim_steps.max(), real_discrim_steps.max()),\n",
    "    color=\"black\",\n",
    "    ls=\"--\",\n",
    "    lw=0.7,\n",
    "    alpha=0.8,\n",
    "    zorder=999,\n",
    ")\n",
    "plt.xlim(-1, max(fake_discrim_steps.max(), real_discrim_steps.max()))\n",
    "plt.ylim(-2.5, 2.5)\n",
    "if enable_fig_save:\n",
    "    plt.savefig(experiment_results_dir / \"discrim_pred_in_training.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a70fc9-ce4d-4c6b-ace7-ef785e5e5f8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Discriminator predictions during validation, e.g. what does the discriminator predict\n",
    "# given entire volumes?\n",
    "\n",
    "plt.figure(dpi=120)\n",
    "\n",
    "# Only fake images are available\n",
    "fake_discrim_steps = list(model.plain_log.val.discrim_preds.fake.keys())\n",
    "fake_discrim_steps = np.asarray(fake_discrim_steps)\n",
    "fake_discrim_preds = list(model.plain_log.val.discrim_preds.fake.values())\n",
    "fake_discrim_preds = torch.stack(fake_discrim_preds, dim=0).cpu().numpy()\n",
    "\n",
    "plt.plot(\n",
    "    fake_discrim_steps,\n",
    "    fake_discrim_preds,\n",
    "    label=[f\"Scale {f}\" for f in params.discriminator_downscale_factors],\n",
    ")\n",
    "plt.title(\n",
    "    \"Prediction of Discriminator in Validation Set During Training\\nAll Fake(=-1)\"\n",
    ")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Discriminator Prediction\")\n",
    "plt.hlines(\n",
    "    [-1, 1], -1, fake_discrim_steps.max(), color=\"black\", lw=0.75, ls=\"--\", zorder=999\n",
    ")\n",
    "plt.xlim(-1, fake_discrim_steps.max())\n",
    "plt.legend()\n",
    "\n",
    "if enable_fig_save:\n",
    "    plt.savefig(experiment_results_dir / \"discrim_pred_in_validation.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4802150c-7406-4821-848d-1450b924ac21",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Final Inference Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f37a3-c7b6-4033-bae2-04fbc6f11501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up visualization objects.\n",
    "# Find a common size for all volumes\n",
    "spatial_shapes = list()\n",
    "for subj in itertools.chain(hcp_subj_dataset, clinic_subj_dataset):\n",
    "    spatial_shapes.append(tuple(subj[\"t1w\"].shape[-3:]))\n",
    "target_spatial_shape = tuple(np.max(np.asarray(spatial_shapes), axis=0))\n",
    "padder = monai.transforms.SpatialPad(\n",
    "    torch.Size(target_spatial_shape), method=\"symmetric\", mode=\"replicate\"\n",
    ")\n",
    "cropper = monai.transforms.CenterSpatialCrop(torch.Size(target_spatial_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbc014c-dad4-4056-b322-17f5d92a3ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate viz objects/volumes.\n",
    "viz_data = Box(default_box=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    # Grab HCP data for viz.\n",
    "    for subj in hcp_subj_dataset:\n",
    "        data = Box(default_box=True)\n",
    "\n",
    "        vol = subj[\"t1w\"][\n",
    "            None,\n",
    "        ].to(model.device)\n",
    "        # ! Make sure to perform model inference *before* descaling volume.\n",
    "        translated = model.forward(vol, input_noise=True)\n",
    "        pred_class = model.discriminator(translated)\n",
    "\n",
    "        # Pad and crop vols to be the same shape.\n",
    "        # There is a bug in monai that makes the cropping output as an ndarray\n",
    "        vol = np.asarray(cropper(padder(vol[0].cpu())))\n",
    "        vol = torch.from_numpy(vol)\n",
    "        data.vol = subj[\"scaler\"].descale(vol).numpy()\n",
    "\n",
    "        mask = cropper(padder(subj[\"mask\"].float())).astype(bool)\n",
    "        mask = np.asarray(mask)\n",
    "        data.mask = mask\n",
    "\n",
    "        translated = np.asarray(cropper(padder(translated[0].cpu())))\n",
    "        translated = torch.from_numpy(translated)\n",
    "        translated = subj[\"scaler\"].descale(translated.cpu()).numpy()\n",
    "        data.translated = translated\n",
    "        data.pred_class = pred_class[0].cpu().numpy()\n",
    "        data.affine = subj[\"t1w_meta_dict\"][\"affine\"]\n",
    "\n",
    "        viz_data.hcp[str(subj[\"subj_id\"])] = data\n",
    "\n",
    "    # Grab clinic data for viz.\n",
    "    for subj in clinic_subj_dataset:\n",
    "\n",
    "        vol = subj[\"t1w\"][\n",
    "            None,\n",
    "        ].to(model.device)\n",
    "        # ! Make sure to predict real/fake *before* descaling volume.\n",
    "        pred_class = model.discriminator(vol)\n",
    "\n",
    "        # Pad and crop vols to be the same shape.\n",
    "        # There is a bug in monai that makes the cropping output as an ndarray\n",
    "        vol = np.asarray(cropper(padder(vol[0].cpu())))\n",
    "        vol = torch.from_numpy(vol)\n",
    "        data.vol = subj[\"scaler\"].descale(vol).numpy()\n",
    "\n",
    "        mask = cropper(padder(subj[\"mask\"].float())).astype(bool)\n",
    "        mask = np.asarray(mask)\n",
    "        data.mask = mask\n",
    "        data.pred_class = pred_class[0].cpu().numpy()\n",
    "        data.affine = subj[\"t1w_meta_dict\"][\"affine\"]\n",
    "\n",
    "        viz_data.clinic[str(subj[\"subj_id\"])] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c646fc-51e3-4e60-b58f-cdd9d53c96ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save out all network predictions to Nifti2 files and compress them into a zip archive.\n",
    "if enable_fig_save:\n",
    "    img_names = list()\n",
    "    for subj_id, viz in viz_data.hcp.items():\n",
    "        pred_vol = viz.translated\n",
    "        affine = viz.affine\n",
    "        nib_img = nib.Nifti2Image(pred_vol, affine)\n",
    "\n",
    "        filename = experiment_results_dir / f\"{subj_id}_translated_t1w.nii.gz\"\n",
    "        nib.save(nib_img, str(filename))\n",
    "        img_names.append(filename)\n",
    "\n",
    "    with zipfile.ZipFile(experiment_results_dir / \"translated_t1w.zip\", \"w\") as fzip:\n",
    "        for filename in img_names:\n",
    "            fzip.write(\n",
    "                filename,\n",
    "                arcname=filename.name,\n",
    "                compress_type=zipfile.ZIP_DEFLATED,\n",
    "                compresslevel=6,\n",
    "            )\n",
    "            os.remove(filename)\n",
    "    # Make sure we exit the 'with' statement above.\n",
    "    print(\"Done with files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b189bc79-2f81-4e4b-af5d-f071f9abd5aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(log_txt_file, \"a+\") as f:\n",
    "    f.write(\"Fake: -1\\n\")\n",
    "    f.write(str([v.pred_class for v in viz_data.hcp.values()]))\n",
    "    f.write(\"\\n\\nReal: 1\\n\")\n",
    "    f.write(str([v.pred_class for v in viz_data.clinic.values()]))\n",
    "    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a71c3a-5292-4ed3-81e8-498b1e27121e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predictions over real data, with a target value of 1\n",
    "viz_real_d_preds = [v.pred_class for v in viz_data.hcp.values()]\n",
    "viz_real_d_preds = np.stack(viz_real_d_preds)\n",
    "\n",
    "viz_fake_d_preds = [v.pred_class for v in viz_data.clinic.values()]\n",
    "viz_fake_d_preds = np.stack(viz_fake_d_preds)\n",
    "\n",
    "print(f\"Fake = -1, scales {params.discriminator_downscale_factors}\")\n",
    "print(viz_real_d_preds)\n",
    "print(f\"Real = 1, scales {params.discriminator_downscale_factors}\")\n",
    "print(viz_fake_d_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9a48ad-2963-45de-976d-8dba9fecd8f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=2, sharey=True, dpi=100)\n",
    "cmap = \"Spectral\"\n",
    "\n",
    "axs[0, 0].matshow(viz_real_d_preds.T, cmap=cmap)\n",
    "axs[0, 0].set_title(\"Pred. on Real(=1)\")\n",
    "axs[0, 0].set_ylabel(\"Downscale Factor\")\n",
    "axs[0, 0].set_xticks([])\n",
    "\n",
    "axs[1, 0].matshow(viz_fake_d_preds.T, cmap=cmap)\n",
    "axs[1, 0].set_title(\"Pred. on Fake(=-1)\")\n",
    "axs[1, 0].set_xticks([])\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for ax, preds in zip([axs[0, 0], axs[1, 0]], [viz_real_d_preds.T, viz_fake_d_preds.T]):\n",
    "    for i in range(preds.shape[0]):\n",
    "        for j in range(preds.shape[1]):\n",
    "            ax.text(\n",
    "                j,\n",
    "                i,\n",
    "                f\"{preds[i, j]:.2f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                color=\"black\",\n",
    "                size=\"xx-small\",\n",
    "            )\n",
    "\n",
    "plt.yticks(\n",
    "    list(range(len(params.discriminator_downscale_factors))),\n",
    "    params.discriminator_downscale_factors,\n",
    ")\n",
    "vmax = max(viz_real_d_preds.max(), viz_fake_d_preds.max())\n",
    "vmin = min(viz_real_d_preds.min(), viz_fake_d_preds.min())\n",
    "vmax = np.abs(max(vmax, vmin))\n",
    "vmin = -1 * vmax\n",
    "color_norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\n",
    "fig.colorbar(\n",
    "    mpl.cm.ScalarMappable(norm=color_norm, cmap=cmap),\n",
    "    ax=axs[1, :],\n",
    "    location=\"top\",\n",
    "    #     fraction=0.1,\n",
    "    #     pad=0.03,\n",
    ")\n",
    "axs[1, 0].axis(\"off\")\n",
    "axs[1, 1].axis(\"off\")\n",
    "# if enable_fig_save:\n",
    "#     plt.savefig(experiment_results_dir / \"whole_vol_discrim_preds.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca39a4-4446-4125-8c46-14a753f5bbb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hcp_viz_subj_idx = 1\n",
    "hcp_viz_subj_id = list(viz_data.hcp.keys())[hcp_viz_subj_idx]\n",
    "hcp_viz_subj = viz_data.hcp[hcp_viz_subj_id]\n",
    "clinic_viz_subj_idx = 0\n",
    "clinic_viz_subj_id = list(viz_data.clinic.keys())[clinic_viz_subj_idx]\n",
    "clinic_viz_subj = viz_data.clinic[clinic_viz_subj_id]\n",
    "\n",
    "# Grab 3 slices from each axis in roughly the center, offset by a few mms.\n",
    "viz_slices = list()\n",
    "\n",
    "# Horiztonal slice\n",
    "viz_slices.append(\n",
    "    (\n",
    "        (\n",
    "            0,\n",
    "            slice(None),\n",
    "            slice(None),\n",
    "            (hcp_viz_subj.vol.shape[3] // 2) - 2,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "# Coronal slice\n",
    "viz_slices.append(\n",
    "    (\n",
    "        (\n",
    "            0,\n",
    "            slice(None),\n",
    "            (hcp_viz_subj.vol.shape[2] // 2) + 3,\n",
    "            slice(None),\n",
    "        )\n",
    "    )\n",
    ")\n",
    "# Saggital slice\n",
    "viz_slices.append(\n",
    "    (\n",
    "        0,\n",
    "        (hcp_viz_subj.vol.shape[1] // 2) + 4,\n",
    "        slice(None),\n",
    "        slice(None),\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "def abs_error_map(y, y_pred):\n",
    "    y = torch.as_tensor(y)\n",
    "    y_pred = torch.as_tensor(y_pred)\n",
    "    error = torch.abs(y - y_pred)\n",
    "\n",
    "    return error.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c8aa7-13eb-4ea2-955c-0b9d00781218",
   "metadata": {
    "tags": []
   },
   "source": [
    "### T1 Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ddc56-9f60-4a16-8e71-d934ac0144db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display vols for the following:\n",
    "# Source domain\n",
    "# Translated\n",
    "# Target domain\n",
    "# Absolute error between source and translated\n",
    "\n",
    "cmap = \"gray\"\n",
    "\n",
    "col_names = [\"Horizontal\", \"Coronal\", \"Saggital\"]\n",
    "\n",
    "row_names = [\n",
    "    \"Source HCP\",\n",
    "    \"Translated\",\n",
    "    \"Target Clinic\",\n",
    "    \"Abs Error\\nSource-Translated\",\n",
    "]\n",
    "\n",
    "vol_rows = [\n",
    "    hcp_viz_subj.vol,\n",
    "    hcp_viz_subj.translated,\n",
    "    clinic_viz_subj.vol,\n",
    "    abs_error_map(hcp_viz_subj.vol, hcp_viz_subj.translated) * hcp_viz_subj.mask,\n",
    "]\n",
    "\n",
    "vols_to_plot = list()\n",
    "for i in range(len(row_names)):\n",
    "    col_plot = list()\n",
    "    for j in range(len(col_names)):\n",
    "        vol_to_plot = vol_rows[i]\n",
    "        slice_to_plot = vol_to_plot[viz_slices[j]]\n",
    "        col_plot.append(slice_to_plot)\n",
    "    vols_to_plot.append(col_plot)\n",
    "\n",
    "nrows = len(row_names)\n",
    "ncols = len(col_names)\n",
    "\n",
    "# Don't take the absolute max and min values, as there exist some extreme (e.g., > 3\n",
    "# orders of magnitude) outliers. Instead, take some percente quantile.\n",
    "# Reshape and concatenate the vols in order to compute the quantiles of images with\n",
    "# different shapes (e.g., the low-res input patch).\n",
    "max_vol = np.quantile(\n",
    "    np.concatenate(\n",
    "        [np.asarray(a).flatten() for a in itertools.chain.from_iterable(vols_to_plot)]\n",
    "    ),\n",
    "    1.0,\n",
    ")\n",
    "min_vol = np.quantile(\n",
    "    np.concatenate(\n",
    "        [np.asarray(a).flatten() for a in itertools.chain.from_iterable(vols_to_plot)]\n",
    "    ),\n",
    "    0.0,\n",
    ")\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(4.5, 6), dpi=180)\n",
    "\n",
    "grid = mpl.gridspec.GridSpec(\n",
    "    nrows,\n",
    "    ncols,\n",
    "    figure=fig,\n",
    "    hspace=0.05,\n",
    "    wspace=0.05,\n",
    ")\n",
    "axs = list()\n",
    "max_subplot_height = 0\n",
    "for i_row in range(nrows):\n",
    "    vol = vols_to_plot[i_row]\n",
    "\n",
    "    for j_col in range(ncols):\n",
    "        ax = fig.add_subplot(grid[i_row, j_col])\n",
    "        ax.imshow(\n",
    "            np.rot90(vol[j_col]),\n",
    "            cmap=cmap,\n",
    "            interpolation=None,\n",
    "            vmin=min_vol,\n",
    "            vmax=max_vol,\n",
    "        )\n",
    "        if ax.get_subplotspec().is_first_col():\n",
    "            ax.set_ylabel(row_names[i_row], size=\"xx-small\")\n",
    "        if ax.get_subplotspec().is_last_row():\n",
    "            ax.set_xlabel(col_names[j_col])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        # Update highest subplot to put the `suptitle` later on.\n",
    "        max_subplot_height = max(\n",
    "            max_subplot_height, ax.get_position(original=False).get_points()[1, 1]\n",
    "        )\n",
    "        axs.append(ax)\n",
    "\n",
    "color_norm = mpl.colors.Normalize(vmin=min_vol, vmax=max_vol)\n",
    "fig.colorbar(\n",
    "    mpl.cm.ScalarMappable(norm=color_norm, cmap=cmap),\n",
    "    ax=axs,\n",
    "    location=\"right\",\n",
    "    fraction=0.1,\n",
    "    pad=0.03,\n",
    ")\n",
    "plt.suptitle(\n",
    "    \"Vol and Abs. Error, Normalized over All Images\",\n",
    "    y=max_subplot_height + 0.015,\n",
    "    verticalalignment=\"bottom\",\n",
    ")\n",
    "if enable_fig_save:\n",
    "    plt.savefig(experiment_results_dir / \"Vol_w_Abs_Err.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970e1df8-391c-4f6a-b91e-4ecf55903823",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# No normalization.\n",
    "# Display vols for the following:\n",
    "# Source domain\n",
    "# Translated\n",
    "# Target domain\n",
    "# Absolute error between source and translated\n",
    "\n",
    "cmap = \"gray\"\n",
    "\n",
    "col_names = [\"Horizontal\", \"Coronal\", \"Saggital\"]\n",
    "\n",
    "row_names = [\n",
    "    \"Source HCP\",\n",
    "    \"Translated\",\n",
    "    \"Target Clinic\",\n",
    "    \"Abs Error\\nSource-Translated\",\n",
    "]\n",
    "\n",
    "vol_rows = [\n",
    "    hcp_viz_subj.vol,\n",
    "    hcp_viz_subj.translated,\n",
    "    clinic_viz_subj.vol,\n",
    "    abs_error_map(hcp_viz_subj.vol, hcp_viz_subj.translated) * hcp_viz_subj.mask,\n",
    "]\n",
    "\n",
    "vols_to_plot = list()\n",
    "for i in range(len(row_names)):\n",
    "    col_plot = list()\n",
    "    for j in range(len(col_names)):\n",
    "        vol_to_plot = vol_rows[i]\n",
    "        slice_to_plot = vol_to_plot[viz_slices[j]]\n",
    "        col_plot.append(slice_to_plot)\n",
    "    vols_to_plot.append(col_plot)\n",
    "\n",
    "nrows = len(row_names)\n",
    "ncols = len(col_names)\n",
    "\n",
    "fig = plt.figure(figsize=(4.5, 6), dpi=180)\n",
    "\n",
    "grid = mpl.gridspec.GridSpec(\n",
    "    nrows,\n",
    "    ncols,\n",
    "    figure=fig,\n",
    "    hspace=0.05,\n",
    "    wspace=0.05,\n",
    ")\n",
    "axs = list()\n",
    "max_subplot_height = 0\n",
    "for i_row in range(nrows):\n",
    "    vol = vols_to_plot[i_row]\n",
    "\n",
    "    for j_col in range(ncols):\n",
    "        ax = fig.add_subplot(grid[i_row, j_col])\n",
    "        ax.imshow(\n",
    "            np.rot90(vol[j_col]),\n",
    "            cmap=cmap,\n",
    "            interpolation=None,\n",
    "        )\n",
    "        if ax.get_subplotspec().is_first_col():\n",
    "            ax.set_ylabel(row_names[i_row], size=\"xx-small\")\n",
    "        if ax.get_subplotspec().is_last_row():\n",
    "            ax.set_xlabel(col_names[j_col])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        # Update highest subplot to put the `suptitle` later on.\n",
    "        max_subplot_height = max(\n",
    "            max_subplot_height, ax.get_position(original=False).get_points()[1, 1]\n",
    "        )\n",
    "        axs.append(ax)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Vol and Abs. Error, No normalization over All Images\",\n",
    "    y=max_subplot_height + 0.015,\n",
    "    verticalalignment=\"bottom\",\n",
    ")\n",
    "if enable_fig_save:\n",
    "    plt.savefig(experiment_results_dir / \"No_Norm_Vol_slices.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8391ae69-af3d-422d-89c3-c76febe9d74d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74158e75-a06b-49f6-87fa-92fe13b70db4",
   "metadata": {},
   "source": [
    "## End Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218ba5bc-3518-468f-95c3-0e2a094d94eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pl_logger.experiment.flush()\n",
    "# Close tensorboard logger.\n",
    "# Don't finalize if the experiment was for debugging.\n",
    "if \"debug\" not in EXPERIMENT_NAME.casefold():\n",
    "    pl_logger.finalize(\"success\")\n",
    "    # Experiment is complete, move the results directory to its final location.\n",
    "    if experiment_results_dir != final_experiment_results_dir:\n",
    "        print(\"Moving out of tmp location\")\n",
    "        experiment_results_dir = experiment_results_dir.rename(\n",
    "            final_experiment_results_dir\n",
    "        )\n",
    "        log_txt_file = experiment_results_dir / log_txt_file.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c074eb0-56aa-473b-97ea-7809f154571d",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda-pitn]",
   "language": "python",
   "name": "conda-env-miniconda-pitn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
