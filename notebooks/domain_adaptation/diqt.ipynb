{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pain in the Net\n",
    "Application of Deep Image Quality Transfer (DIQT) with domain adaptation.\n",
    "\n",
    "\n",
    "Code by:\n",
    "\n",
    "Tyler Spears - tas6hh@virginia.edu\n",
    "\n",
    "Dr. Tom Fletcher\n",
    "\n",
    "---\n",
    "\n",
    "Source work:\n",
    "`S. B. Blumberg, R. Tanno, I. Kokkinos, and D. C. Alexander, “Deeper Image Quality Transfer: Training Low-Memory Neural Networks for 3D Images,” in Medical Image Computing and Computer Assisted Intervention – MICCAI 2018, Cham, 2018, pp. 118–125, doi: 10.1007/978-3-030-00928-1_14.`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports & Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automatically re-import project-specific modules.\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# imports\n",
    "import collections\n",
    "import functools\n",
    "import io\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "import copy\n",
    "import pdb\n",
    "import inspect\n",
    "import random\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import typing\n",
    "import zipfile\n",
    "\n",
    "import ants\n",
    "import dipy\n",
    "import dipy.core\n",
    "import dipy.reconst\n",
    "import dipy.reconst.dti\n",
    "import dipy.segment.mask\n",
    "import dotenv\n",
    "\n",
    "# visualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import IPython\n",
    "\n",
    "# Try importing GPUtil for printing GPU specs.\n",
    "# May not be installed if using CPU only.\n",
    "try:\n",
    "    import GPUtil\n",
    "except ImportError:\n",
    "    warnings.warn(\"WARNING: Package GPUtil not found, cannot print GPU specs\")\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, Markdown\n",
    "import ipyplot\n",
    "\n",
    "# Data management libraries.\n",
    "import nibabel as nib\n",
    "import natsort\n",
    "from natsort import natsorted\n",
    "from addict import Addict\n",
    "import pprint\n",
    "from pprint import pprint as ppr\n",
    "\n",
    "# Computation & ML libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchio\n",
    "import pytorch_lightning as pl\n",
    "import monai\n",
    "\n",
    "import skimage\n",
    "import skimage.feature\n",
    "import skimage.filters\n",
    "import skimage.measure\n",
    "import scipy\n",
    "\n",
    "plt.rcParams.update({\"figure.autolayout\": True})\n",
    "plt.rcParams.update({\"figure.facecolor\": [1.0, 1.0, 1.0, 1.0]})\n",
    "\n",
    "# Set print options for ndarrays/tensors.\n",
    "np.set_printoptions(suppress=True, threshold=100, linewidth=88)\n",
    "torch.set_printoptions(sci_mode=False, threshold=100, linewidth=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update notebook's environment variables with direnv.\n",
    "# This requires the python-dotenv package, and direnv be installed on the system\n",
    "# This will not work on Windows.\n",
    "# NOTE: This is kind of hacky, and not necessarily safe. Be careful...\n",
    "# Libraries needed on the python side:\n",
    "# - os\n",
    "# - subprocess\n",
    "# - io\n",
    "# - dotenv\n",
    "\n",
    "# Form command to be run in direnv's context. This command will print out\n",
    "# all environment variables defined in the subprocess/sub-shell.\n",
    "command = \"direnv exec {} /usr/bin/env\".format(os.getcwd())\n",
    "# Run command in a new subprocess.\n",
    "proc = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, cwd=os.getcwd())\n",
    "# Store and format the subprocess' output.\n",
    "proc_out = proc.communicate()[0].strip().decode(\"utf-8\")\n",
    "# Use python-dotenv to load the environment variables by using the output of\n",
    "# 'direnv exec ...' as a 'dummy' .env file.\n",
    "dotenv.load_dotenv(stream=io.StringIO(proc_out), override=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Project-specific scripts\n",
    "# It's easier to import it this way rather than make an entirely new package, due to\n",
    "# conflicts with local packages and anaconda installations.\n",
    "# You made me do this, poor python package management!!\n",
    "if \"PROJECT_ROOT\" in os.environ:\n",
    "    lib_location = str(Path(os.environ[\"PROJECT_ROOT\"]).resolve())\n",
    "else:\n",
    "    lib_location = str(Path(\"../../\").resolve())\n",
    "if lib_location not in sys.path:\n",
    "    sys.path.insert(0, lib_location)\n",
    "import lib as pitn\n",
    "\n",
    "# Include the top-level lib module along with its submodules.\n",
    "%aimport lib\n",
    "# Grab all submodules of lib, not including modules outside of the package.\n",
    "includes = list(\n",
    "    filter(\n",
    "        lambda m: m.startswith(\"lib.\"),\n",
    "        map(lambda x: x[1].__name__, inspect.getmembers(pitn, inspect.ismodule)),\n",
    "    )\n",
    ")\n",
    "# Run aimport magic with constructed includes.\n",
    "ipy = IPython.get_ipython()\n",
    "ipy.run_line_magic(\"aimport\", \", \".join(includes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch setup\n",
    "# allow for CUDA usage, if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# keep device as the cpu\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Specs Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr cap\n",
    "# Capture output and save to log. Needs to be at the *very first* line of the cell.\n",
    "# Watermark\n",
    "%load_ext watermark\n",
    "%watermark --author \"Tyler Spears\" --updated --iso8601  --python --machine --iversions --githash\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    # GPU information\n",
    "    # Taken from\n",
    "    # <https://www.thepythoncode.com/article/get-hardware-system-information-python>.\n",
    "    # If GPUtil is not installed, skip this step.\n",
    "    try:\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        print(\"=\" * 50, \"GPU Specs\", \"=\" * 50)\n",
    "        list_gpus = []\n",
    "        for gpu in gpus:\n",
    "            # get the GPU id\n",
    "            gpu_id = gpu.id\n",
    "            # name of GPU\n",
    "            gpu_name = gpu.name\n",
    "            driver_version = gpu.driver\n",
    "            cuda_version = torch.version.cuda\n",
    "            # get total memory\n",
    "            gpu_total_memory = f\"{gpu.memoryTotal}MB\"\n",
    "            gpu_uuid = gpu.uuid\n",
    "            list_gpus.append(\n",
    "                (\n",
    "                    gpu_id,\n",
    "                    gpu_name,\n",
    "                    driver_version,\n",
    "                    cuda_version,\n",
    "                    gpu_total_memory,\n",
    "                    gpu_uuid,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            tabulate(\n",
    "                list_gpus,\n",
    "                headers=(\n",
    "                    \"id\",\n",
    "                    \"Name\",\n",
    "                    \"Driver Version\",\n",
    "                    \"CUDA Version\",\n",
    "                    \"Total Memory\",\n",
    "                    \"uuid\",\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    except NameError:\n",
    "        print(\"CUDA Version: \", torch.version.cuda)\n",
    "\n",
    "else:\n",
    "    print(\"CUDA not in use, falling back to CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Tyler Spears\n",
      "\n",
      "Last updated: 2021-10-05T14:13:43.997845+00:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.8\n",
      "IPython version      : 7.23.1\n",
      "\n",
      "Compiler    : GCC 7.3.0\n",
      "OS          : Linux\n",
      "Release     : 5.4.0-88-generic\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: 5123101df9112c32921025ab162ce3c3ee751aa1\n",
      "\n",
      "seaborn          : 0.11.1\n",
      "GPUtil           : 1.4.0\n",
      "json             : 2.0.9\n",
      "dipy             : 1.4.1\n",
      "IPython          : 7.23.1\n",
      "ipywidgets       : 7.6.3\n",
      "matplotlib       : 3.4.1\n",
      "pandas           : 1.2.3\n",
      "scipy            : 1.5.3\n",
      "natsort          : 7.1.1\n",
      "torch            : 1.9.0\n",
      "skimage          : 0.18.1\n",
      "monai            : 0.7.dev2138\n",
      "pytorch_lightning: 1.4.5\n",
      "kornia           : 0.5.8\n",
      "torchio          : 0.18.37\n",
      "numpy            : 1.20.2\n",
      "addict           : 2.4.0\n",
      "ants             : 0.2.7\n",
      "sys              : 3.8.8 (default, Feb 24 2021, 21:46:12) \n",
      "[GCC 7.3.0]\n",
      "nibabel          : 3.2.1\n",
      "\n",
      "================================================== GPU Specs ==================================================\n",
      "  id  Name       Driver Version      CUDA Version  Total Memory    uuid\n",
      "----  ---------  ----------------  --------------  --------------  ----------------------------------------\n",
      "   0  TITAN RTX  460.91.03                   11.1  24217.0MB       GPU-586d2016-71ef-ebb3-437b-6721a22191ec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cap is defined in an ipython magic command\n",
    "print(cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Variables & Definitions Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up directories\n",
    "data_dir = pathlib.Path(os.environ[\"DATA_DIR\"]) / \"hcp\"\n",
    "assert data_dir.exists()\n",
    "write_data_dir = pathlib.Path(os.environ[\"WRITE_DATA_DIR\"]) / \"hcp\"\n",
    "assert write_data_dir.exists()\n",
    "results_dir = pathlib.Path(os.environ[\"RESULTS_DIR\"])\n",
    "assert results_dir.exists()\n",
    "tmp_results_dir = pathlib.Path(os.environ[\"TMP_RESULTS_DIR\"])\n",
    "assert tmp_results_dir.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tensorboard experiment logging setup.\n",
    "EXPERIMENT_NAME = \"debug_dti_nifti_saving\"\n",
    "\n",
    "ts = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "# Break ISO format because many programs don't like having colons ':' in a filename.\n",
    "ts = ts.replace(\":\", \"_\")\n",
    "experiment_name = ts + \"__\" + EXPERIMENT_NAME\n",
    "run_name = experiment_name\n",
    "# experiment_results_dir = results_dir / experiment_name\n",
    "\n",
    "# Create temporary directory for results directory, in case experiment does not finish.\n",
    "tmp_dirs = list(tmp_results_dir.glob(\"*\"))\n",
    "\n",
    "# Only keep up to N tmp results.\n",
    "n_tmp_to_keep = 3\n",
    "if len(tmp_dirs) > (n_tmp_to_keep - 1):\n",
    "    print(f\"More than {n_tmp_to_keep} temporary results, culling to the most recent\")\n",
    "    tmps_to_delete = natsorted([str(tmp_dir) for tmp_dir in tmp_dirs])[\n",
    "        : -(n_tmp_to_keep - 1)\n",
    "    ]\n",
    "    for tmp_dir in tmps_to_delete:\n",
    "        shutil.rmtree(tmp_dir)\n",
    "        print(\"Deleted temporary results directory \", tmp_dir)\n",
    "\n",
    "experiment_results_dir = tmp_results_dir / experiment_name\n",
    "# Final target directory, to be made when experiment is complete.\n",
    "final_experiment_results_dir = results_dir / experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "keep_output"
    ]
   },
   "outputs": [],
   "source": [
    "print(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pass this object into the pytorchlightning Trainer object, for easier logging within\n",
    "# the training/testing loops.\n",
    "pl_logger = pl.loggers.TensorBoardLogger(\n",
    "    tmp_results_dir,\n",
    "    name=experiment_name,\n",
    "    version=\"\",\n",
    "    log_graph=False,\n",
    "    default_hp_metric=False,\n",
    ")\n",
    "# Use the lower-level logger for logging histograms, images, etc.\n",
    "logger = pl_logger.experiment\n",
    "\n",
    "# Create a separate txt file to log streams of events & info besides parameters & results.\n",
    "log_txt_file = Path(logger.log_dir) / \"log.txt\"\n",
    "with open(log_txt_file, \"a+\") as f:\n",
    "    f.write(f\"Experiment Name: {experiment_name}\\n\")\n",
    "    f.write(f\"Timestamp: {ts}\\n\")\n",
    "    # cap is defined in an ipython magic command\n",
    "    f.write(f\"Environment and Hardware Info:\\n {cap}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parameters and Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dict to keep track of experiment configuration parameters. Will not be logged to\n",
    "# tensorboard.\n",
    "exp_params = Addict()\n",
    "# Dict to keep track of tensorboard hparams that we *specifically* want to compare\n",
    "# between runs.\n",
    "compare_hparams = Addict(hparam=Addict(), metric=Addict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Voxel sizes for the mean downsampling.\n",
    "source_vox_size = 1.25\n",
    "target_vox_size = 2.0\n",
    "\n",
    "downsample_factor = target_vox_size / source_vox_size\n",
    "# Include b=0 shells and b=1000 shells for DTI fitting.\n",
    "bval_range = (0, 1500)\n",
    "dti_fit_method = \"WLS\"\n",
    "exp_params.update(\n",
    "    {\n",
    "        \"downsample_factor\": downsample_factor,\n",
    "        \"source_vox_size\": source_vox_size,\n",
    "        \"target_vox_size\": target_vox_size,\n",
    "        \"bval_range\": bval_range,\n",
    "        \"dti_fit_method\": dti_fit_method,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Patch Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Patch parameters\n",
    "batch_size = 12\n",
    "# 6 channels for the 6 DTI components\n",
    "channels = 6\n",
    "\n",
    "# Output patch shapes\n",
    "h_out = 14\n",
    "w_out = 14\n",
    "d_out = 14\n",
    "\n",
    "# This is the factor that determines how over-extended the input patch should be\n",
    "# relative to the size of the full-res patch.\n",
    "# $low_res_patch_dim = \\frac{full_res_patch_dim}{downsample_factor} \\times low_res_sample_extension$\n",
    "# A value of 1 indicates that the input patch dims will be exactly divided by the\n",
    "# downsample factor. A dilation > 1 increases the \"spatial extent\" of the input\n",
    "# patch, providing information outside of the target HR patch.\n",
    "low_res_sample_extension = 1.556\n",
    "\n",
    "# Output shape after shuffling.\n",
    "output_patch_shape = (channels, h_out, w_out, d_out)\n",
    "output_spatial_patch_shape = output_patch_shape[1:]\n",
    "\n",
    "# Two-step upsampling?\n",
    "two_step_upsample = True\n",
    "\n",
    "if two_step_upsample:\n",
    "    diqt_downsample_factor = np.ceil(downsample_factor).astype(int)\n",
    "    interp_out_vox_size = source_vox_size\n",
    "    interp_in_vox_size = target_vox_size / diqt_downsample_factor\n",
    "\n",
    "    interp_downsample_factor = interp_in_vox_size / interp_out_vox_size\n",
    "\n",
    "    # Input size to non-integer interpolated downsampleing.\n",
    "    interp_h_in = np.ceil(h_out / interp_downsample_factor)\n",
    "    interp_w_in = np.ceil(w_out / interp_downsample_factor)\n",
    "    interp_d_in = np.ceil(d_out / interp_downsample_factor)\n",
    "\n",
    "    # Input shape to ESPCN shuffling.\n",
    "    shuffle_h_in = interp_h_in / diqt_downsample_factor\n",
    "    shuffle_w_in = interp_w_in / diqt_downsample_factor\n",
    "    shuffle_d_in = interp_d_in / diqt_downsample_factor\n",
    "\n",
    "    # DIQT network input patch shapes\n",
    "    h_in = round(shuffle_h_in * low_res_sample_extension)\n",
    "    w_in = round(shuffle_w_in * low_res_sample_extension)\n",
    "    d_in = round(shuffle_d_in * low_res_sample_extension)\n",
    "    input_patch_shape = (channels, h_in, w_in, d_in)\n",
    "    input_spatial_patch_shape = input_patch_shape[1:]\n",
    "\n",
    "    # Patch size in FR-space when accounting for the low-res over-extension/over-sampling\n",
    "    # factor.\n",
    "    fr_extension_patch_size = tuple(\n",
    "        np.asarray(input_spatial_patch_shape) * downsample_factor\n",
    "    )\n",
    "    fr_extension_amount = tuple(\n",
    "        np.ceil(\n",
    "            np.asarray(fr_extension_patch_size) - np.asarray(output_spatial_patch_shape)\n",
    "        ).astype(int)\n",
    "    )\n",
    "else:\n",
    "    # Input patch parameters\n",
    "    h_in = round(h_out / (downsample_factor) * low_res_sample_extension)\n",
    "    w_in = round(w_out / (downsample_factor) * low_res_sample_extension)\n",
    "    d_in = round(d_out / (downsample_factor) * low_res_sample_extension)\n",
    "    input_patch_shape = (channels, h_in, w_in, d_in)\n",
    "    input_spatial_patch_shape = input_patch_shape[1:]\n",
    "\n",
    "    # Pre-shuffle output patch sizes.\n",
    "    rounded_downsample_factor = int(np.ceil(downsample_factor))\n",
    "    unshuffled_channels_out = int(channels * rounded_downsample_factor**3)\n",
    "    # Output before shuffling\n",
    "    unshuffled_output_patch_shape = (\n",
    "        unshuffled_channels_out,\n",
    "        h_out // rounded_downsample_factor,\n",
    "        w_out // rounded_downsample_factor,\n",
    "        d_out // rounded_downsample_factor,\n",
    "    )\n",
    "\n",
    "    # Patch size in FR-space when accounting for the low-res over-extension/over-sampling\n",
    "    # factor.\n",
    "    fr_extension_patch_size = tuple(\n",
    "        np.ceil(np.asarray(input_spatial_patch_shape) * downsample_factor).astype(int)\n",
    "    )\n",
    "    fr_extension_amount = tuple(\n",
    "        np.asarray(fr_extension_patch_size) - np.asarray(output_spatial_patch_shape)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_params.patch.update(\n",
    "    batch_size=batch_size,\n",
    "    channels=channels,\n",
    "    low_res_sample_extension=low_res_sample_extension,\n",
    "    input_shape=input_patch_shape,\n",
    "    output_shape=output_patch_shape,\n",
    "    two_step_upsample=two_step_upsample,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data parameters.\n",
    "num_subject_samples = 16\n",
    "# Should the data be normalized as a pre-processing step?\n",
    "# Can be:\n",
    "# { None, \"channels\" }\n",
    "data_norm_method = \"channels\"\n",
    "# Quantile range of DTI channels that will have voxel intensities clamped.\n",
    "# Only has an effect if using data normalization.\n",
    "\n",
    "# In other words, for each subject, for each channel, for each vol in {FR, LR}, any voxel\n",
    "# values <= the first quantile value will be clamped to that quantile, and any voxels\n",
    "# >= the second quantile value will be clamped to that quantile.\n",
    "clamp_quantiles = (0.0001, 0.9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_params.data.update(\n",
    "    num_subject=num_subject_samples,\n",
    "    data_norm_method=data_norm_method,\n",
    "    clamp_quantiles=clamp_quantiles,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Training and Testing Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training/testing parameters.\n",
    "# Percentages will be rounded off to the nearest subject, with the test and validation\n",
    "# sizes rounded *up*, ensuring at least 1 subject in each.\n",
    "test_percent = 0.4\n",
    "val_percent = 0.2\n",
    "train_percent = 1 - (test_percent + val_percent)\n",
    "\n",
    "# NN parameters.\n",
    "max_epochs = 200\n",
    "network_norm_method = None\n",
    "train_loss_name = \"mse\"\n",
    "\n",
    "# Optimization parameters.\n",
    "opt_name = \"Adam\"\n",
    "opt_params = {\"lr\": 7e-4, \"betas\": (0.9, 0.999)}\n",
    "\n",
    "# Spline interpolation baseline parameters.\n",
    "spline_interp_order = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of voxels to dilate the mask in FR space.\n",
    "# Just make it 0...\n",
    "dilation_size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_params.update(test_percent=test_percent)\n",
    "exp_params.train.update(\n",
    "    train_percent=train_percent, max_epochs=max_epochs, train_loss_name=train_loss_name\n",
    ")\n",
    "exp_params.nn.update(network_norm_method=network_norm_method)\n",
    "exp_params.opt.update(opt_params)\n",
    "exp_params.opt.name = opt_name\n",
    "exp_params.spline.update(order=spline_interp_order)\n",
    "exp_params.preproc.update(dilation_size=dilation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(log_txt_file, \"a+\") as f:\n",
    "    f.write(pprint.pformat(exp_params) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def patch_center(\n",
    "    patch: torch.Tensor, sub_sample_strategy=\"lower\", keepdim=False\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Extract 3D multi-channel patch center.\n",
    "\n",
    "    Expects patch of shape '[B x C x] W x H x D'\n",
    "\n",
    "    sub_sample_strategy: str\n",
    "        Strategy for handling center coordinates of even-sized dimensions.\n",
    "        Options:\n",
    "            Strategies over indices:\n",
    "                'lower': Take the voxel to the left of the center.\n",
    "                'upper': Take the voxel to the right of the center.\n",
    "\n",
    "            Strategies over multi-dimensional voxels:\n",
    "                'max': Take the max of all center voxels.\n",
    "                'min': Take the minimum of all center voxels.\n",
    "                'mean': Take the average of all center voxels.\n",
    "                'agg': Don't reduce at all, and return the center voxels.\n",
    "    \"\"\"\n",
    "    strategy_fn = {\n",
    "        \"idx_fns\": {\n",
    "            \"lower\".casefold(): lambda i: int(i),\n",
    "            \"upper\".casefold(): lambda i: int(i) + 1,\n",
    "        },\n",
    "        \"vox_fns\": {\n",
    "            \"max\".casefold(): lambda p: torch.amax(p, dim=(-3, -2, -1), keepdim=True),\n",
    "            \"min\".casefold(): lambda p: torch.amin(p, dim=(-3, -2, -1), keepdim=True),\n",
    "            \"mean\".casefold(): lambda p: p.mean(dim=(-3, -2, -1), keepdim=True),\n",
    "            \"agg\".casefold(): lambda p: p,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    strat = sub_sample_strategy.casefold()\n",
    "    if (strat not in strategy_fn[\"idx_fns\"].keys()) and (\n",
    "        strat not in strategy_fn[\"vox_fns\"].keys()\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"ERROR: Invalid strategy; got {sub_sample_strategy}, expected one of\"\n",
    "            + f\"{list(strategy_fn['idx_fns'].keys()) + list(strategy_fn['vox_fns'].keys())}\"\n",
    "        )\n",
    "    patch_spatial_shape = patch.shape[-3:]\n",
    "    centers = torch.as_tensor(patch_spatial_shape) / 2\n",
    "    center = list()\n",
    "    for dim in centers:\n",
    "        if int(dim) != dim:\n",
    "            dim = slice(int(math.floor(dim)), int(math.ceil(dim)))\n",
    "        elif strat in strategy_fn[\"idx_fns\"].keys():\n",
    "            dim = int(strategy_fn[\"idx_fns\"][strat](dim))\n",
    "            dim = slice(dim, dim + 1)\n",
    "        elif strat in strategy_fn[\"vox_fns\"].keys():\n",
    "            dim = slice(int(dim), int(dim) + 2)\n",
    "        else:\n",
    "            raise ValueError(\"ERROR: Invalid strategy\")\n",
    "        center.append(dim)\n",
    "\n",
    "    center_patches = patch[..., center[0], center[1], center[2]]\n",
    "\n",
    "    if (\n",
    "        center_patches.shape[-3:] != (1, 1, 1)\n",
    "        and strat in strategy_fn[\"vox_fns\"].keys()\n",
    "    ):\n",
    "        center_patches = strategy_fn[\"vox_fns\"][strat](center_patches)\n",
    "\n",
    "    if not keepdim:\n",
    "        center_patches = center_patches.squeeze()\n",
    "\n",
    "    return center_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BoxplotStats = collections.namedtuple(\n",
    "    \"BoxplotStats\",\n",
    "    [\"low_outliers\", \"low\", \"q1\", \"median\", \"q3\", \"high\", \"high_outliers\"],\n",
    ")\n",
    "\n",
    "\n",
    "def batch_boxplot_stats(batch):\n",
    "    \"\"\"Quick calculation of a batch of 1D values for showing boxplot stats.\"\"\"\n",
    "    q1, median, q3 = np.quantile(batch, q=[0.25, 0.5, 0.75], axis=1)\n",
    "    iqr = q3 - q1\n",
    "    low = q1 - (1.5 * iqr)\n",
    "    high = q3 + (1.5 * iqr)\n",
    "    low_outliers = list()\n",
    "    high_outliers = list()\n",
    "    # Number of outliers may be different for each batch, so it needs to be a list of\n",
    "    # arrays.\n",
    "    for i_batch in range(len(batch)):\n",
    "        batch_i = batch[i_batch]\n",
    "        low_i = low[i_batch]\n",
    "        low_outliers.append(batch_i[np.where(batch_i < low_i)])\n",
    "        high_i = high[i_batch]\n",
    "        high_outliers.append(batch_i[np.where(batch_i > high_i)])\n",
    "\n",
    "    return BoxplotStats(low_outliers, low, q1, median, q3, high, high_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quick check on full volume/batch distributions.\n",
    "\n",
    "\n",
    "def desc_channel_dists(vols, mask=None):\n",
    "    t_vols = torch.as_tensor(vols)\n",
    "\n",
    "    if t_vols.ndim == 4:\n",
    "        t_vols = t_vols[None, ...]\n",
    "\n",
    "    if mask is not None:\n",
    "        t_mask = torch.as_tensor(mask)\n",
    "        if mask.ndim == 4:\n",
    "            mask = mask[0]\n",
    "    else:\n",
    "        t_mask = torch.ones_like(t_vols[0, 0]).bool()\n",
    "\n",
    "    results = \"means | vars\\n\"\n",
    "    for t_vol_i in t_vols:\n",
    "        masked_vol = torch.masked_select(t_vol_i, t_mask).reshape(t_vol_i.shape[0], -1)\n",
    "        mean_i = torch.mean(masked_vol, dim=1)\n",
    "        var_i = torch.var(masked_vol, dim=1)\n",
    "        mvs = [\n",
    "            (f\"{mv[0]} | {mv[1]}\\n\")\n",
    "            for mv in torch.stack([mean_i, var_i], dim=-1).tolist()\n",
    "        ]\n",
    "        results = results + \"\".join(mvs)\n",
    "        results = results + (\"=\" * (len(mvs[-1]) - 1)) + \"\\n\"\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_dti_box_row(\n",
    "    fig,\n",
    "    grid,\n",
    "    row_idx: int,\n",
    "    subj_id: int,\n",
    "    shared_axs_rows: list,\n",
    "    shared_axs_cols: list,\n",
    "    fr_vol: np.ndarray,\n",
    "    lr_vol: np.ndarray,\n",
    "    colors: list = list(sns.color_palette(\"Set2\", n_colors=2)),\n",
    "):\n",
    "\n",
    "    dti_channel_names = [\n",
    "        \"$D_{xx}$\",\n",
    "        \"$D_{xy}$\",\n",
    "        \"$D_{yy}$\",\n",
    "        \"$D_{xz}$\",\n",
    "        \"$D_{yz}$\",\n",
    "        \"$D_{zz}$\",\n",
    "    ]\n",
    "\n",
    "    for i_channel, channel_name in enumerate(dti_channel_names):\n",
    "        cell = grid[row_idx, i_channel]\n",
    "\n",
    "        ax = fig.add_subplot(\n",
    "            cell,\n",
    "            sharex=shared_axs_cols[channel_name],\n",
    "            sharey=shared_axs_rows[subj_id],\n",
    "        )\n",
    "        if shared_axs_cols[channel_name] is None:\n",
    "            shared_axs_cols[channel_name] = ax\n",
    "        if shared_axs_rows[subj_id] is None:\n",
    "            shared_axs_rows[subj_id] = ax\n",
    "\n",
    "        #         quantile_outlier_cutoff = (0.1, 0.9)\n",
    "        fr_channel = fr_vol[i_channel]\n",
    "        #         fr_nn = fr_nn[\n",
    "        #             (np.quantile(fr_nn, quantile_outlier_cutoff[0]) <= fr_nn)\n",
    "        #             & (fr_nn <= np.quantile(fr_nn, quantile_outlier_cutoff[1]))\n",
    "        #         ]\n",
    "        lr_channel = lr_vol[i_channel]\n",
    "        #         lr_nn = lr_nn[\n",
    "        #             (np.quantile(lr_nn, quantile_outlier_cutoff[0]) <= lr_nn)\n",
    "        #             & (lr_nn <= np.quantile(lr_nn, quantile_outlier_cutoff[1]))\n",
    "        #         ]\n",
    "        #         fr_norm = normed_fr_vol[i_channel].detach().cpu().numpy()\n",
    "        #         lr_norm = normed_lr_vol[i_channel].detach().cpu().numpy()\n",
    "\n",
    "        num_fr_vox = len(fr_channel)\n",
    "        num_lr_vox = len(lr_channel)\n",
    "\n",
    "        resolution_labels = ([\"FR\",] * num_fr_vox) + (\n",
    "            [\n",
    "                \"LR\",\n",
    "            ]\n",
    "            * num_lr_vox\n",
    "        )\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"data\": np.concatenate([fr_channel, lr_channel]),\n",
    "                \"resolution\": resolution_labels,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        sns.boxenplot(\n",
    "            data=df,\n",
    "            y=\"resolution\",\n",
    "            x=\"data\",\n",
    "            orient=\"h\",\n",
    "            ax=ax,\n",
    "            palette=colors,\n",
    "            k_depth=\"proportion\",\n",
    "            outlier_prop=0.11,\n",
    "            showfliers=False,\n",
    "        )\n",
    "\n",
    "        if not cell.is_last_row():\n",
    "            plt.setp(ax.get_xticklabels(), visible=False)\n",
    "        else:\n",
    "            plt.setp(ax.get_xticklabels(), fontsize=\"x-small\", rotation=25)\n",
    "\n",
    "        if not cell.is_first_col():\n",
    "            plt.setp(ax.get_yticklabels(), visible=False)\n",
    "            ax.set_ylabel(\"\")\n",
    "        else:\n",
    "            ax.set_ylabel(subj_id)\n",
    "\n",
    "        ax.set_xlabel(\"\")\n",
    "        if cell.is_first_row():\n",
    "            ax.set_title(channel_name)\n",
    "\n",
    "    return fig, shared_axs_rows, shared_axs_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Subject ID Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "keep_output"
    ]
   },
   "outputs": [],
   "source": [
    "# Find data directories for each subject.\n",
    "subj_dirs: dict = dict()\n",
    "\n",
    "selected_ids = [\n",
    "    \"397154\",\n",
    "    \"224022\",\n",
    "    \"140117\",\n",
    "    \"751348\",\n",
    "    \"894774\",\n",
    "    \"156637\",\n",
    "    \"227432\",\n",
    "    \"303624\",\n",
    "    \"185947\",\n",
    "    \"810439\",\n",
    "    \"753251\",\n",
    "    \"644246\",\n",
    "    \"141422\",\n",
    "    \"135528\",\n",
    "    \"103010\",\n",
    "    \"700634\",\n",
    "]\n",
    "\n",
    "## Sub-set the chosen participants for dev and debugging!\n",
    "selected_ids = random.sample(selected_ids, num_subject_samples)\n",
    "warnings.warn(\n",
    "    \"WARNING: Sub-selecting participants for dev and debugging. \"\n",
    "    + f\"Subj IDs selected: {selected_ids}\"\n",
    ")\n",
    "# ### A nested warning! For debugging only.\n",
    "# warnings.warn(\"WARNING: Mixing training and testing subjects\")\n",
    "# selected_ids.append(selected_ids[0])\n",
    "# ###\n",
    "##\n",
    "\n",
    "selected_ids = natsorted(list(map(lambda s: int(s), selected_ids)))\n",
    "\n",
    "for subj_id in selected_ids:\n",
    "    subj_dirs[subj_id] = data_dir / f\"{subj_id}/T1w/Diffusion\"\n",
    "    assert subj_dirs[subj_id].exists()\n",
    "subj_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 90 scans are taken from the $b=1000 \\ s/mm^2$. However, the $b=0$ shells are still required for fitting the diffusion tensors (DTI's), so those will need to be kept, too.\n",
    "\n",
    "To find those, sub-select with the $0 < bvals < 1500$, or roughly thereabout. A b-val of $995$ or $1005$ still counts as a b=1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(log_txt_file, \"a+\") as f:\n",
    "    f.write(f\"Selected Subjects: {selected_ids}\\n\")\n",
    "\n",
    "logger.add_text(\"subjs\", pprint.pformat(selected_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the transformation pipeline.\n",
    "\n",
    "preproc_transforms = torchio.Compose(\n",
    "    [\n",
    "        torchio.transforms.ToCanonical(include=(\"dwi\", \"brain_mask\"), copy=False),\n",
    "        pitn.transforms.BValSelectionTransform(\n",
    "            bval_range=bval_range,\n",
    "            bval_key=\"bvals\",\n",
    "            bvec_key=\"bvecs\",\n",
    "            include=\"dwi\",\n",
    "            copy=False,\n",
    "        ),\n",
    "        # Pad by the dilation factor, then dilate the mask.\n",
    "        torchio.transforms.Pad(\n",
    "            dilation_size,\n",
    "            padding_mode=0,\n",
    "            include=(\"dwi\", \"brain_mask\"),\n",
    "            copy=False,\n",
    "        ),\n",
    "        # Pad by the amount of extension voxels in FR space, so LR indices cannot\n",
    "        # go out of bounds.\n",
    "        torchio.transforms.Pad(\n",
    "            fr_extension_amount,\n",
    "            padding_mode=0,\n",
    "            include=(\"dwi\", \"brain_mask\"),\n",
    "            copy=False,\n",
    "        ),\n",
    "        pitn.transforms.FractionalMeanDownsampleTransform(\n",
    "            source_vox_size=exp_params.source_vox_size,\n",
    "            target_vox_size=exp_params.target_vox_size,\n",
    "            include=(\"dwi\", \"brain_mask\"),\n",
    "            keep={\"dwi\": \"fr_dwi\", \"brain_mask\": \"fr_brain_mask\"},\n",
    "            copy=False,\n",
    "        ),\n",
    "        pitn.transforms.RenameImageTransform(\n",
    "            {\"dwi\": \"lr_dwi\", \"brain_mask\": \"lr_brain_mask\"}, copy=False\n",
    "        ),\n",
    "        pitn.transforms.FitDTITransform(\n",
    "            \"bvals\",\n",
    "            \"bvecs\",\n",
    "            \"fr_brain_mask\",\n",
    "            fit_method=dti_fit_method,\n",
    "            include=(\"fr_dwi\"),\n",
    "            #             cache_dir=\"./.cache\",\n",
    "            copy=False,\n",
    "        ),\n",
    "        pitn.transforms.FitDTITransform(\n",
    "            \"bvals\",\n",
    "            \"bvecs\",\n",
    "            \"lr_brain_mask\",\n",
    "            fit_method=dti_fit_method,\n",
    "            include=(\"lr_dwi\"),\n",
    "            #             cache_dir=\"./.cache\",\n",
    "            copy=False,\n",
    "        ),\n",
    "        pitn.transforms.RenameImageTransform(\n",
    "            {\"fr_dwi\": \"gt_dti\", \"lr_dwi\": \"lr_dti\"}, copy=False\n",
    "        ),\n",
    "        pitn.transforms.ImageToDictTransform(\n",
    "            include=(\"lr_dti\", \"lr_brain_mask\"), copy=False\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loading & Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## DEBUG fractional downsampling\n",
    "\n",
    "\n",
    "# c = torchio.Compose(preproc_transforms.transforms[:7])\n",
    "# t = list()\n",
    "# for subj_id, subj_dir in subj_dirs.items():\n",
    "\n",
    "#     # Sub-select volumes with only bvals in a certain range. E.x. bvals <= 1100 mm/s^2,\n",
    "#     # a.k.a. only the b=0 and b=1000 shells.\n",
    "#     bvals = torch.as_tensor(np.loadtxt(subj_dir / \"bvals\").astype(int))\n",
    "#     bvecs = torch.as_tensor(np.loadtxt(subj_dir / \"bvecs\"))\n",
    "#     # Reshape to be N x 3\n",
    "#     if bvecs.shape[0] == 3:\n",
    "#         bvecs = bvecs.T\n",
    "\n",
    "#     brain_mask = torchio.LabelMap(\n",
    "#         subj_dir / \"nodif_brain_mask.nii.gz\",\n",
    "#         type=torchio.LABEL,\n",
    "#         channels_last=False,\n",
    "#     )\n",
    "#     brain_mask.set_data(brain_mask.data.bool())\n",
    "#     mask_volume = brain_mask[\"data\"].sum()\n",
    "\n",
    "#     dwi = torchio.ScalarImage(\n",
    "#         subj_dir / \"data.nii.gz\",\n",
    "#         type=torchio.INTENSITY,\n",
    "#         bvals=bvals,\n",
    "#         bvecs=bvecs,\n",
    "#         reader=pitn.io.nifti_reader,\n",
    "#         channels_last=True,\n",
    "#     )\n",
    "\n",
    "#     subject_dict = torchio.Subject(subj_id=subj_id, dwi=dwi, brain_mask=brain_mask)\n",
    "\n",
    "#     preproc_subj = c(subject_dict)\n",
    "#     t.append(preproc_subj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_stats_fmt = \"github\"\n",
    "summary_stats_header = [\n",
    "    \"Subj ID\",\n",
    "    \"Resolution\",\n",
    "    \"Channel Index\",\n",
    "    \"Mean\",\n",
    "    \"Var\",\n",
    "    \"Num Outliers (Lower)\",\n",
    "    \"Low\",\n",
    "    \"25th Percentile\",\n",
    "    \"Median\",\n",
    "    \"75th Percentile\",\n",
    "    \"High\",\n",
    "    \"Num Outliers (Upper)\",\n",
    "]\n",
    "dti_channel_names = [\"D xx\", \"D xy\", \"D yy\", \"D xz\", \"D yz\", \"D zz\"]\n",
    "n_subj = len(subj_dirs.keys())\n",
    "colors = list(sns.color_palette(\"Set2\", n_colors=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import all image data into a sequence of `torchio.Subject` objects.\n",
    "subj_data: dict = dict()\n",
    "\n",
    "subj_stats = dict()\n",
    "# Grid plot for displaying enhanced boxplots of subject DTI image intensities.\n",
    "plt.clf()\n",
    "\n",
    "fig_non_norm = plt.figure(dpi=120, figsize=(6 * 2.5, 1.5 * n_subj))\n",
    "fig_non_norm.suptitle(\"Non-Normalized Subject DTI Distributions\")\n",
    "grid_non_norm = mpl.gridspec.GridSpec(\n",
    "    nrows=len(subj_dirs.keys()), ncols=len(dti_channel_names), figure=fig_non_norm\n",
    ")\n",
    "\n",
    "share_axs_non_norm = collections.defaultdict(\n",
    "    lambda: None,\n",
    "    row=collections.defaultdict(lambda: None),\n",
    "    col=collections.defaultdict(lambda: None),\n",
    ")\n",
    "\n",
    "for k in summary_stats_header:\n",
    "    subj_stats[k] = list()\n",
    "\n",
    "for subj_id, subj_dir in subj_dirs.items():\n",
    "\n",
    "    # Sub-select volumes with only bvals in a certain range. E.x. bvals <= 1100 mm/s^2,\n",
    "    # a.k.a. only the b=0 and b=1000 shells.\n",
    "    bvals = torch.as_tensor(np.loadtxt(subj_dir / \"bvals\").astype(int))\n",
    "    bvecs = torch.as_tensor(np.loadtxt(subj_dir / \"bvecs\"))\n",
    "    # Reshape to be N x 3\n",
    "    if bvecs.shape[0] == 3:\n",
    "        bvecs = bvecs.T\n",
    "\n",
    "    brain_mask = torchio.LabelMap(\n",
    "        subj_dir / \"nodif_brain_mask.nii.gz\",\n",
    "        type=torchio.LABEL,\n",
    "        channels_last=False,\n",
    "    )\n",
    "    brain_mask.set_data(brain_mask.data.bool())\n",
    "    mask_volume = brain_mask[\"data\"].sum()\n",
    "\n",
    "    dwi = torchio.ScalarImage(\n",
    "        subj_dir / \"data.nii.gz\",\n",
    "        type=torchio.INTENSITY,\n",
    "        bvals=bvals,\n",
    "        bvecs=bvecs,\n",
    "        reader=pitn.io.nifti_reader,\n",
    "        channels_last=True,\n",
    "    )\n",
    "\n",
    "    subject_dict = torchio.Subject(subj_id=subj_id, dwi=dwi, brain_mask=brain_mask)\n",
    "    preproc_subj = preproc_transforms(subject_dict)\n",
    "\n",
    "    # Print and log some statistics of the subject data.\n",
    "    # Process FR (ground truth) volumes.\n",
    "    # Grab FR voxels within the mask.\n",
    "    # The normalized target is named \"fr_dti\", whereas the source/ground truth DTI\n",
    "    # (without normalization or clamping) is named \"gt_dti\".\n",
    "    fr_vol = preproc_subj.gt_dti.tensor\n",
    "    fr_mask = preproc_subj.fr_brain_mask.tensor.bool()\n",
    "    masked_fr_vol = torch.masked_select(fr_vol, fr_mask).reshape(fr_vol.shape[0], -1)\n",
    "    # Estimate FR means and vars.\n",
    "    fr_channel_means = masked_fr_vol.mean(dim=1)\n",
    "    fr_channel_means = fr_channel_means.reshape(-1, 1, 1, 1)\n",
    "    fr_channel_vars = masked_fr_vol.var(dim=1)\n",
    "    fr_channel_vars = fr_channel_vars.reshape(-1, 1, 1, 1)\n",
    "\n",
    "    # Add FR stats to the summary table.\n",
    "    fr_vol_stats = batch_boxplot_stats(masked_fr_vol)\n",
    "    subj_stats[\"Subj ID\"].extend(\n",
    "        list(itertools.repeat(subj_id, len(fr_vol_stats.median)))\n",
    "    )\n",
    "    subj_stats[\"Resolution\"].extend(\n",
    "        list(itertools.repeat(tuple(fr_vol.shape[1:]), len(fr_vol_stats.median)))\n",
    "    )\n",
    "    subj_stats[\"Channel Index\"].extend(dti_channel_names)\n",
    "    subj_stats[\"Mean\"].extend(fr_channel_means.cpu().flatten().tolist())\n",
    "    subj_stats[\"Var\"].extend(fr_channel_vars.cpu().flatten().tolist())\n",
    "    # Append FR boxplot stats to their corresponding fields. In other words, all columns\n",
    "    # after the var column.\n",
    "    for i, field in enumerate(\n",
    "        summary_stats_header[(summary_stats_header.index(\"Var\") + 1) :]\n",
    "    ):\n",
    "        if \"Num\" in field:\n",
    "            subj_stats[field].extend(list(map(len, fr_vol_stats[i])))\n",
    "        else:\n",
    "            subj_stats[field].extend(fr_vol_stats[i])\n",
    "\n",
    "    # Process LR volumes\n",
    "    # Grab LR voxels within the mask.\n",
    "    lr_vol = preproc_subj.lr_dti[\"data\"]\n",
    "    lr_mask = preproc_subj.lr_brain_mask[\"data\"].bool()\n",
    "    masked_lr_vol = torch.masked_select(lr_vol, lr_mask).reshape(lr_vol.shape[0], -1)\n",
    "\n",
    "    lr_channel_means = masked_lr_vol.mean(dim=1)\n",
    "    lr_channel_means = lr_channel_means.reshape(-1, 1, 1, 1)\n",
    "    lr_channel_vars = masked_lr_vol.var(dim=1)\n",
    "    lr_channel_vars = lr_channel_vars.reshape(-1, 1, 1, 1)\n",
    "\n",
    "    # Add LR stats to the summary table.\n",
    "    lr_vol_stats = batch_boxplot_stats(masked_lr_vol)\n",
    "    subj_stats[\"Subj ID\"].extend(\n",
    "        list(itertools.repeat(subj_id, len(fr_vol_stats.median)))\n",
    "    )\n",
    "    subj_stats[\"Resolution\"].extend(\n",
    "        list(itertools.repeat(tuple(lr_vol.shape[1:]), len(lr_vol_stats.median)))\n",
    "    )\n",
    "    subj_stats[\"Channel Index\"].extend(dti_channel_names)\n",
    "    subj_stats[\"Mean\"].extend(lr_channel_means.cpu().flatten().tolist())\n",
    "    subj_stats[\"Var\"].extend(lr_channel_vars.cpu().flatten().tolist())\n",
    "    # Append LR boxplot stats to their corresponding fields. In other words, all columns\n",
    "    # after the var column.\n",
    "    for i, field in enumerate(\n",
    "        summary_stats_header[(summary_stats_header.index(\"Var\") + 1) :]\n",
    "    ):\n",
    "        if \"Num\" in field:\n",
    "            subj_stats[field].extend(list(map(len, lr_vol_stats[i])))\n",
    "        else:\n",
    "            subj_stats[field].extend(lr_vol_stats[i])\n",
    "\n",
    "    subj_idx = list(subj_dirs.keys()).index(subj_id)\n",
    "    # Generate row of boxplots for the non-normalized volumes.\n",
    "    fig_non_norm, share_axs_rows, share_axs_cols = plot_dti_box_row(\n",
    "        fig_non_norm,\n",
    "        grid_non_norm,\n",
    "        row_idx=subj_idx,\n",
    "        subj_id=subj_id,\n",
    "        shared_axs_rows=share_axs_non_norm[\"row\"],\n",
    "        shared_axs_cols=share_axs_non_norm[\"col\"],\n",
    "        fr_vol=masked_fr_vol.detach().cpu().numpy(),\n",
    "        lr_vol=masked_lr_vol.detach().cpu().numpy(),\n",
    "    )\n",
    "    share_axs_non_norm[\"row\"] = share_axs_rows\n",
    "    share_axs_non_norm[\"col\"] = share_axs_cols\n",
    "\n",
    "    # Finalize this subject.\n",
    "    subj_data[subj_id] = preproc_subj\n",
    "\n",
    "    print(\"=\" * 20)\n",
    "\n",
    "\n",
    "print(\"===Data Loaded & Transformed===\")\n",
    "\n",
    "subj_stats_str = tabulate(\n",
    "    subj_stats, headers=summary_stats_header, tablefmt=summary_stats_fmt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fig_non_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### !Temporary! Will be removed after fitting UVA DTI.\n",
    "# # Import all image data into a sequence of `torchio.Subject` objects.\n",
    "# subj_data: dict = dict()\n",
    "\n",
    "# subj_stats = dict()\n",
    "# # Grid plot for displaying enhanced boxplots of subject DTI image intensities.\n",
    "# plt.clf()\n",
    "\n",
    "# fig_non_norm = plt.figure(dpi=120, figsize=(6 * 2.5, 1.5 * n_subj))\n",
    "# fig_non_norm.suptitle(\"Non-Normalized Subject DTI Distributions\")\n",
    "# grid_non_norm = mpl.gridspec.GridSpec(\n",
    "#     nrows=len(subj_dirs.keys()), ncols=len(dti_channel_names), figure=fig_non_norm\n",
    "# )\n",
    "\n",
    "# share_axs_non_norm = collections.defaultdict(\n",
    "#     lambda: None,\n",
    "#     row=collections.defaultdict(lambda: None),\n",
    "#     col=collections.defaultdict(lambda: None),\n",
    "# )\n",
    "\n",
    "# for k in summary_stats_header:\n",
    "#     subj_stats[k] = list()\n",
    "\n",
    "# subj_dirs = {\"001\": pathlib.Path(os.environ[\"DATA_DIR\"]) / \"uva/001\"}\n",
    "\n",
    "# for subj_id, subj_dir in subj_dirs.items():\n",
    "\n",
    "#     # Sub-select volumes with only bvals in a certain range. E.x. bvals <= 1100 mm/s^2,\n",
    "#     # a.k.a. only the b=0 and b=1000 shells.\n",
    "#     bvals = torch.as_tensor(np.loadtxt(subj_dir / \"sub-001_ses-01_run-2_dwi.bval\").astype(int))\n",
    "#     bvecs = torch.as_tensor(np.loadtxt(subj_dir / \"sub-001_ses-01_run-2_dwi.bvec\"))\n",
    "#     # Reshape to be N x 3\n",
    "#     if bvecs.shape[0] == 3:\n",
    "#         bvecs = bvecs.T\n",
    "\n",
    "#     brain_mask = torchio.LabelMap(\n",
    "#         Path(\n",
    "#             \"/srv/tmp/pitn/uva/chronic_pain_head_and_neck/derivatives/diffusion/sub-001/ses-01/mask/sub-001_space-orig_ses-01_run-2_mask.nii.gz\"\n",
    "#         ),\n",
    "#         #         subj_dir / \"nodif_brain_mask.nii.gz\",\n",
    "#         type=torchio.LABEL,\n",
    "#         channels_last=False,\n",
    "#     )\n",
    "#     brain_mask.set_data(brain_mask.data.bool())\n",
    "#     mask_volume = brain_mask[\"data\"].sum()\n",
    "\n",
    "#     dwi = torchio.ScalarImage(\n",
    "#         subj_dir / \"sub-001_ses-01_run-2_dwi_epi.nii.gz\",\n",
    "#         type=torchio.INTENSITY,\n",
    "#         bvals=bvals,\n",
    "#         bvecs=bvecs,\n",
    "#         reader=pitn.io.nifti_reader,\n",
    "#         channels_last=True,\n",
    "#     )\n",
    "\n",
    "#     subject_dict = torchio.Subject(subj_id=subj_id, dwi=dwi, brain_mask=brain_mask)\n",
    "#     preproc_subj = preproc_transforms(subject_dict)\n",
    "\n",
    "#     # Print and log some statistics of the subject data.\n",
    "#     # Process FR (ground truth) volumes.\n",
    "#     # Grab FR voxels within the mask.\n",
    "#     # The normalized target is named \"fr_dti\", whereas the source/ground truth DTI\n",
    "#     # (without normalization or clamping) is named \"gt_dti\".\n",
    "#     fr_vol = preproc_subj.gt_dti.tensor\n",
    "#     fr_mask = preproc_subj.fr_brain_mask.tensor.bool()\n",
    "#     masked_fr_vol = torch.masked_select(fr_vol, fr_mask).reshape(fr_vol.shape[0], -1)\n",
    "#     # Estimate FR means and vars.\n",
    "#     fr_channel_means = masked_fr_vol.mean(dim=1)\n",
    "#     fr_channel_means = fr_channel_means.reshape(-1, 1, 1, 1)\n",
    "#     fr_channel_vars = masked_fr_vol.var(dim=1)\n",
    "#     fr_channel_vars = fr_channel_vars.reshape(-1, 1, 1, 1)\n",
    "\n",
    "#     # Add FR stats to the summary table.\n",
    "#     fr_vol_stats = batch_boxplot_stats(masked_fr_vol)\n",
    "#     subj_stats[\"Subj ID\"].extend(\n",
    "#         list(itertools.repeat(subj_id, len(fr_vol_stats.median)))\n",
    "#     )\n",
    "#     subj_stats[\"Resolution\"].extend(\n",
    "#         list(itertools.repeat(tuple(fr_vol.shape[1:]), len(fr_vol_stats.median)))\n",
    "#     )\n",
    "#     subj_stats[\"Channel Index\"].extend(dti_channel_names)\n",
    "#     subj_stats[\"Mean\"].extend(fr_channel_means.cpu().flatten().tolist())\n",
    "#     subj_stats[\"Var\"].extend(fr_channel_vars.cpu().flatten().tolist())\n",
    "#     # Append FR boxplot stats to their corresponding fields. In other words, all columns\n",
    "#     # after the var column.\n",
    "#     for i, field in enumerate(\n",
    "#         summary_stats_header[(summary_stats_header.index(\"Var\") + 1) :]\n",
    "#     ):\n",
    "#         if \"Num\" in field:\n",
    "#             subj_stats[field].extend(list(map(len, fr_vol_stats[i])))\n",
    "#         else:\n",
    "#             subj_stats[field].extend(fr_vol_stats[i])\n",
    "\n",
    "# #     # Process LR volumes\n",
    "# #     # Grab LR voxels within the mask.\n",
    "# #     lr_vol = preproc_subj.lr_dti[\"data\"]\n",
    "# #     lr_mask = preproc_subj.lr_brain_mask[\"data\"].bool()\n",
    "# #     masked_lr_vol = torch.masked_select(lr_vol, lr_mask).reshape(lr_vol.shape[0], -1)\n",
    "\n",
    "# #     lr_channel_means = masked_lr_vol.mean(dim=1)\n",
    "# #     lr_channel_means = lr_channel_means.reshape(-1, 1, 1, 1)\n",
    "# #     lr_channel_vars = masked_lr_vol.var(dim=1)\n",
    "# #     lr_channel_vars = lr_channel_vars.reshape(-1, 1, 1, 1)\n",
    "\n",
    "# #     # Add LR stats to the summary table.\n",
    "# #     lr_vol_stats = batch_boxplot_stats(masked_lr_vol)\n",
    "# #     subj_stats[\"Subj ID\"].extend(\n",
    "# #         list(itertools.repeat(subj_id, len(fr_vol_stats.median)))\n",
    "# #     )\n",
    "# #     subj_stats[\"Resolution\"].extend(\n",
    "# #         list(itertools.repeat(tuple(lr_vol.shape[1:]), len(lr_vol_stats.median)))\n",
    "# #     )\n",
    "# #     subj_stats[\"Channel Index\"].extend(dti_channel_names)\n",
    "# #     subj_stats[\"Mean\"].extend(lr_channel_means.cpu().flatten().tolist())\n",
    "# #     subj_stats[\"Var\"].extend(lr_channel_vars.cpu().flatten().tolist())\n",
    "# #     # Append LR boxplot stats to their corresponding fields. In other words, all columns\n",
    "# #     # after the var column.\n",
    "# #     for i, field in enumerate(\n",
    "# #         summary_stats_header[(summary_stats_header.index(\"Var\") + 1) :]\n",
    "# #     ):\n",
    "# #         if \"Num\" in field:\n",
    "# #             subj_stats[field].extend(list(map(len, lr_vol_stats[i])))\n",
    "# #         else:\n",
    "# #             subj_stats[field].extend(lr_vol_stats[i])\n",
    "\n",
    "# #     subj_idx = list(subj_dirs.keys()).index(subj_id)\n",
    "# #     # Generate row of boxplots for the non-normalized volumes.\n",
    "# #     fig_non_norm, share_axs_rows, share_axs_cols = plot_dti_box_row(\n",
    "# #         fig_non_norm,\n",
    "# #         grid_non_norm,\n",
    "# #         row_idx=subj_idx,\n",
    "# #         subj_id=subj_id,\n",
    "# #         shared_axs_rows=share_axs_non_norm[\"row\"],\n",
    "# #         shared_axs_cols=share_axs_non_norm[\"col\"],\n",
    "# #         fr_vol=masked_fr_vol.detach().cpu().numpy(),\n",
    "# #         lr_vol=masked_lr_vol.detach().cpu().numpy(),\n",
    "# #     )\n",
    "# #     share_axs_non_norm[\"row\"] = share_axs_rows\n",
    "# #     share_axs_non_norm[\"col\"] = share_axs_cols\n",
    "\n",
    "#     # Finalize this subject.\n",
    "#     subj_data[subj_id] = preproc_subj\n",
    "\n",
    "#     print(\"=\" * 20)\n",
    "\n",
    "\n",
    "# print(\"===Data Loaded & Transformed===\")\n",
    "\n",
    "# # subj_stats_str = tabulate(\n",
    "# #     subj_stats, headers=summary_stats_header, tablefmt=summary_stats_fmt\n",
    "# # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Save out DTIs to NIFTI files\n",
    "# # Save out full resolution DTI's and masks.\n",
    "\n",
    "# # File path relative to the `write_data_dir` where the subjects are located.\n",
    "# # Trying to follow BIDS format as much as possible...\n",
    "# prefix_dir = 'uva/derivatives/diffusion/'\n",
    "# save_dir = write_data_dir / prefix_dir\n",
    "# # Names that further define sub-directories under each subject.\n",
    "# dti_type_name = 'dti'\n",
    "# mask_type_name = 'mask'\n",
    "# for subj_id, subj_dti in subj_data.items():\n",
    "#     print(subj_id)\n",
    "#     sub_dir = save_dir / f'sub-{subj_id}'\n",
    "\n",
    "#     # Save out DTI\n",
    "#     nifti_img = nib.Nifti2Image(subj_dti['gt_dti'].data.detach().cpu().numpy(), affine=subj_dti['gt_dti'].affine)\n",
    "#     fname = f'sub-{subj_id}_dti.nii.gz'\n",
    "#     img_save_dir = sub_dir / dti_type_name\n",
    "#     img_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "#     nib.save(nifti_img, img_save_dir / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display(Markdown(\"**Stats for Ground Truth and LR DTI's, before normalization**\"))\n",
    "# display(Markdown(subj_stats_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(log_txt_file, \"a+\") as f:\n",
    "    f.write(f\"Preprocessing transformation pipeline: {str(preproc_transforms)}\\n\")\n",
    "    f.write(f\"Data Summary Statistics, no normalization:\\n {subj_stats_str}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Save out the fitted DTIs for both the full resolution and mean downsampled versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Save out DTIs to NIFTI files\n",
    "# # Save out full resolution DTI's and masks.\n",
    "\n",
    "# # File path relative to the `write_data_dir` where the subjects are located.\n",
    "# # Trying to follow BIDS format as much as possible...\n",
    "# prefix_dir = 'derivatives/diffusion/mean_downsample/scale-orig'\n",
    "# save_dir = write_data_dir / prefix_dir\n",
    "# # Names that further define sub-directories under each subject.\n",
    "# dti_type_name = 'dti'\n",
    "# mask_type_name = 'mask'\n",
    "# for subj_id, subj_dti in subj_data.items():\n",
    "#     print(subj_id)\n",
    "#     sub_dir = save_dir / f'sub-{subj_id}'\n",
    "\n",
    "#     # Save out DTI\n",
    "#     nifti_img = nib.Nifti2Image(subj_dti['gt_dti'].data.detach().cpu().numpy(), affine=subj_dti['gt_dti'].affine)\n",
    "#     fname = f'sub-{subj_id}_dti.nii.gz'\n",
    "#     img_save_dir = sub_dir / dti_type_name\n",
    "#     img_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "#     nib.save(nifti_img, img_save_dir / fname)\n",
    "\n",
    "#     # Save out mask\n",
    "#     nifti_img = nib.Nifti2Image(subj_dti['fr_brain_mask'].data.detach().cpu().numpy(), affine=subj_dti['fr_brain_mask'].affine)\n",
    "#     fname = f'sub-{subj_id}_mask.nii.gz'\n",
    "#     img_save_dir = sub_dir / mask_type_name\n",
    "#     img_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "#     nib.save(nifti_img, img_save_dir / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Save out downsampled DTIs\n",
    "# prefix_dir = 'derivatives/diffusion/mean_downsample/scale-2.00mm'\n",
    "# save_dir = write_data_dir / prefix_dir\n",
    "# # Names that further define sub-directories under each subject.\n",
    "# dti_type_name = 'dti'\n",
    "# mask_type_name = 'mask'\n",
    "# for subj_id, subj_dti in subj_data.items():\n",
    "#     print(subj_id)\n",
    "#     sub_dir = save_dir / f'sub-{subj_id}'\n",
    "\n",
    "#     # Save out DTI\n",
    "#     nifti_img = nib.Nifti2Image(subj_dti['lr_dti']['data'].detach().cpu().numpy(), affine=subj_dti['lr_dti']['affine'])\n",
    "#     fname = f'sub-{subj_id}_meandownsample-2.00mm_dti.nii.gz'\n",
    "#     img_save_dir = sub_dir / dti_type_name\n",
    "#     img_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "#     nib.save(nifti_img, img_save_dir / fname)\n",
    "\n",
    "#     # Save out mask\n",
    "#     nifti_img = nib.Nifti2Image(subj_dti['lr_brain_mask']['data'].detach().cpu().numpy(), affine=subj_dti['lr_brain_mask']['affine'])\n",
    "#     fname = f'sub-{subj_id}_meandownsample-2.00mm_mask.nii.gz'\n",
    "#     img_save_dir = sub_dir / mask_type_name\n",
    "#     img_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "#     nib.save(nifti_img, img_save_dir / fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Volume Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if isinstance(data_norm_method, str) and \"channel\" in data_norm_method.casefold():\n",
    "    normalize = True\n",
    "else:\n",
    "    normalize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Collect global statistics\n",
    "# Create digest for keeping a running estimate of global-level statistics.\n",
    "digests = Addict()\n",
    "quantiles = Addict()\n",
    "for i in range(exp_params.patch.channels):\n",
    "    digests.lr[i] = list()\n",
    "    digests.fr[i] = list()\n",
    "    quantiles.lr[i] = (-np.inf, np.inf)\n",
    "    quantiles.fr[i] = (-np.inf, np.inf)\n",
    "\n",
    "for i_channel in range(exp_params.patch.channels):\n",
    "    # FR loop\n",
    "    for subj_id, subj in subj_data.items():\n",
    "        gt_dti = subj[\"gt_dti\"].data[i_channel]\n",
    "        fr_mask = subj[\"fr_brain_mask\"].data[0].bool()\n",
    "        masked_fr_vol = torch.masked_select(gt_dti, fr_mask).view(-1)\n",
    "\n",
    "        digests.fr[i_channel].append(masked_fr_vol)\n",
    "\n",
    "    # Calculate full quantiles\n",
    "    quantile = torch.quantile(\n",
    "        torch.cat(digests.fr[i_channel]),\n",
    "        torch.as_tensor(exp_params.data.clamp_quantiles).to(masked_fr_vol),\n",
    "    )\n",
    "    # Round quantiles to resolve round-off errors.\n",
    "    # Pytorch `round()` doesn't have a decimals argument? Seriously?\n",
    "    quantile = torch.from_numpy(\n",
    "        np.round(quantile.detach().cpu().numpy(), decimals=7)\n",
    "    ).to(quantile)\n",
    "    # Store quantiles.\n",
    "    quantiles.fr[i_channel] = quantile\n",
    "    # Delete concatenated volumes.\n",
    "    digests.fr[i_channel].clear()\n",
    "\n",
    "    # LR loop\n",
    "    for subj_id, subj in subj_data.items():\n",
    "        lr_dti = subj[\"lr_dti\"][\"data\"][i_channel]\n",
    "        lr_mask = subj[\"lr_brain_mask\"][\"data\"][0].bool()\n",
    "        masked_lr_vol = torch.masked_select(lr_dti, lr_mask).view(-1)\n",
    "\n",
    "        digests.lr[i_channel].append(masked_lr_vol)\n",
    "    # Calculate full quantiles\n",
    "    quantile = torch.quantile(\n",
    "        torch.cat(digests.lr[i_channel]),\n",
    "        torch.as_tensor(exp_params.data.clamp_quantiles).to(masked_lr_vol),\n",
    "    )\n",
    "    # Round quantiles to resolve round-off errors.\n",
    "    quantile = torch.from_numpy(\n",
    "        np.round(quantile.detach().cpu().numpy(), decimals=7)\n",
    "    ).to(quantile)\n",
    "    # Store quantiles.\n",
    "    quantiles.lr[i_channel] = quantile\n",
    "    # Delete concatenated volumes.\n",
    "    digests.lr[i_channel].clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ppr(quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optional data normalization and normalized stats reporting.\n",
    "\n",
    "# Dictionary to hold the subject's summary statistics if image-level or global\n",
    "# normalization is used.\n",
    "norm_subj_stats = dict()\n",
    "# Grid plot for displaying enhanced boxplots of subject DTI image intensities.\n",
    "plt.clf()\n",
    "colors = list(sns.color_palette(\"Set2\", n_colors=2))\n",
    "fig_norm = plt.figure(dpi=120, figsize=(6 * 2.5, 1.5 * n_subj))\n",
    "fig_norm.suptitle(\"Normalized Subject DTI Distributions\")\n",
    "grid_norm = mpl.gridspec.GridSpec(\n",
    "    nrows=len(subj_dirs.keys()), ncols=len(dti_channel_names), figure=fig_norm\n",
    ")\n",
    "share_axs_norm = collections.defaultdict(\n",
    "    lambda: None,\n",
    "    row=collections.defaultdict(lambda: None),\n",
    "    col=collections.defaultdict(lambda: None),\n",
    ")\n",
    "\n",
    "for k in summary_stats_header:\n",
    "    norm_subj_stats[k] = list()\n",
    "\n",
    "# Optionally apply image-level or global normalization.\n",
    "if normalize:\n",
    "    for subj_id, subj in subj_data.items():\n",
    "\n",
    "        # Subject-and-channel-wise standardization/normalization of both the LR and FR vols.\n",
    "        # Note that LR and FR images should have the same means, but *not* the same variances.\n",
    "        fr_vol = subj.gt_dti.tensor\n",
    "        # Perform clamping of quantile ranges\n",
    "        fr_clamp_quantiles = (\n",
    "            torch.stack([q for q in quantiles.fr.values()], dim=1)\n",
    "            .to(fr_vol)\n",
    "            .view(2, 6, 1, 1, 1)\n",
    "        )\n",
    "        fr_mask = subj.fr_brain_mask.tensor.bool()\n",
    "        # Report how many voxels will be clamped.\n",
    "        num_clamps = (\n",
    "            torch.masked_select(fr_vol, fr_mask).reshape(fr_vol.shape[0], -1)\n",
    "            < fr_clamp_quantiles[0, :, 0, 0]\n",
    "        ).sum(dim=1)\n",
    "\n",
    "        num_clamps = (\n",
    "            torch.masked_select(fr_vol, fr_mask).reshape(fr_vol.shape[0], -1)\n",
    "            > fr_clamp_quantiles[1, :, 0, 0]\n",
    "        ).sum(dim=1)\n",
    "\n",
    "        print(\n",
    "            f\"     Subj {subj_id} FR, number of clamped voxels per-channel: \",\n",
    "            num_clamps.tolist(),\n",
    "        )\n",
    "        print(\n",
    "            f\"    Subj {subj_id} LR, percentage of clamped voxels per-channel: \",\n",
    "            (\n",
    "                num_clamps / torch.masked_select(fr_vol[0], fr_mask).numel() * 100\n",
    "            ).tolist(),\n",
    "        )\n",
    "        # Clamp upper and lower quantiles in the volume.\n",
    "        fr_vol = torch.where(\n",
    "            fr_vol < fr_clamp_quantiles[0], fr_clamp_quantiles[0], fr_vol\n",
    "        )\n",
    "        fr_vol = torch.where(\n",
    "            fr_vol > fr_clamp_quantiles[1], fr_clamp_quantiles[1], fr_vol\n",
    "        )\n",
    "\n",
    "        # Estimate means and vars from the masked voxels.\n",
    "        masked_fr_vol = torch.masked_select(fr_vol, fr_mask).reshape(\n",
    "            fr_vol.shape[0], -1\n",
    "        )\n",
    "        fr_channel_means = masked_fr_vol.mean(dim=1)\n",
    "        fr_channel_means = fr_channel_means.reshape(-1, 1, 1, 1)\n",
    "        fr_channel_vars = masked_fr_vol.var(dim=1)\n",
    "        fr_channel_vars = fr_channel_vars.reshape(-1, 1, 1, 1)\n",
    "\n",
    "        # Process LR volumes.\n",
    "        lr_vol = subj[\"lr_dti\"][\"data\"]\n",
    "        subj[\"source_lr_dti\"] = dict()\n",
    "        subj[\"source_lr_dti\"][\"data\"] = lr_vol\n",
    "        subj[\"source_lr_dti\"][\"affine\"] = subj[\"lr_dti\"][\"affine\"]\n",
    "        # Perform clamping of quantile ranges\n",
    "        lr_clamp_quantiles = (\n",
    "            torch.stack([q for q in quantiles.lr.values()], dim=1)\n",
    "            .to(lr_vol)\n",
    "            .view(2, 6, 1, 1, 1)\n",
    "        )\n",
    "        lr_mask = subj[\"lr_brain_mask\"][\"data\"].bool()\n",
    "\n",
    "        # Report how many voxels will be clamped.\n",
    "        num_clamps = (\n",
    "            torch.masked_select(lr_vol, lr_mask).reshape(lr_vol.shape[0], -1)\n",
    "            < lr_clamp_quantiles[0, :, 0, 0]\n",
    "        ).sum(dim=1)\n",
    "\n",
    "        num_clamps = (\n",
    "            torch.masked_select(lr_vol, lr_mask).reshape(lr_vol.shape[0], -1)\n",
    "            > lr_clamp_quantiles[1, :, 0, 0]\n",
    "        ).sum(dim=1)\n",
    "\n",
    "        print(\n",
    "            f\"    Subj {subj_id} LR, number of clamped voxels per-channel: \",\n",
    "            num_clamps.tolist(),\n",
    "        )\n",
    "        print(\n",
    "            f\"    Subj {subj_id} LR, percentage of clamped voxels per-channel: \",\n",
    "            (\n",
    "                num_clamps / torch.masked_select(lr_vol[0], lr_mask).numel() * 100\n",
    "            ).tolist(),\n",
    "        )\n",
    "        lr_vol = torch.where(\n",
    "            lr_vol <= lr_clamp_quantiles[0], lr_clamp_quantiles[0], lr_vol\n",
    "        )\n",
    "        lr_vol = torch.where(\n",
    "            lr_vol >= lr_clamp_quantiles[1], lr_clamp_quantiles[1], lr_vol\n",
    "        )\n",
    "\n",
    "        # Estimate means and vars from the masked voxels.\n",
    "        masked_lr_vol = torch.masked_select(lr_vol, lr_mask).reshape(\n",
    "            lr_vol.shape[0], -1\n",
    "        )\n",
    "        lr_channel_means = masked_lr_vol.mean(dim=1)\n",
    "        lr_channel_means = lr_channel_means.reshape(-1, 1, 1, 1)\n",
    "        lr_channel_vars = masked_lr_vol.var(dim=1)\n",
    "        lr_channel_vars = lr_channel_vars.reshape(-1, 1, 1, 1)\n",
    "\n",
    "        # Normalize the volumes.\n",
    "        fr_vol = pitn.data.norm.normalize_dti(fr_vol, fr_channel_means, fr_channel_vars)\n",
    "        # Zero out voxels outside the mask.\n",
    "        fr_vol = fr_vol * fr_mask\n",
    "\n",
    "        lr_vol = pitn.data.norm.normalize_dti(lr_vol, lr_channel_means, lr_channel_vars)\n",
    "        # Zero out voxels outside the mask.\n",
    "        lr_vol = lr_vol * lr_mask\n",
    "\n",
    "        # Store new volumes back into the Subject object.\n",
    "        fr_dti_img = torchio.ScalarImage(\n",
    "            tensor=fr_vol,\n",
    "            affine=subj[\"gt_dti\"][\"affine\"],\n",
    "        )\n",
    "        # The normalized target is named \"fr_dti\", whereas the source/ground truth DTI\n",
    "        # (without normalization or clamping) is named \"gt_dti\".\n",
    "        subj.add_image(fr_dti_img, \"fr_dti\")\n",
    "        # Store subject-and-channel-wise means and vars.\n",
    "        subj[\"fr_means\"] = fr_channel_means.detach().cpu().numpy()\n",
    "        subj[\"fr_vars\"] = fr_channel_vars.detach().cpu().numpy()\n",
    "\n",
    "        subj[\"lr_dti\"][\"data\"] = lr_vol\n",
    "        # Store subject-and-channel-wise means and vars.\n",
    "        subj[\"lr_means\"] = lr_channel_means.detach().cpu().numpy()\n",
    "        subj[\"lr_vars\"] = lr_channel_vars.detach().cpu().numpy()\n",
    "\n",
    "        # ============= Visualization and tracking procedures =============\n",
    "        # Re-calculate the same statistics post-normalization.\n",
    "        # Only consider the voxels within the mask.\n",
    "        masked_fr_vol = torch.masked_select(fr_vol, fr_mask).reshape(6, -1)\n",
    "\n",
    "        masked_fr_channel_means = masked_fr_vol.mean(dim=1)\n",
    "        masked_fr_channel_means = masked_fr_channel_means.reshape(-1, 1, 1, 1)\n",
    "        masked_fr_channel_vars = masked_fr_vol.var(dim=1)\n",
    "        masked_fr_channel_vars = masked_fr_channel_vars.reshape(-1, 1, 1, 1)\n",
    "        # Print and log some statistics of the subject data.\n",
    "        # Add FR stats to the summary table.\n",
    "        masked_fr_vol_stats = batch_boxplot_stats(masked_fr_vol.detach().cpu().numpy())\n",
    "        norm_subj_stats[\"Subj ID\"].extend(\n",
    "            list(itertools.repeat(subj_id, len(masked_fr_vol_stats.median)))\n",
    "        )\n",
    "        norm_subj_stats[\"Resolution\"].extend(\n",
    "            list(\n",
    "                itertools.repeat(\n",
    "                    tuple(fr_vol.shape[1:]), len(masked_fr_vol_stats.median)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        norm_subj_stats[\"Channel Index\"].extend(dti_channel_names)\n",
    "        norm_subj_stats[\"Mean\"].extend(masked_fr_channel_means.cpu().flatten().tolist())\n",
    "        norm_subj_stats[\"Var\"].extend(masked_fr_channel_vars.cpu().flatten().tolist())\n",
    "        # Append FR boxplot stats to their corresponding fields. In other words, all\n",
    "        # columns after the var column.\n",
    "        for i, field in enumerate(\n",
    "            summary_stats_header[(summary_stats_header.index(\"Var\") + 1) :]\n",
    "        ):\n",
    "            if \"Num\" in field:\n",
    "                norm_subj_stats[field].extend(list(map(len, masked_fr_vol_stats[i])))\n",
    "            else:\n",
    "                norm_subj_stats[field].extend(masked_fr_vol_stats[i])\n",
    "\n",
    "        # Add LR stats to the summary table.\n",
    "        masked_lr_vol = torch.masked_select(lr_vol, lr_mask).reshape(6, -1)\n",
    "        masked_lr_channel_means = masked_lr_vol.mean(dim=1)\n",
    "        masked_lr_channel_means = masked_lr_channel_means.reshape(-1, 1, 1, 1)\n",
    "        masked_lr_channel_vars = masked_lr_vol.var(dim=1)\n",
    "        masked_lr_channel_vars = masked_lr_channel_vars.reshape(-1, 1, 1, 1)\n",
    "\n",
    "        masked_lr_vol_stats = batch_boxplot_stats(masked_lr_vol.detach().cpu().numpy())\n",
    "        norm_subj_stats[\"Subj ID\"].extend(\n",
    "            list(itertools.repeat(subj_id, len(masked_lr_vol_stats.median)))\n",
    "        )\n",
    "        norm_subj_stats[\"Resolution\"].extend(\n",
    "            list(\n",
    "                itertools.repeat(\n",
    "                    tuple(lr_vol.shape[1:]), len(masked_lr_vol_stats.median)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        norm_subj_stats[\"Channel Index\"].extend(dti_channel_names)\n",
    "        norm_subj_stats[\"Mean\"].extend(masked_lr_channel_means.cpu().flatten().tolist())\n",
    "        norm_subj_stats[\"Var\"].extend(masked_lr_channel_vars.cpu().flatten().tolist())\n",
    "        # Append LR boxplot stats to their corresponding fields. In other words, all columns\n",
    "        # after the var column.\n",
    "        for i, field in enumerate(\n",
    "            summary_stats_header[(summary_stats_header.index(\"Var\") + 1) :]\n",
    "        ):\n",
    "            if \"Num\" in field:\n",
    "                norm_subj_stats[field].extend(list(map(len, masked_lr_vol_stats[i])))\n",
    "            else:\n",
    "                norm_subj_stats[field].extend(masked_lr_vol_stats[i])\n",
    "\n",
    "        subj_idx = list(subj_dirs.keys()).index(subj_id)\n",
    "\n",
    "        # Generate row of boxplots for the normalized volumes.\n",
    "        fig_norm, share_axs_rows, share_axs_cols = plot_dti_box_row(\n",
    "            fig_norm,\n",
    "            grid_norm,\n",
    "            row_idx=subj_idx,\n",
    "            subj_id=subj_id,\n",
    "            shared_axs_rows=share_axs_norm[\"row\"],\n",
    "            shared_axs_cols=share_axs_norm[\"col\"],\n",
    "            fr_vol=masked_fr_vol.detach().cpu().numpy(),\n",
    "            lr_vol=masked_lr_vol.detach().cpu().numpy(),\n",
    "        )\n",
    "        share_axs_norm[\"row\"] = share_axs_rows\n",
    "        share_axs_norm[\"col\"] = share_axs_cols\n",
    "\n",
    "        print(\"=\" * 20)\n",
    "\n",
    "    print(\"===Data Normalized===\")\n",
    "\n",
    "    if norm_subj_stats[\"Subj ID\"]:\n",
    "        norm_subj_stats_str = tabulate(\n",
    "            norm_subj_stats, headers=summary_stats_header, tablefmt=summary_stats_fmt\n",
    "        )\n",
    "    else:\n",
    "        norm_subj_stats_str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If the subject data was normalized and those stats were recorded, log those stats.\n",
    "if normalize:\n",
    "    with open(log_txt_file, \"a+\") as f:\n",
    "        f.write(\n",
    "            f\"Data Summary Statistics, after normalization:\\n {norm_subj_stats_str}\\n\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if normalize:\n",
    "#     display(fig_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if normalize:\n",
    "#     display(Markdown(norm_subj_stats_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finalize the dataset.\n",
    "subj_dataset = torchio.SubjectsDataset(list(subj_data.values()), load_getitem=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set Up Patch-Based Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data train/validation/test split\n",
    "\n",
    "num_subjs = len(subj_dataset)\n",
    "num_test_subjs = int(np.ceil(num_subjs * test_percent))\n",
    "num_val_subjs = int(np.ceil(num_subjs * val_percent))\n",
    "num_train_subjs = num_subjs - (num_test_subjs + num_val_subjs)\n",
    "subj_list = subj_dataset.dry_iter()\n",
    "# Randomly shuffle the list of subjects, then choose the first `num_test_subjs` subjects\n",
    "# for testing.\n",
    "random.shuffle(subj_list)\n",
    "\n",
    "# Choose the remaining for training/validation.\n",
    "# If only 1 subject is available, assume this is a debugging run.\n",
    "if num_subjs == 1:\n",
    "    warnings.warn(\n",
    "        \"DEBUG: Only 1 subject selected, mixing training, validation, and testing sets\"\n",
    "    )\n",
    "    num_train_subjs = num_val_subjs = num_test_subjs = 1\n",
    "\n",
    "    test_subjs = subj_list\n",
    "    val_subjs = subj_list\n",
    "    train_subjs = subj_list\n",
    "else:\n",
    "    test_subjs = subj_list[:num_test_subjs]\n",
    "    val_subjs = subj_list[num_test_subjs : (num_test_subjs + num_val_subjs)]\n",
    "    train_subjs = subj_list[(num_test_subjs + num_val_subjs) :]\n",
    "\n",
    "test_dataset = monai.data.Dataset([Addict(subj) for subj in test_subjs])\n",
    "val_dataset = monai.data.Dataset([Addict(subj) for subj in val_subjs])\n",
    "train_dataset = torchio.SubjectsDataset(train_subjs, load_getitem=False)\n",
    "\n",
    "# Training patch sampler, random across all patches of all volumes.\n",
    "# The training targets consist of normalized, transformed DTIs that are *not* exactly\n",
    "# equivalent to the ground truth DTI's.\n",
    "# Training targets use the \"fr_dti\"s, while testing and validation targets are \"gt_dti\"s.\n",
    "train_sampler = pitn.samplers.MultiresSampler(\n",
    "    source_img_key=\"fr_dti\",\n",
    "    low_res_key=\"lr_dti\",\n",
    "    downsample_factor=exp_params.downsample_factor,\n",
    "    low_res_sample_extension=exp_params.patch.low_res_sample_extension,\n",
    "    label_name=\"fr_brain_mask\",\n",
    "    source_spatial_patch_size=output_spatial_patch_shape,\n",
    "    low_res_spatial_patch_size=input_spatial_patch_shape,\n",
    "    label_probabilities={0: 0, 1: 1},\n",
    "    source_mask_key=\"fr_brain_mask\",\n",
    ")\n",
    "\n",
    "patches_per_subj = 8000\n",
    "# Set length of the queue.\n",
    "queue_max_length = patches_per_subj * num_train_subjs\n",
    "\n",
    "# Set up a torchio.Queue to act as a sampler proxy for the torch DataLoader\n",
    "train_queue = torchio.Queue(\n",
    "    train_dataset,\n",
    "    max_length=queue_max_length,\n",
    "    samples_per_volume=patches_per_subj,\n",
    "    sampler=train_sampler,\n",
    "    shuffle_patches=True,\n",
    "    shuffle_subjects=True,\n",
    "    num_workers=7,\n",
    "    #     verbose=True,\n",
    ")\n",
    "\n",
    "# Create partial function to collect list of samples and form a tuple of tensors.\n",
    "collate_fn = functools.partial(\n",
    "    pitn.samplers.collate_subj_mask,\n",
    "    full_res_key=\"fr_dti\",\n",
    "    low_res_key=\"lr_dti\",\n",
    "    full_res_mask_key=\"fr_brain_mask\",\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_queue,\n",
    "    batch_size=exp_params.patch.batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Statically set training patches\n",
    "# static_train_patches = [train_queue[i] for i in range(train_queue.iterations_per_epoch)]\n",
    "# static_dataset = torchio.SubjectsDataset(static_train_patches)\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     static_dataset,\n",
    "#     batch_size=exp_params.patch.batch_size,\n",
    "#     shuffle=True,\n",
    "#     collate_fn=collate_fn,\n",
    "#     pin_memory=True,\n",
    "#     num_workers=0,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "keep_output"
    ]
   },
   "outputs": [],
   "source": [
    "# Set up validation and testing objects.\n",
    "\n",
    "# Validation samplers/datasets\n",
    "# Alter the collate function to include locations, masks, and subj ids in batches for\n",
    "# visualization, reconstruction, analysis, etc.\n",
    "# This collate function determines the contents of the `batch` parameters in the\n",
    "# PytorchLightning system's `test_step` and `validation_step` methods.\n",
    "collate_meta = functools.partial(\n",
    "    pitn.viz.collate_locs_and_keys,\n",
    "    full_res_key=\"gt_dti\",\n",
    "    low_res_key=\"lr_dti\",\n",
    "    full_res_mask_key=\"source_mask\",\n",
    "    subj_id=\"subj_id\",\n",
    "    fr_mean=\"fr_means\",\n",
    "    fr_var=\"fr_vars\",\n",
    ")\n",
    "\n",
    "collate_keys = lambda samples: pitn.samplers.collate_dicts(\n",
    "    samples,\n",
    "    \"subj_id\",\n",
    "    \"fr_brain_mask\",\n",
    "    low_res=\"lr_dti\",\n",
    "    full_res=\"gt_dti\",\n",
    "    fr_mean=\"fr_means\",\n",
    "    fr_var=\"fr_vars\",\n",
    ")\n",
    "\n",
    "\n",
    "# Construct val and test DataLoaders to operate on one full volume per batch.\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1,\n",
    "    collate_fn=collate_keys,\n",
    "    pin_memory=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "# Test samplers/datasets\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    collate_fn=collate_keys,\n",
    "    pin_memory=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "print(\"Training subject ID(s): \", [s.subj_id for s in train_dataset.dry_iter()])\n",
    "print(\"Validation subject ID(s): \", [s.subj_id for s in val_dataset])\n",
    "print(\"Test subject ID(s): \", [s.subj_id for s in test_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Bounding Box Selection for Visualization During Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose only one subject for visualization during validation.\n",
    "val_viz_subj_id = random.choice([s.subj_id for s in val_dataset])\n",
    "# Store its index in the Dataset object for easier indexing.\n",
    "val_viz_dataset_idx = [s.subj_id for s in val_dataset].index(val_viz_subj_id)\n",
    "print(val_viz_subj_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create bbox coordinates for visualizing validation aggregate patches.\n",
    "# NOTE: This presumes that there are no overlap patches in the validation step, and that\n",
    "# each validation volume is divisible by the patch shape in each spatial dimension.\n",
    "bbox_coords = list()\n",
    "region_size = torch.as_tensor(output_spatial_patch_shape) * 3\n",
    "vol_shape = torch.as_tensor(val_dataset[val_viz_dataset_idx].fr_dti.data.shape[1:])\n",
    "possible_bbox_ini = [\n",
    "    torch.arange(0, vol_shape[0], output_spatial_patch_shape[0]),\n",
    "    torch.arange(0, vol_shape[1], output_spatial_patch_shape[1]),\n",
    "    torch.arange(0, vol_shape[2], output_spatial_patch_shape[2]),\n",
    "]\n",
    "\n",
    "# Create bbox that spans roughly the center of the volume.\n",
    "bbox_idx_ini = list()\n",
    "for possible_bbox_part in possible_bbox_ini:\n",
    "    num_parts = len(possible_bbox_part)\n",
    "    bbox_idx_ini.append(possible_bbox_part[round(num_parts * 0.4)])\n",
    "bbox_idx_ini = torch.as_tensor(bbox_idx_ini)\n",
    "\n",
    "bbox_coord = torch.cat([bbox_idx_ini, bbox_idx_ini + region_size])\n",
    "bbox_coords.append(bbox_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create bbox that covers an edge.\n",
    "vol_mask = val_dataset[val_viz_dataset_idx].fr_brain_mask.data[0]\n",
    "bbox_mask_coverage = list()\n",
    "possible_bbox_coords_ini = list(itertools.product(*possible_bbox_ini))\n",
    "for bbox_w, bbox_h, bbox_d in possible_bbox_coords_ini:\n",
    "    bbox_start = (bbox_w, bbox_h, bbox_d)\n",
    "    bbox_end = (\n",
    "        bbox_start[0] + output_spatial_patch_shape[0],\n",
    "        bbox_start[1] + output_spatial_patch_shape[1],\n",
    "        bbox_start[2] + output_spatial_patch_shape[2],\n",
    "    )\n",
    "    patch = vol_mask[\n",
    "        bbox_start[0] : bbox_end[0],\n",
    "        bbox_start[1] : bbox_end[1],\n",
    "        bbox_start[2] : bbox_end[2],\n",
    "    ]\n",
    "    # Validation visuals are shown over the Superior-Anterior axis, so the edge should be\n",
    "    # located there.\n",
    "    bbox_mask_coverage.append(\n",
    "        patch[:, output_spatial_patch_shape[1] // 2, :].sum().item()\n",
    "    )\n",
    "\n",
    "# Now search all coordinates' mask coverages for the one closest to a 50% of the total\n",
    "# patch volume.\n",
    "patch_vol = output_spatial_patch_shape[0] * output_spatial_patch_shape[2]\n",
    "target_mask_vol = patch_vol // (2**2)\n",
    "\n",
    "bbox_coord_idx = np.argmin(np.abs(np.asarray(bbox_mask_coverage) - patch_vol))\n",
    "bbox_idx_ini = torch.tensor(possible_bbox_coords_ini[bbox_coord_idx])\n",
    "bbox_coord = torch.cat([bbox_idx_ini, bbox_idx_ini + region_size])\n",
    "bbox_coords.append(bbox_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bbox_coords = torch.stack(bbox_coords)\n",
    "\n",
    "bbox_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(log_txt_file, \"a+\") as f:\n",
    "    f.write(f\"Training Set Subjects: {[s.subj_id for s in train_dataset.dry_iter()]}\\n\")\n",
    "    f.write(f\"Validation Set Subjects: {[s.subj_id for s in val_dataset]}\\n\")\n",
    "    f.write(f\"Test Set Subjects: {[s.subj_id for s in test_dataset]}\\n\")\n",
    "\n",
    "logger.add_text(\"train_subjs\", str([s.subj_id for s in train_dataset.dry_iter()]))\n",
    "logger.add_text(\"val_subjs\", str([s.subj_id for s in val_dataset]))\n",
    "logger.add_text(\"test_subjs\", str([s.subj_id for s in test_dataset]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Full pytorch-lightning module for contained training, validation, and testing.\n",
    "debug_prob = -1 / (patches_per_subj * num_train_subjs / exp_params.patch.batch_size)\n",
    "\n",
    "\n",
    "class DIQTSystem(pl.LightningModule):\n",
    "\n",
    "    # Specify training loss methods with mappings to their names as strings.\n",
    "    loss_methods = {\n",
    "        \"MSE\".casefold(): torch.nn.MSELoss(reduction=\"mean\"),\n",
    "        \"SSE\".casefold(): torch.nn.MSELoss(reduction=\"sum\"),\n",
    "        \"RMSE\".casefold(): lambda y_hat, y: torch.sqrt(\n",
    "            F.mse_loss(y_hat, y, reduction=\"mean\")\n",
    "        ),\n",
    "        \"L1\".casefold(): torch.nn.L1Loss(reduction=\"mean\"),\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        downsample_factor,\n",
    "        source_vox_size: float,\n",
    "        target_vox_size: float,\n",
    "        train_loss_method: str,\n",
    "        opt_params: dict,\n",
    "        norm_method=None,\n",
    "        val_viz_bboxes=None,\n",
    "        val_viz_subj_id=None,\n",
    "        #         val_patch_overlap=(0, 0, 0),\n",
    "        val_viz_every_n_epochs=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self._channels = channels\n",
    "        self._downsample_factor = downsample_factor\n",
    "\n",
    "        # Parameters\n",
    "        # Network parameters\n",
    "        self.net = pitn.nn.models.FractDownReduceBy5Conv(\n",
    "            self._channels, self._downsample_factor\n",
    "        )\n",
    "        #         self.net = pitn.nn.models.DebugFC(\n",
    "        #             input_patch_shape, output_patch_shape\n",
    "        #         )\n",
    "        self._norm_eps = 1e-10\n",
    "        ## Training parameters\n",
    "        self.opt_params = opt_params\n",
    "\n",
    "        # Select loss method as either one of the pre-selected methods, or a custom\n",
    "        # callable.\n",
    "        try:\n",
    "            self._loss_fn = self.loss_methods[train_loss_method.casefold()]\n",
    "        except (AttributeError, KeyError):\n",
    "            if callable(train_loss_method):\n",
    "                self._loss_fn = train_loss_method\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"ERROR: Invalid loss function specification {train_loss_method}, \"\n",
    "                    + f\"expected one of {self.loss_methods.keys()} or a callable.\"\n",
    "                )\n",
    "\n",
    "        # Sub-regions of the volume that should be logged in validation.\n",
    "        if val_viz_bboxes is None:\n",
    "            self.val_bboxes = torch.zeros(0, 6)\n",
    "            self._val_subj_id = None\n",
    "        else:\n",
    "            self.val_bboxes = val_viz_bboxes\n",
    "            self._val_subj_id = val_viz_subj_id\n",
    "        #         self.val_patch_overlap = val_patch_overlap\n",
    "\n",
    "        # Store the validation set min and max for each bounding box, to keep a consistent\n",
    "        # color scale on the color bar.\n",
    "        self.val_vmin = [\n",
    "            None,\n",
    "        ] * self.val_bboxes.shape[0]\n",
    "        self.val_vmax = [\n",
    "            None,\n",
    "        ] * self.val_bboxes.shape[0]\n",
    "        self.val_viz_every_n_epochs = val_viz_every_n_epochs\n",
    "        self._last_val_viz_epoch = -1\n",
    "\n",
    "        # My own dinky logging object.\n",
    "        self.plain_log = Addict(\n",
    "            {\n",
    "                \"train_loss\": list(),\n",
    "                \"val_loss\": list(),\n",
    "                \"test_loss\": dict(),\n",
    "                \"spline_loss\": list(),\n",
    "                \"viz\": Addict(\n",
    "                    {\n",
    "                        \"test_preds\": Addict(),\n",
    "                        \"test_squared_error\": Addict(),\n",
    "                    }\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.net(x)\n",
    "        return y\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, masks = batch\n",
    "        masks = masks.bool()\n",
    "\n",
    "        y_pred = self.net(x, norm_output=False)\n",
    "\n",
    "        loss = self._loss_fn(\n",
    "            torch.masked_select(y_pred, masks), torch.masked_select(y, masks)\n",
    "        )\n",
    "\n",
    "        # Random debug scheduling.\n",
    "        if random.random() <= debug_prob:\n",
    "            #         try:\n",
    "            #             n_m_3_loss = np.mean(self.plain_log.train_loss[-3:])\n",
    "            #         except IndexError:\n",
    "            #             n_m_3_loss = loss.detach().item()\n",
    "            #         if (self.current_epoch >= 3) and np.abs(loss.detach().item() - n_m_3_loss) >= (\n",
    "            #             100 * n_m_3_loss\n",
    "            #         ):\n",
    "            batch_size = x.shape[0]\n",
    "            images = list()\n",
    "            tab_labels = list()\n",
    "            image_labels = list()\n",
    "            channel_labels = [\n",
    "                \"D x,x\",\n",
    "                \"D x,y\",\n",
    "                \"D y,y\",\n",
    "                \"D x,z\",\n",
    "                \"D y,z\",\n",
    "                \"D z,z\",\n",
    "            ]\n",
    "            # Construct the images to display, their names, and their batch index\n",
    "            # as the \"class\" tab label.\n",
    "            # ipyplot only takes flat lists as parameters, so no fancy multi-dimensional\n",
    "            # lists or anything like that.\n",
    "            # Each batch is its own \"class\" (UI tab).\n",
    "            for batch_i in range(batch_size):\n",
    "                # Create a new set of images for every channel.\n",
    "                for channel_i, channel_name in enumerate(channel_labels):\n",
    "\n",
    "                    # Collect the input image.\n",
    "                    input_img = x[batch_i, channel_i].detach().cpu().numpy()\n",
    "                    # ipyplot will use PIL.Image when given a numpy array to display, so\n",
    "                    # float ndarrays must be scaled between 0 and 1.\n",
    "                    input_img = skimage.exposure.rescale_intensity(\n",
    "                        input_img, out_range=(0.0, 1.0)\n",
    "                    )\n",
    "                    # Remove the oversampled pixels.\n",
    "                    #                     input_img = input_img[2:-2, 2:-2, 2:-2]\n",
    "                    input_slice_idx = input_img.shape[1] // 2\n",
    "                    input_img = input_img[:, input_slice_idx, :]\n",
    "                    tab_labels.append(str(batch_i))\n",
    "                    images.append(np.rot90(input_img))\n",
    "                    image_labels.append(f\"Input {channel_name}\")\n",
    "\n",
    "                    # Collect the target image.\n",
    "                    target_img = y[batch_i, channel_i].detach().cpu().numpy()\n",
    "                    target_img = skimage.exposure.rescale_intensity(\n",
    "                        target_img, out_range=(0.0, 1.0)\n",
    "                    )\n",
    "                    tab_labels.append(str(batch_i))\n",
    "                    hr_slice_idx = target_img.shape[1] // 2\n",
    "                    target_img = target_img[:, hr_slice_idx, :]\n",
    "                    images.append(np.rot90(target_img))\n",
    "                    image_labels.append(f\"Target {channel_name}\")\n",
    "\n",
    "                    # Collect the predicted image.\n",
    "                    pred_img = y_pred[batch_i, channel_i].detach().cpu().numpy()\n",
    "                    pred_img = skimage.exposure.rescale_intensity(\n",
    "                        pred_img, out_range=(0.0, 1.0)\n",
    "                    )\n",
    "                    tab_labels.append(str(batch_i))\n",
    "                    pred_img = pred_img[:, hr_slice_idx, :]\n",
    "                    images.append(np.rot90(pred_img))\n",
    "                    image_labels.append(f\"Prediction {channel_name}\")\n",
    "\n",
    "                # Collect the mask image (only 1 per batch).\n",
    "                mask_img = masks[batch_i][0].detach().cpu().numpy()\n",
    "                mask_img = mask_img.astype(bool)\n",
    "                mask_img = mask_img[:, hr_slice_idx, :]\n",
    "                tab_labels.append(str(batch_i))\n",
    "                images.append(np.rot90(mask_img))\n",
    "                image_labels.append(\"Mask\")\n",
    "\n",
    "            ipyplot.plot_class_tabs(\n",
    "                images, tab_labels, custom_texts=image_labels, show_url=False\n",
    "            )\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.plain_log[\"train_loss\"].append(float(loss.cpu()))\n",
    "        return loss\n",
    "\n",
    "    def log_bbox_figure(\n",
    "        self,\n",
    "        fr,\n",
    "        pred,\n",
    "        abs_error,\n",
    "        vmin: float,\n",
    "        vmax: float,\n",
    "        fig_name: str,\n",
    "        slice_idx=(0, slice(None), slice(None), 86, slice(None)),\n",
    "    ):\n",
    "        # Slice into full volume and just grab a B x C x H x W slice.\n",
    "        row_fr = pitn.viz.make_grid(\n",
    "            [torch.rot90(t) for t in fr[slice_idx]], padding=2, pad_value=-2, nrow=1\n",
    "        )\n",
    "        row_pred = pitn.viz.make_grid(\n",
    "            [torch.rot90(t) for t in pred[slice_idx]], padding=2, pad_value=-2, nrow=1\n",
    "        )\n",
    "        row_abs_error = pitn.viz.make_grid(\n",
    "            [torch.rot90(t) for t in abs_error[slice_idx]],\n",
    "            padding=2,\n",
    "            pad_value=-2,\n",
    "            nrow=1,\n",
    "        )\n",
    "        reg_grid = pitn.viz.make_grid(\n",
    "            [row_fr, row_pred, row_abs_error], nrow=3, pad_value=-2\n",
    "        )\n",
    "\n",
    "        fig = plt.figure(figsize=(8, 4.5), dpi=110, clear=True)\n",
    "\n",
    "        plt.imshow(\n",
    "            #             np.rot90(reg_grid.cpu().numpy(), axes=(-2, -1)),\n",
    "            reg_grid.cpu().numpy(),\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "            cmap=\"jet\",\n",
    "        )\n",
    "        plt.xticks([], [])\n",
    "        plt.yticks([], [])\n",
    "        plt.colorbar(location=\"bottom\")\n",
    "        self.logger.experiment.add_figure(fig_name, fig, self.global_step)\n",
    "\n",
    "    def validation_step(self, batch: dict, batch_idx):\n",
    "        # The validation samples include extra metadata regarding the patches.\n",
    "        # NOTE: This assumes a batch size of 1!\n",
    "        batch = Addict(batch)\n",
    "        x = batch.low_res.data\n",
    "        y = batch.full_res.data\n",
    "        y_mask = batch.fr_brain_mask.data\n",
    "        y_mask = y_mask.bool()\n",
    "        fr_mean = batch.fr_mean\n",
    "        fr_var = batch.fr_var\n",
    "        fr_mean = torch.as_tensor(fr_mean).to(x)\n",
    "        fr_var = torch.as_tensor(fr_var).to(fr_mean)\n",
    "        subj_id = batch.subj_id[0]\n",
    "\n",
    "        y_pred = self.net(\n",
    "            x,\n",
    "            norm_output=False,\n",
    "            pad_reduced_shape=True,\n",
    "            interp_to_spatial_shape=y.shape[-3:],\n",
    "        )\n",
    "\n",
    "        # Denormalize the predictions and calculate RMSE on intensity values\n",
    "        # in the original data's scale.\n",
    "        if (not (fr_mean == 0).all()) and (not (fr_var == 1).all()):\n",
    "            y_pred = pitn.data.norm.denormalize_batch(\n",
    "                y_pred, mean=fr_mean, var=fr_var, eps=self._norm_eps\n",
    "            )\n",
    "        loss = torch.sqrt(\n",
    "            F.mse_loss(\n",
    "                torch.masked_select(y_pred, y_mask),\n",
    "                torch.masked_select(y, y_mask),\n",
    "                reduction=\"mean\",\n",
    "            )\n",
    "        )\n",
    "        #         loss = torch.sqrt(F.mse_loss(y_pred * y_mask, y * y_mask, reduction=\"mean\"))\n",
    "\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.plain_log[\"val_loss\"].append(float(loss.cpu()))\n",
    "\n",
    "        # Create validation set visualizations, if necessary.\n",
    "        # Only create viz for one subject in the validation set.\n",
    "        if subj_id == self._val_subj_id:\n",
    "            # Only create a validation visualization if the current global epoch has reached a\n",
    "            # threshold.\n",
    "            if (\n",
    "                self.current_epoch % self.val_viz_every_n_epochs == 0\n",
    "                or (self.current_epoch - self._last_val_viz_epoch)\n",
    "                > self.val_viz_every_n_epochs\n",
    "            ):\n",
    "                self._last_val_viz_epoch = self.current_epoch\n",
    "\n",
    "                # Create a plot for each of the given bounding boxes/regions of interest.\n",
    "                for i_bbox, bbox_range in enumerate(self.val_bboxes):\n",
    "\n",
    "                    # Grab the range of the bbox region and create the index object.\n",
    "                    bbox_range = bbox_range.detach().cpu().int()\n",
    "                    b_start = bbox_range[:3].tolist()\n",
    "                    b_end = bbox_range[3:].tolist()\n",
    "                    #                     breakpoint()\n",
    "                    # Select the target FR region, the predicted region, and the absolute\n",
    "                    # error between the two.\n",
    "                    fr_patch = (\n",
    "                        y[\n",
    "                            ...,\n",
    "                            b_start[0] : b_end[0],\n",
    "                            b_start[1] : b_end[1],\n",
    "                            b_start[2] : b_end[2],\n",
    "                        ]\n",
    "                        .detach()\n",
    "                        .cpu()\n",
    "                    )\n",
    "\n",
    "                    pred_patch = (\n",
    "                        y_pred[\n",
    "                            ...,\n",
    "                            b_start[0] : b_end[0],\n",
    "                            b_start[1] : b_end[1],\n",
    "                            b_start[2] : b_end[2],\n",
    "                        ]\n",
    "                        .detach()\n",
    "                        .cpu()\n",
    "                    )\n",
    "\n",
    "                    abs_error = torch.sqrt(\n",
    "                        F.mse_loss(y_pred * y_mask, y * y_mask, reduction=\"none\")\n",
    "                    ).detach()\n",
    "                    abs_error_patch = abs_error[\n",
    "                        ...,\n",
    "                        b_start[0] : b_end[0],\n",
    "                        b_start[1] : b_end[1],\n",
    "                        b_start[2] : b_end[2],\n",
    "                    ].cpu()\n",
    "\n",
    "                    # Choose which 2D plane to plot.\n",
    "                    slice_idx = (\n",
    "                        0,\n",
    "                        slice(None),\n",
    "                        fr_patch.shape[-3] // 2,\n",
    "                        slice(None),\n",
    "                        slice(None),\n",
    "                    )\n",
    "\n",
    "                    # Select vmin and vmax values to maintain throughout training.\n",
    "                    if self.val_vmin[i_bbox] is None:\n",
    "                        vmin = np.quantile(fr_patch[slice_idx].numpy().flatten(), 0.05)\n",
    "                        self.val_vmin[i_bbox] = vmin\n",
    "                    if self.val_vmax[i_bbox] is None:\n",
    "                        vmax = np.quantile(fr_patch[slice_idx].numpy().flatten(), 0.95)\n",
    "                        self.val_vmax[i_bbox] = vmax\n",
    "\n",
    "                    self.log_bbox_figure(\n",
    "                        fr_patch,\n",
    "                        pred_patch,\n",
    "                        abs_error_patch,\n",
    "                        vmin=self.val_vmin[i_bbox],\n",
    "                        vmax=self.val_vmax[i_bbox],\n",
    "                        fig_name=f\"val_region_{i_bbox}\",\n",
    "                        slice_idx=slice_idx,\n",
    "                    )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch: dict, batch_idx):\n",
    "\n",
    "        # The testing samples include extra metadata regarding the patches.\n",
    "        # NOTE: This assumes a batch size of 1!\n",
    "        batch = Addict(batch)\n",
    "        x = batch.low_res.data\n",
    "        y = batch.full_res.data\n",
    "        y_mask = batch.fr_brain_mask.data\n",
    "        y_mask = y_mask.bool()\n",
    "        fr_mean = batch.fr_mean\n",
    "        fr_mean = torch.as_tensor(fr_mean).to(x)\n",
    "        fr_var = batch.fr_var\n",
    "        fr_var = torch.as_tensor(fr_var).to(fr_mean)\n",
    "        subj_id = batch.subj_id[0]\n",
    "\n",
    "        y_pred = self.net(\n",
    "            x,\n",
    "            norm_output=False,\n",
    "            pad_reduced_shape=True,\n",
    "            interp_to_spatial_shape=y.shape[-3:],\n",
    "        )\n",
    "\n",
    "        # Denormalize the predictions and calculate RMSE on intensity values\n",
    "        # in the original data's scale.\n",
    "        if (not (fr_mean == 0).all()) and (not (fr_var == 1).all()):\n",
    "            y_pred = pitn.data.norm.denormalize_batch(\n",
    "                y_pred, mean=fr_mean, var=fr_var, eps=self._norm_eps\n",
    "            )\n",
    "        loss = torch.sqrt(\n",
    "            F.mse_loss(\n",
    "                torch.masked_select(y_pred, y_mask),\n",
    "                torch.masked_select(y, y_mask),\n",
    "                reduction=\"mean\",\n",
    "            )\n",
    "        )\n",
    "        #         loss = torch.sqrt(F.mse_loss(y_pred * y_mask, y * y_mask, reduction=\"mean\"))\n",
    "\n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.plain_log[\"test_loss\"][subj_id] = loss.detach().cpu().item()\n",
    "\n",
    "        self.plain_log.viz.test_preds[subj_id] = y_pred[0].detach().cpu()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.net.parameters(), **self.opt_params)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_start_timestamp = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "model = DIQTSystem(\n",
    "    channels=channels,\n",
    "    downsample_factor=exp_params.downsample_factor,\n",
    "    source_vox_size=exp_params.source_vox_size,\n",
    "    target_vox_size=exp_params.target_vox_size,\n",
    "    norm_method=network_norm_method,\n",
    "    train_loss_method=train_loss_name,\n",
    "    opt_params=opt_params,\n",
    "    val_viz_subj_id=val_viz_subj_id,\n",
    "    val_viz_bboxes=bbox_coords,\n",
    "    val_viz_every_n_epochs=5,\n",
    ")\n",
    "\n",
    "with open(log_txt_file, \"a+\") as f:\n",
    "    f.write(f\"Model overview: {model}\\n\")\n",
    "\n",
    "# Create trainer object.\n",
    "trainer = pl.Trainer(\n",
    "    #     fast_dev_run=10,\n",
    "    gpus=1,\n",
    "    max_epochs=max_epochs,\n",
    "    logger=pl_logger,\n",
    "    log_every_n_steps=50,\n",
    "    check_val_every_n_epoch=1,\n",
    "    progress_bar_refresh_rate=10,\n",
    "    terminate_on_nan=True,\n",
    ")\n",
    "\n",
    "# Many warnings are produced here, so it's better for my sanity (i.e., worse in every other\n",
    "# way) to just filter and ignore them...\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "train_duration = datetime.datetime.now().replace(microsecond=0) - train_start_timestamp\n",
    "print(f\"Train duration: {train_duration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# debug\n",
    "# %autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save out trained model\n",
    "trainer.save_checkpoint(str(experiment_results_dir / \"model.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(log_txt_file, \"a+\") as f:\n",
    "    f.write(\"\\n\")\n",
    "    f.write(f\"Training time: {train_duration}\\n\")\n",
    "    f.write(\n",
    "        f\"\\t{train_duration.days} Days, \"\n",
    "        + f\"{train_duration.seconds // 3600} Hours,\"\n",
    "        + f\"{(train_duration.seconds // 60) % 60} Minutes,\"\n",
    "        + f'{train_duration.seconds % 60} Seconds\"\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot rolling average window of training loss values.\n",
    "plt.figure(dpi=110)\n",
    "window = 1000\n",
    "rolling_mean = (\n",
    "    np.convolve(model.plain_log[\"train_loss\"], np.ones(window), \"valid\") / window\n",
    ")\n",
    "rolling_start = 100\n",
    "plt.plot(\n",
    "    np.arange(\n",
    "        window + rolling_start,\n",
    "        window + rolling_start + len(rolling_mean[rolling_start:]),\n",
    "    ),\n",
    "    rolling_mean[rolling_start:],\n",
    ")\n",
    "plt.title(\"Training Loss \" + train_loss_name + f\"\\nRolling Mean {window}\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "# plt.ylim(0, 1)\n",
    "print(np.median(rolling_mean))\n",
    "print(\n",
    "    np.mean(model.plain_log[\"train_loss\"][: window + rolling_start]),\n",
    "    np.var(model.plain_log[\"train_loss\"][: window + rolling_start]),\n",
    "    np.max(model.plain_log[\"train_loss\"][: window + rolling_start]),\n",
    ")\n",
    "\n",
    "plt.savefig(experiment_results_dir / \"train_loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing/Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store test reconstructions along the way for later visualization.\n",
    "test_vol_viz = Addict()\n",
    "\n",
    "# Structure is as follows:\n",
    "# {subj_id_1:\n",
    "#    fr_mask: np.ndarray,\n",
    "#    dti: {\n",
    "#          diqt: np.ndarray,\n",
    "#          spline: np.ndarray,\n",
    "#          fr: np.ndarray,\n",
    "#          lr: np.ndarray\n",
    "#          ...\n",
    "#         }\n",
    "#    fa: {\n",
    "#         diqt: np.ndarray,\n",
    "#         ...\n",
    "#        },\n",
    "# pitn.data.norm.denormalize_batch(y, mean=fr_means, var=fr_vars, eps=self._norm_eps)\n",
    "#  subj_id_2:\n",
    "#     ....\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Debug code for model editing after training has been completed.\n",
    "\n",
    "# # If the model needs to be created, a.k.a. the training loop cell wasn't executed.\n",
    "# model = DIQTSystem(\n",
    "#     channels=channels,\n",
    "#     downsample_factor=exp_params.downsample_factor,\n",
    "#     source_vox_size=exp_params.source_vox_size,\n",
    "#     target_vox_size=exp_params.target_vox_size,\n",
    "#     norm_method=network_norm_method,\n",
    "#     train_loss_method=train_loss_name,\n",
    "#     opt_params=opt_params,\n",
    "#     val_viz_subj_id=val_viz_subj_id,\n",
    "#     val_viz_bboxes=bbox_coords,\n",
    "#     val_viz_every_n_epochs=5,\n",
    "# )\n",
    "\n",
    "# model.load_from_checkpoint(\n",
    "#     experiment_results_dir / \"model.ckpt\",,\n",
    "#     channels=channels,\n",
    "#     downsample_factor=exp_params.downsample_factor,\n",
    "#     source_vox_size=exp_params.source_vox_size,\n",
    "#     target_vox_size=exp_params.target_vox_size,\n",
    "#     norm_method=network_norm_method,\n",
    "#     train_loss_method=train_loss_name,\n",
    "#     opt_params=opt_params,\n",
    "#     val_viz_subj_id=val_viz_subj_id,\n",
    "#     val_viz_bboxes=bbox_coords,\n",
    "#     val_viz_every_n_epochs=5,\n",
    "# )\n",
    "\n",
    "# trainer.test(model, dataloaders=test_loader, ckpt_path=None, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.test(model, test_dataloaders=test_loader, ckpt_path=None, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store DIQT test reconstructions for visualization.\n",
    "for subj in test_dataset:\n",
    "    subj_id = subj.subj_id\n",
    "    test_vol_viz[subj_id].dti.diqt = (\n",
    "        model.plain_log.viz.test_preds[subj_id].cpu().numpy()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_vol_viz.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loss_name = \"RMSE\"\n",
    "test_loss_log_file = experiment_results_dir / \"test_loss.csv\"\n",
    "\n",
    "with open(log_txt_file, \"a+\") as f:\n",
    "    f.write(f\"Test loss function: {test_loss_name}\\n\")\n",
    "\n",
    "row_iter = enumerate(list(model.plain_log[\"test_loss\"].values()))\n",
    "test_loss_tabular = \"\".join([f\"{batch_idx}, {loss}\\n\" for batch_idx, loss in row_iter])\n",
    "\n",
    "with open(test_loss_log_file, \"a+\") as f:\n",
    "    f.write(\"batch_idx, loss\\n\")\n",
    "    f.write(test_loss_tabular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Spline Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spline_test_log = dict()\n",
    "\n",
    "for subj in test_dataset:\n",
    "    print(\"---\")\n",
    "    target_shape = np.asarray(subj[\"gt_dti\"][\"data\"].cpu().numpy().shape[1:])\n",
    "    # Fractional downsampling can't have a perfect correspondance between source shape and\n",
    "    # target shape, so calculate the \"actual\" downsample done by the NN and have the\n",
    "    # spline match that downsample factor.\n",
    "    downsample_adjusted = target_shape / np.asarray(\n",
    "        subj[\"lr_dti\"][\"data\"].numpy().shape[1:]\n",
    "    )\n",
    "\n",
    "    interp_spline = scipy.ndimage.zoom(\n",
    "        subj[\"source_lr_dti\"][\"data\"].cpu().numpy(),\n",
    "        zoom=(1,) + tuple(downsample_adjusted),\n",
    "        order=exp_params.spline.order,\n",
    "    )\n",
    "    # 0-out everything not in the mask, for both visualization and quantification.\n",
    "    interp_spline = interp_spline * subj[\"fr_brain_mask\"][\"data\"].bool().cpu().numpy()\n",
    "\n",
    "    subj_id = subj[\"subj_id\"]\n",
    "    print(f\"Subj {subj_id} done\")\n",
    "\n",
    "    spline_test_log[subj_id] = interp_spline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spline_loss = list()\n",
    "\n",
    "# Calculate spline loss for test images.\n",
    "for subj in test_dataset:\n",
    "    subj_id = subj[\"subj_id\"]\n",
    "    spline_pred = spline_test_log[subj_id]\n",
    "\n",
    "    gt = subj[\"gt_dti\"][\"data\"]\n",
    "    gt_means = torch.from_numpy(subj[\"fr_means\"])\n",
    "    gt_vars = torch.from_numpy(subj[\"fr_vars\"])\n",
    "    lr_means = torch.from_numpy(subj[\"lr_means\"])\n",
    "    lr_vars = torch.from_numpy(subj[\"lr_vars\"])\n",
    "\n",
    "    brain_mask = subj[\"fr_brain_mask\"][\"data\"].bool().cpu().numpy()[0]\n",
    "    #     if (\n",
    "    #         exp_params.data.data_norm_method is not None\n",
    "    #         and \"channel\" in exp_params.data.data_norm_method.casefold()\n",
    "    #     ):\n",
    "    #         print(\"De-normalizing\")\n",
    "    #         spline_pred = torch.from_numpy(spline_pred)\n",
    "    #         spline_pred = pitn.data.norm.denormalize_batch(\n",
    "    #             spline_pred, mean=lr_means, var=lr_vars, eps=1e-10\n",
    "    #         )\n",
    "\n",
    "    #         spline_pred = spline_pred.detach().cpu().numpy()\n",
    "\n",
    "    gt = gt.detach().cpu().numpy()\n",
    "\n",
    "    # Calculate the RMSE of just the values found in the mask.\n",
    "    se = (gt - spline_pred) ** 2\n",
    "    se = se[:, brain_mask]\n",
    "    loss = np.sqrt(se.mean())\n",
    "    spline_loss.append(loss)\n",
    "\n",
    "    # Store spline test reconstructions for visualization.\n",
    "    test_vol_viz[subj_id].dti.spline = spline_pred.copy()\n",
    "\n",
    "# Find the grand mean of the spline RMSE's\n",
    "spline_loss_mean = np.mean(spline_loss)\n",
    "print(spline_loss_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluation Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot testing loss values over all patches.\n",
    "fig, ax_prob = plt.subplots(figsize=(8, 4), dpi=120)\n",
    "log_scale = False\n",
    "\n",
    "hist = sns.histplot(\n",
    "    list(model.plain_log[\"test_loss\"].values()),\n",
    "    alpha=0.5,\n",
    "    stat=\"count\",\n",
    "    log_scale=log_scale,\n",
    "    ax=ax_prob,\n",
    "    legend=False,\n",
    "    hatch=\"\\\\\\\\\",\n",
    "    ec=\"blue\",\n",
    ")\n",
    "hist.yaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n",
    "plt.xlabel(\"Loss in $mm^2/second$\")\n",
    "\n",
    "# Draw means of different comparison models.\n",
    "comparison_kwargs = {\"ls\": \"-\", \"lw\": 2.5}\n",
    "# Plot the current DNN model performance.\n",
    "plt.axvline(\n",
    "    np.asarray(list(model.plain_log[\"test_loss\"].values())).mean(),\n",
    "    label=\"Current Model Mean\",\n",
    "    color=\"blue\",\n",
    "    **comparison_kwargs,\n",
    ")\n",
    "# Our spline mean performance.\n",
    "plt.axvline(\n",
    "    spline_loss_mean,\n",
    "    label=f\"(Ours) Spline Mean Order {exp_params.spline.order}\",\n",
    "    color=\"black\",\n",
    "    **comparison_kwargs,\n",
    ")\n",
    "sns.histplot(\n",
    "    spline_loss,\n",
    "    alpha=0.5,\n",
    "    stat=\"count\",\n",
    "    log_scale=log_scale,\n",
    "    ax=ax_prob,\n",
    "    legend=False,\n",
    "    color=\"black\",\n",
    "    hatch=\"//\",\n",
    ")\n",
    "\n",
    "# Tanno, et. al., 2021 model comparisons.\n",
    "# Taken from Table 2, HCP exterior.\n",
    "plt.axvline(\n",
    "    31.738e-4,\n",
    "    label=\"(Tanno etal, 2021) C-spline Mean\",\n",
    "    color=\"red\",\n",
    "    **comparison_kwargs,\n",
    ")\n",
    "plt.axvline(\n",
    "    23.139e-4,\n",
    "    label=\"(Tanno etal, 2021) RF\",\n",
    "    color=\"orange\",\n",
    "    **comparison_kwargs,\n",
    ")\n",
    "plt.axvline(\n",
    "    13.609e-4,\n",
    "    label=\"(Tanno etal, 2021) ESPCN Baseline\",\n",
    "    color=\"green\",\n",
    "    **comparison_kwargs,\n",
    ")\n",
    "plt.axvline(\n",
    "    13.412e-4,\n",
    "    label=\"(Tanno etal, 2021) Best\",\n",
    "    color=\"purple\",\n",
    "    **comparison_kwargs,\n",
    ")\n",
    "# Best performing Blumberg, et. al., 2018 paper model.\n",
    "plt.axvline(\n",
    "    12.78e-4,\n",
    "    label=\"(Blumberg etal, 2018) Best\",\n",
    "    color=\"pink\",\n",
    "    **comparison_kwargs,\n",
    ")\n",
    "\n",
    "plt.legend(fontsize=\"small\")\n",
    "plt.title(f\"Test Loss Histogram Over All Subjects with Test Metric {test_loss_name}\")\n",
    "plt.savefig(experiment_results_dir / \"test_loss_hist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot testing loss values over all subjects.\n",
    "fig, ax = plt.subplots(figsize=(8, 4), dpi=120)\n",
    "\n",
    "models = (\n",
    "    \"(Ours)\\nCurrent Model\",\n",
    "    f\"(Ours)\\nSpline Order {exp_params.spline.order}\",\n",
    "    \"(Tanno etal, 2021)\\nC-spline Mean\",\n",
    "    \"(Tanno etal, 2021)\\nRF\",\n",
    "    \"(Tanno etal, 2021)\\nESPCN Baseline\",\n",
    "    \"(Tanno etal, 2021)\\nBest\",\n",
    "    \"(Blumberg etal, 2018)\\nBest\",\n",
    ")\n",
    "\n",
    "rmse_scores = (\n",
    "    np.asarray(list(model.plain_log[\"test_loss\"].values())).mean(),\n",
    "    spline_loss_mean,\n",
    "    31.738e-4,\n",
    "    23.139e-4,\n",
    "    13.609e-4,\n",
    "    13.412e-4,\n",
    "    12.13e-4,\n",
    ")\n",
    "rmse_std_error = np.asarray([0, 0, 0, 0.351e-4, 0.084e-4, 0.041e-4, 1.24e-4])\n",
    "\n",
    "ax.grid(True, axis=\"y\", zorder=1000)\n",
    "ax.set_axisbelow(True)\n",
    "ax.bar(\n",
    "    models,\n",
    "    rmse_scores,\n",
    "    yerr=rmse_std_error,\n",
    "    color=sns.color_palette(\"deep\", n_colors=len(rmse_scores)),\n",
    "    edgecolor=\"black\",\n",
    "    lw=0.75,\n",
    ")\n",
    "\n",
    "for container in ax.containers:\n",
    "    if isinstance(container, mpl.container.BarContainer):\n",
    "        ax.bar_label(container, fmt=\"%.3e\")\n",
    "\n",
    "ax.set_ylim(bottom=0, top=ax.get_ylim()[1] * 1.1)\n",
    "ax.set_xlabel(\"Model\")\n",
    "ax.set_ylabel(\"Loss in $mm^2/second$\")\n",
    "\n",
    "ax.set_title(f\"Mean Over Subjects Test Loss {test_loss_name}\")\n",
    "ax.set_xticks(models)\n",
    "ax.set_xticklabels(\n",
    "    models, fontsize=\"x-small\", rotation=25, ha=\"right\", rotation_mode=\"anchor\"\n",
    ")\n",
    "plt.savefig(experiment_results_dir / \"test_loss_bar.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model.plain_log[\"test_loss\"])\n",
    "print(np.mean(list(model.plain_log[\"test_loss\"].values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted_test_idx = np.argsort(np.asarray(list(model.plain_log[\"test_loss\"].values())))\n",
    "sorted_test_results = dict(\n",
    "    list(model.plain_log[\"test_loss\"].items())[i] for i in sorted_test_idx\n",
    ")\n",
    "ppr(sorted_test_results, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(spline_loss)\n",
    "print(spline_loss_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.add_histogram(\n",
    "    \"test/rmse_dist\", np.asarray(list(model.plain_log[\"test_loss\"].values()))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save out metrics and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compare_hparams.metric[\"hparam/rmse\"] = np.mean(\n",
    "    list(model.plain_log[\"test_loss\"].values())\n",
    ")\n",
    "with open(log_txt_file, \"a+\") as f:\n",
    "    f.write(\n",
    "        f\"Mean RMSE Testing Value: {np.mean(list(model.plain_log['test_loss'].values()))}\\n\"\n",
    "    )\n",
    "    f.write(f\"Mean RMSE Spline Value: {spline_loss_mean}\\n\")\n",
    "\n",
    "logger.add_scalar(\"metric/rmse\", np.mean(list(model.plain_log[\"test_loss\"].values())))\n",
    "logger.add_scalar(\"metric/spline\", spline_loss_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Whole-Volume Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Debug flag(s)\n",
    "disable_fig_save = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create full 3D volume of full-res ground truth, low-res downsample, full-res mask, and\n",
    "# full-res predictions.\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for subj_i in test_dataset:\n",
    "        subj = copy.copy(subj_i)\n",
    "        subj_id = subj[\"subj_id\"]\n",
    "        print(f\"Starting subject {subj_id}\")\n",
    "\n",
    "        # Collect all variants of the volume and aggregate into one container object.\n",
    "        fr_vol = subj[\"gt_dti\"][\"data\"].clone()\n",
    "        lr_vol = subj[\"lr_dti\"][\"data\"].clone()\n",
    "\n",
    "        full_res_predicted = torch.from_numpy(test_vol_viz[subj_id].dti.diqt.copy())\n",
    "        full_res_spline = torch.from_numpy(test_vol_viz[subj_id].dti.spline.copy())\n",
    "\n",
    "        #         warnings.warn(\"======== Skipping all de-normalization for visualization.\")\n",
    "        #         if False:\n",
    "        if data_norm_method is not None and \"channel\" in data_norm_method.casefold():\n",
    "            print(\"Normalizing\")\n",
    "\n",
    "            lr_means = torch.as_tensor(subj[\"lr_means\"]).to(lr_vol).clone()\n",
    "            lr_vars = torch.as_tensor(subj[\"lr_vars\"]).to(lr_vol).clone()\n",
    "\n",
    "            lr_vol = pitn.data.norm.denormalize_dti(\n",
    "                lr_vol, mean=lr_means, var=lr_vars\n",
    "            ).clone()\n",
    "\n",
    "        # Zero-out all voxels outside the mask.\n",
    "\n",
    "        fr_mask = subj[\"fr_brain_mask\"][\"data\"].bool().clone()\n",
    "        full_res_spline = full_res_spline * fr_mask.to(full_res_spline).bool()\n",
    "        fr_vol = fr_vol * fr_mask.to(fr_vol).bool()\n",
    "        full_res_predicted = full_res_predicted * fr_mask.to(full_res_predicted).bool()\n",
    "        lr_vol = lr_vol * subj[\"lr_brain_mask\"][\"data\"].to(lr_vol).bool()\n",
    "        abs_error = torch.abs(full_res_predicted - fr_vol)\n",
    "\n",
    "        test_vol_viz[subj.subj_id].dti.update(\n",
    "            fr=fr_vol.cpu().numpy(),\n",
    "            lr=lr_vol.cpu().numpy(),\n",
    "            diqt=full_res_predicted.cpu().numpy(),\n",
    "            spline=full_res_spline.cpu().numpy(),\n",
    "        )\n",
    "\n",
    "        test_vol_viz[subj.subj_id].fa.update(\n",
    "            itertools.starmap(\n",
    "                lambda k, v: (k, pitn.viz.fa_map(test_vol_viz[subj_id].dti[k])),\n",
    "                test_vol_viz[subj_id].dti.items(),\n",
    "            )\n",
    "        )\n",
    "        test_vol_viz[subj_id].fr_mask = fr_mask.cpu().numpy()\n",
    "        test_vol_viz[subj_id].dti.abs_error = abs_error.cpu().numpy()\n",
    "\n",
    "        print(f\"Finished subject {subj_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save out all network predictions to Nifti2 files and compress them into a zip archive.\n",
    "if not disable_fig_save:\n",
    "    img_names = list()\n",
    "    for subj_id, viz in test_vol_viz.items():\n",
    "        pred_vol = viz.dti.diqt\n",
    "        affine = subj_data[subj_id][\"gt_dti\"].affine\n",
    "        nib_img = nib.Nifti2Image(pred_vol, affine)\n",
    "\n",
    "        filename = experiment_results_dir / f\"{subj_id}_predicted_dti.nii.gz\"\n",
    "        nib.save(nib_img, str(filename))\n",
    "        img_names.append(filename)\n",
    "\n",
    "    with zipfile.ZipFile(experiment_results_dir / \"predicted_dti.zip\", \"w\") as fzip:\n",
    "        for filename in img_names:\n",
    "            fzip.write(\n",
    "                filename,\n",
    "                arcname=filename.name,\n",
    "                compress_type=zipfile.ZIP_DEFLATED,\n",
    "                compresslevel=6,\n",
    "            )\n",
    "            os.remove(filename)\n",
    "    # Make sure we exit the 'with' statement above.\n",
    "    print(\"Done with files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "viz_subj_idx = random.choice(list(range(len(test_vol_viz.keys()))))\n",
    "# viz_subj_id = list(test_vol_viz.keys())[viz_subj_idx]\n",
    "# Pick the worst performing subject from the test set.\n",
    "viz_subj_id = list(sorted_test_results.keys())[-1]\n",
    "print(list(test_vol_viz.keys()))\n",
    "print(viz_subj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FA-Weighted Direction Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate FA-weighted diffusion direction map for prediction.\n",
    "pred_dir_map = pitn.viz.direction_map(test_vol_viz[viz_subj_id].dti.diqt)\n",
    "# Set channels last for matplotlib\n",
    "pred_dir_map = pred_dir_map.transpose(1, 2, 3, 0)\n",
    "test_vol_viz[viz_subj_id].color_fa.diqt = pred_dir_map\n",
    "\n",
    "spline_dir_map = pitn.viz.direction_map(test_vol_viz[viz_subj_id].dti.spline)\n",
    "# Set channels last for matplotlib\n",
    "spline_dir_map = spline_dir_map.transpose(1, 2, 3, 0)\n",
    "test_vol_viz[viz_subj_id].color_fa.spline = spline_dir_map\n",
    "\n",
    "fr_dir_map = pitn.viz.direction_map(test_vol_viz[viz_subj_id].dti.fr)\n",
    "# Set channels last for matplotlib\n",
    "fr_dir_map = fr_dir_map.transpose(1, 2, 3, 0)\n",
    "test_vol_viz[viz_subj_id].color_fa.fr = fr_dir_map\n",
    "\n",
    "lr_dir_map = pitn.viz.direction_map(test_vol_viz[viz_subj_id].dti.lr)\n",
    "# Set channels last for matplotlib\n",
    "lr_dir_map = lr_dir_map.transpose(1, 2, 3, 0)\n",
    "test_vol_viz[viz_subj_id].color_fa.lr = lr_dir_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "slice_idx = (slice(None, None, None), slice(None, None, None), 86)\n",
    "low_res_slice_idx = tuple(\n",
    "    int(np.round(s / downsample_factor)) if isinstance(s, int) else s for s in slice_idx\n",
    ")\n",
    "print(slice_idx)\n",
    "print(low_res_slice_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.imshow(np.rot90(pred_dir_map[slice_idx]), interpolation=\"none\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Predicted with DIQT Net\")\n",
    "if not disable_fig_save:\n",
    "    plt.savefig(experiment_results_dir / \"pred_dir_map_sample.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.imshow(np.rot90(fr_dir_map[slice_idx]), interpolation=\"none\")\n",
    "# plt.colorbar()\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Ground Truth\")\n",
    "if not disable_fig_save:\n",
    "    plt.savefig(experiment_results_dir / \"ground_truth_dir_map_sample.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.imshow(np.rot90(spline_dir_map[slice_idx]), interpolation=\"none\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"Spline Interpolation Order {exp_params.spline.order}\")\n",
    "if not disable_fig_save:\n",
    "    plt.savefig(experiment_results_dir / \"cubic_spline_dir_map_sample.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.imshow(np.rot90(lr_dir_map[low_res_slice_idx]), interpolation=\"none\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"LR Input\")\n",
    "if not disable_fig_save:\n",
    "    plt.savefig(experiment_results_dir / \"low_res_map_sample.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DTI Channel-Wise Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "channel_names = [\n",
    "    \"$D_{x,x}$\",\n",
    "    \"$D_{x,y}$\",\n",
    "    \"$D_{y,y}$\",\n",
    "    \"$D_{x,z}$\",\n",
    "    \"$D_{y,z}$\",\n",
    "    \"$D_{z,z}$\",\n",
    "]\n",
    "dti_names = [\n",
    "    \"Full-Res\",\n",
    "    \"Low-Res Input\",\n",
    "    f\"Spline Order {exp_params.spline.order}\",\n",
    "    \"Predicted\",\n",
    "    \"Absolute Error\\nFR vs. Predicted\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display all 6 DTIs for ground truth, predicted, and root squared error\n",
    "\n",
    "cmap = \"jet\"\n",
    "\n",
    "dtis = [\n",
    "    test_vol_viz[viz_subj_id].dti.fr[(slice(None), *slice_idx)],\n",
    "    test_vol_viz[viz_subj_id].dti.lr[(slice(None), *low_res_slice_idx)],\n",
    "    test_vol_viz[viz_subj_id].dti.spline[(slice(None), *slice_idx)],\n",
    "    test_vol_viz[viz_subj_id].dti.diqt[(slice(None), *slice_idx)],\n",
    "    test_vol_viz[viz_subj_id].dti.abs_error[(slice(None), *slice_idx)],\n",
    "]\n",
    "\n",
    "nrows = len(dtis)\n",
    "ncols = len(channel_names)\n",
    "\n",
    "# Don't take the absolute max and min values, as there exist some extreme (e.g., > 3\n",
    "# orders of magnitude) outliers. Instead, take some percente quantile.\n",
    "# Reshape and concatenate the dtis in order to compute the quantiles of images with\n",
    "# different shapes (e.g., the low-res input patch).\n",
    "max_dti = np.quantile(np.concatenate([di.reshape(6, -1) for di in dtis], axis=1), 0.95)\n",
    "min_dti = np.quantile(np.concatenate([di.reshape(6, -1) for di in dtis], axis=1), 0.05)\n",
    "\n",
    "nrows = len(dtis)\n",
    "ncols = len(channel_names)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6), dpi=160)\n",
    "\n",
    "grid = mpl.gridspec.GridSpec(\n",
    "    nrows,\n",
    "    ncols,\n",
    "    figure=fig,\n",
    "    hspace=0.05,\n",
    "    wspace=0.05,\n",
    ")\n",
    "axs = list()\n",
    "max_subplot_height = 0\n",
    "for i_row in range(nrows):\n",
    "    dti = dtis[i_row]\n",
    "\n",
    "    for j_col in range(ncols):\n",
    "        ax = fig.add_subplot(grid[i_row, j_col])\n",
    "        ax.imshow(\n",
    "            np.rot90(dti[j_col]),\n",
    "            cmap=cmap,\n",
    "            interpolation=None,\n",
    "            vmin=min_dti,\n",
    "            vmax=max_dti,\n",
    "        )\n",
    "        if ax.get_subplotspec().is_first_col():\n",
    "            ax.set_ylabel(dti_names[i_row], size=\"small\")\n",
    "        if ax.get_subplotspec().is_last_row():\n",
    "            ax.set_xlabel(channel_names[j_col])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        # Update highest subplot to put the `suptitle` later on.\n",
    "        max_subplot_height = max(\n",
    "            max_subplot_height, ax.get_position(original=False).get_points()[1, 1]\n",
    "        )\n",
    "        axs.append(ax)\n",
    "\n",
    "color_norm = mpl.colors.Normalize(vmin=min_dti, vmax=max_dti)\n",
    "fig.colorbar(\n",
    "    mpl.cm.ScalarMappable(norm=color_norm, cmap=cmap),\n",
    "    ax=axs,\n",
    "    location=\"right\",\n",
    "    fraction=0.1,\n",
    "    pad=0.03,\n",
    ")\n",
    "plt.suptitle(\n",
    "    \"DTI Channel Breakdown, Normalized over All Images\",\n",
    "    y=max_subplot_height + 0.015,\n",
    "    verticalalignment=\"bottom\",\n",
    ")\n",
    "if not disable_fig_save:\n",
    "    plt.savefig(experiment_results_dir / \"DTI_channel_sample_global_norm.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Channel-Wise Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display all 6 DTIs for ground truth, predicted, low-res input, and root squared error\n",
    "# Normalize by index in the DTI coefficients.\n",
    "# Reshape and concatenate the dtis in order to compute the quantiles of images with\n",
    "# different shapes (e.g., the low-res input patch).\n",
    "\n",
    "cmap = \"coolwarm\"\n",
    "dtis = [\n",
    "    test_vol_viz[viz_subj_id].dti.fr[(slice(None), *slice_idx)],\n",
    "    test_vol_viz[viz_subj_id].dti.lr[(slice(None), *low_res_slice_idx)],\n",
    "    test_vol_viz[viz_subj_id].dti.spline[(slice(None), *slice_idx)],\n",
    "    test_vol_viz[viz_subj_id].dti.diqt[(slice(None), *slice_idx)],\n",
    "    test_vol_viz[viz_subj_id].dti.abs_error[(slice(None), *slice_idx)],\n",
    "]\n",
    "\n",
    "nrows = len(dtis)\n",
    "ncols = len(channel_names)\n",
    "\n",
    "# Don't take the absolute max and min values, as there exist some extreme (e.g., > 3\n",
    "# orders of magnitude) outliers. Instead, take some percente quantile.\n",
    "max_dtis = np.quantile(\n",
    "    np.concatenate([di.reshape(6, -1) for di in dtis], axis=1), 0.95, axis=1\n",
    ")\n",
    "min_dtis = np.quantile(\n",
    "    np.concatenate([di.reshape(6, -1) for di in dtis], axis=1), 0.05, axis=1\n",
    ")\n",
    "\n",
    "max_dtis = np.max(np.abs([max_dtis, min_dtis]), axis=0)\n",
    "min_dtis = -1 * max_dtis\n",
    "\n",
    "nrows = len(dtis)\n",
    "ncols = len(channel_names)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 7), dpi=160)\n",
    "\n",
    "grid = mpl.gridspec.GridSpec(\n",
    "    nrows,\n",
    "    ncols,\n",
    "    figure=fig,\n",
    "    hspace=0.05,\n",
    "    wspace=0.05,\n",
    ")\n",
    "\n",
    "axs = list()\n",
    "max_subplot_height = 0\n",
    "for i_row in range(nrows):\n",
    "    dti = dtis[i_row]\n",
    "    axs_cols = list()\n",
    "\n",
    "    for j_col in range(ncols):\n",
    "        ax = fig.add_subplot(grid[i_row, j_col])\n",
    "        ax.imshow(\n",
    "            np.rot90(dti[j_col]),\n",
    "            cmap=cmap,\n",
    "            interpolation=None,\n",
    "            vmin=min_dtis[j_col],\n",
    "            vmax=max_dtis[j_col],\n",
    "        )\n",
    "        if ax.get_subplotspec().is_first_col():\n",
    "            ax.set_ylabel(dti_names[i_row], size=\"small\")\n",
    "        if ax.get_subplotspec().is_last_row():\n",
    "            ax.set_xlabel(channel_names[j_col])\n",
    "\n",
    "        # Update highest subplot to put the `suptitle` later on.\n",
    "        max_subplot_height = max(\n",
    "            max_subplot_height, ax.get_position(original=False).get_points()[1, 1]\n",
    "        )\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "        axs_cols.append(ax)\n",
    "\n",
    "    axs.append(axs_cols)\n",
    "\n",
    "# Place colorbars on each column.\n",
    "for j_col in range(ncols):\n",
    "\n",
    "    full_col_ax = [axs[i][j_col] for i in range(nrows)]\n",
    "\n",
    "    color_norm = mpl.colors.Normalize(vmin=min_dtis[j_col], vmax=max_dtis[j_col])\n",
    "\n",
    "    color_mappable = mpl.cm.ScalarMappable(norm=color_norm, cmap=cmap)\n",
    "    cbar = fig.colorbar(\n",
    "        color_mappable,\n",
    "        ax=full_col_ax,\n",
    "        location=\"top\",\n",
    "        orientation=\"horizontal\",\n",
    "        pad=0.01,\n",
    "        shrink=0.85,\n",
    "    )\n",
    "    cbar.ax.tick_params(labelsize=8, rotation=35)\n",
    "    cbar.ax.xaxis.set_major_formatter(mpl.ticker.StrMethodFormatter(\"{x:g}\"))\n",
    "#     cbar.ax.ticklabel_format(scilimits=(3, -3), useOffset=False)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"DTI Channel Breakdown, Channel-Wise Normalization\",\n",
    "    y=max_subplot_height + 0.01,\n",
    "    verticalalignment=\"bottom\",\n",
    ")\n",
    "if not disable_fig_save:\n",
    "    plt.savefig(experiment_results_dir / \"DTI_channel_sample_channel_wise_norm.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### FA Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Slice locations for 2D visualization\n",
    "\n",
    "half_fr_space_shape = np.floor(\n",
    "    np.asarray(test_vol_viz[viz_subj_id].fa.fr.shape) // 2\n",
    ").astype(int)\n",
    "half_fr_space_shape = half_fr_space_shape.tolist()\n",
    "\n",
    "slice_indices = [\n",
    "    (half_fr_space_shape[0], slice(None, None, None), slice(None, None, None)),\n",
    "    (slice(None, None, None), half_fr_space_shape[1], slice(None, None, None)),\n",
    "    (slice(None, None, None), slice(None, None, None), half_fr_space_shape[2]),\n",
    "]\n",
    "\n",
    "low_res_slice_indices = list()\n",
    "for slice_idx_i in slice_indices:\n",
    "    slice_coords = tuple()\n",
    "    for s in slice_idx_i:\n",
    "        slice_coords = slice_coords + (\n",
    "            int(np.floor(s / downsample_factor)) if isinstance(s, int) else s,\n",
    "        )\n",
    "    low_res_slice_indices.append(slice_coords)\n",
    "\n",
    "print(slice_indices)\n",
    "print()\n",
    "print(low_res_slice_indices)\n",
    "\n",
    "row_names = [\n",
    "    \"Saggital\",\n",
    "    \"Coronal\",\n",
    "    \"Horizontal\",\n",
    "]\n",
    "\n",
    "model_names = [\n",
    "    \"Full-Res\",\n",
    "    \"Low-Res Input\",\n",
    "    f\"Spline Order {exp_params.spline.order}\",\n",
    "    \"Predicted\",\n",
    "    #     \"Absolute Error\\nFR vs. Predicted\",\n",
    "]\n",
    "\n",
    "nrows = len(row_names)\n",
    "ncols = len(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imgs = dict()\n",
    "for i_row, slice_i in enumerate(slice_indices):\n",
    "    low_res_slice_i = low_res_slice_indices[i_row]\n",
    "    imgs[i_row] = dict()\n",
    "    col_imgs = [\n",
    "        test_vol_viz[viz_subj_id].fa.fr[(*slice_i,)],\n",
    "        test_vol_viz[viz_subj_id].fa.lr[(*low_res_slice_i,)],\n",
    "        test_vol_viz[viz_subj_id].fa.spline[(*slice_i,)],\n",
    "        test_vol_viz[viz_subj_id].fa.diqt[(*slice_i,)],\n",
    "        #         test_vol_viz[viz_subj_id].fa.abs_error[(*slice_i,)],\n",
    "    ]\n",
    "\n",
    "    imgs[i_row].update(tuple(enumerate(col_imgs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display all 6 DTIs for ground truth, predicted, and root squared error\n",
    "\n",
    "cmap = \"gist_gray\"\n",
    "\n",
    "# Don't take the absolute max and min values, as there exist some extreme (e.g., > 3\n",
    "# orders of magnitude) outliers. Instead, take some percente quantile.\n",
    "# Concatenate the images in order to compute the quantiles of images with\n",
    "# different shapes (e.g., the low-res input patch).\n",
    "max_intensity = np.quantile(\n",
    "    np.concatenate(\n",
    "        [imgs[i][j] for (i, j) in itertools.product(range(nrows), range(ncols))],\n",
    "        axis=None,\n",
    "    ),\n",
    "    0.95,\n",
    ")\n",
    "min_intensity = np.quantile(\n",
    "    np.concatenate(\n",
    "        [imgs[i][j] for (i, j) in itertools.product(range(nrows), range(ncols))],\n",
    "        axis=None,\n",
    "    ),\n",
    "    0.05,\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(5 * 1.5, 3 * 1.5), dpi=160)\n",
    "\n",
    "grid = mpl.gridspec.GridSpec(\n",
    "    nrows,\n",
    "    ncols,\n",
    "    figure=fig,\n",
    "    hspace=0.05,\n",
    "    wspace=0.05,\n",
    ")\n",
    "axs = list()\n",
    "max_subplot_height = 0\n",
    "for i_row in range(nrows):\n",
    "    img_row = imgs[i_row]\n",
    "\n",
    "    for j_col in range(ncols):\n",
    "        ax = fig.add_subplot(grid[i_row, j_col])\n",
    "        ax.imshow(\n",
    "            np.rot90(img_row[j_col]),\n",
    "            cmap=cmap,\n",
    "            interpolation=None,\n",
    "            vmin=min_intensity,\n",
    "            vmax=max_intensity,\n",
    "        )\n",
    "        if ax.get_subplotspec().is_first_col():\n",
    "            ax.set_ylabel(row_names[i_row], size=\"small\")\n",
    "        if ax.get_subplotspec().is_last_row():\n",
    "            ax.set_xlabel(model_names[j_col], size=\"small\")\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        # Update highest subplot to put the `suptitle` later on.\n",
    "        max_subplot_height = max(\n",
    "            max_subplot_height, ax.get_position(original=False).get_points()[1, 1]\n",
    "        )\n",
    "        axs.append(ax)\n",
    "\n",
    "color_norm = mpl.colors.Normalize(vmin=min_intensity, vmax=max_intensity)\n",
    "fig.colorbar(\n",
    "    mpl.cm.ScalarMappable(norm=color_norm, cmap=cmap),\n",
    "    ax=axs,\n",
    "    location=\"right\",\n",
    "    fraction=0.1,\n",
    "    pad=0.03,\n",
    ")\n",
    "plt.suptitle(\n",
    "    \"FA Maps, Normalized over All Images\",\n",
    "    y=max_subplot_height + 0.015,\n",
    "    verticalalignment=\"bottom\",\n",
    ")\n",
    "if not disable_fig_save:\n",
    "    plt.savefig(experiment_results_dir / \"fa_sample_global_norm.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pl_logger.experiment.flush()\n",
    "# Close tensorboard logger.\n",
    "# Don't finalize if the experiment was for debugging.\n",
    "if \"debug\" not in EXPERIMENT_NAME.casefold():\n",
    "    pl_logger.finalize(\"success\")\n",
    "    # Experiment is complete, move the results directory to its final location.\n",
    "    if experiment_results_dir != final_experiment_results_dir:\n",
    "        print(\"Moving out of tmp location\")\n",
    "        experiment_results_dir = experiment_results_dir.rename(\n",
    "            final_experiment_results_dir\n",
    "        )\n",
    "        log_txt_file = experiment_results_dir / log_txt_file.name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda-pitn]",
   "language": "python",
   "name": "conda-env-miniconda-pitn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
