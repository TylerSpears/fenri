{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13edb204-c964-4525-80d8-0fdd2730df3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pain in the Net - Baseline Shallow Methods on T1w Images\n",
    "\n",
    "\n",
    "Code by:\n",
    "\n",
    "Tyler Spears - tas6hh@virginia.edu\n",
    "\n",
    "Dr. Tom Fletcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da03c5c0-aea8-4c1c-98ca-bf001e59dde2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports & Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6f91f0-eca2-4990-9214-f34aa16b4d05",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bbe9cc-5b3f-4204-870d-4a5bc5f28aab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automatically re-import project-specific modules.\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# imports\n",
    "import collections\n",
    "import functools\n",
    "import io\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "import copy\n",
    "import pdb\n",
    "import inspect\n",
    "import random\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import typing\n",
    "import zipfile\n",
    "\n",
    "import ants\n",
    "import dipy\n",
    "import dipy.core\n",
    "import dipy.reconst\n",
    "import dipy.reconst.dti\n",
    "import dipy.segment.mask\n",
    "import dipy.viz\n",
    "import dipy.viz.regtools\n",
    "import dotenv\n",
    "\n",
    "# visualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import mpl_toolkits\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import IPython\n",
    "\n",
    "# Try importing GPUtil for printing GPU specs.\n",
    "# May not be installed if using CPU only.\n",
    "try:\n",
    "    import GPUtil\n",
    "except ImportError:\n",
    "    warnings.warn(\"WARNING: Package GPUtil not found, cannot print GPU specs\")\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, Markdown\n",
    "import ipyplot\n",
    "\n",
    "# Data management libraries.\n",
    "import nibabel as nib\n",
    "import natsort\n",
    "from natsort import natsorted\n",
    "import addict\n",
    "from addict import Addict\n",
    "import box\n",
    "from box import Box\n",
    "import pprint\n",
    "from pprint import pprint as ppr\n",
    "\n",
    "# Computation & ML libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchio\n",
    "import pytorch_lightning as pl\n",
    "import monai\n",
    "\n",
    "import skimage\n",
    "import skimage.feature\n",
    "import skimage.filters\n",
    "import skimage.measure\n",
    "import scipy\n",
    "import sklearn\n",
    "import sklearn.neighbors\n",
    "import sklearn.pipeline\n",
    "import sklearn.mixture\n",
    "\n",
    "plt.rcParams.update({\"figure.autolayout\": True})\n",
    "plt.rcParams.update({\"figure.facecolor\": [1.0, 1.0, 1.0, 1.0]})\n",
    "\n",
    "# Set print options for ndarrays/tensors.\n",
    "np.set_printoptions(suppress=True, edgeitems=2, threshold=100, linewidth=88)\n",
    "torch.set_printoptions(\n",
    "    sci_mode=False, edgeitems=2, threshold=100, linewidth=88, profile=\"short\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b8326b-55e1-4195-a85f-7627f591f1f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update notebook's environment variables with direnv.\n",
    "# This requires the python-dotenv package, and direnv be installed on the system\n",
    "# This will not work on Windows.\n",
    "# NOTE: This is kind of hacky, and not necessarily safe. Be careful...\n",
    "# Libraries needed on the python side:\n",
    "# - os\n",
    "# - subprocess\n",
    "# - io\n",
    "# - dotenv\n",
    "\n",
    "# Form command to be run in direnv's context. This command will print out\n",
    "# all environment variables defined in the subprocess/sub-shell.\n",
    "command = f\"direnv exec {os.getcwd()} /usr/bin/env\"\n",
    "# Run command in a new subprocess.\n",
    "proc = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, cwd=os.getcwd())\n",
    "# Store and format the subprocess' output.\n",
    "proc_out = proc.communicate()[0].strip().decode(\"utf-8\")\n",
    "# Use python-dotenv to load the environment variables by using the output of\n",
    "# 'direnv exec ...' as a 'dummy' .env file.\n",
    "dotenv.load_dotenv(stream=io.StringIO(proc_out), override=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8449ea-9648-4517-8aab-7e85308436de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Project-specific scripts\n",
    "# It's easier to import it this way rather than make an entirely new package, due to\n",
    "# conflicts with local packages and anaconda installations.\n",
    "# You made me do this, poor python package management!!\n",
    "if \"PROJECT_ROOT\" in os.environ:\n",
    "    lib_location = str(Path(os.environ[\"PROJECT_ROOT\"]).resolve())\n",
    "else:\n",
    "    lib_location = str(Path(\"../../../\").resolve())\n",
    "if lib_location not in sys.path:\n",
    "    sys.path.insert(0, lib_location)\n",
    "import lib as pitn\n",
    "\n",
    "# Include the top-level lib module along with its submodules.\n",
    "%aimport lib\n",
    "# Grab all submodules of lib, not including modules outside of the package.\n",
    "includes = list(\n",
    "    filter(\n",
    "        lambda m: m.startswith(\"lib.\"),\n",
    "        map(lambda x: x[1].__name__, inspect.getmembers(pitn, inspect.ismodule)),\n",
    "    )\n",
    ")\n",
    "# Run aimport magic with constructed includes.\n",
    "ipy = IPython.get_ipython()\n",
    "ipy.run_line_magic(\"aimport\", \", \".join(includes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a706635-b544-4f41-886b-d6997dda27e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch setup\n",
    "# allow for CUDA usage, if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# keep device as the cpu\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498af6cf-4263-4415-b6f3-fd32abab4bcb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Specs Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff2952b-b4a3-4241-9322-e317c32cb2db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr cap\n",
    "# Capture output and save to log. Needs to be at the *very first* line of the cell.\n",
    "# Watermark\n",
    "%load_ext watermark\n",
    "%watermark --author \"Tyler Spears\" --updated --iso8601  --python --machine --iversions --githash\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    # GPU information\n",
    "    # Taken from\n",
    "    # <https://www.thepythoncode.com/article/get-hardware-system-information-python>.\n",
    "    # If GPUtil is not installed, skip this step.\n",
    "    try:\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        print(\"=\" * 50, \"GPU Specs\", \"=\" * 50)\n",
    "        list_gpus = []\n",
    "        for gpu in gpus:\n",
    "            # get the GPU id\n",
    "            gpu_id = gpu.id\n",
    "            # name of GPU\n",
    "            gpu_name = gpu.name\n",
    "            driver_version = gpu.driver\n",
    "            cuda_version = torch.version.cuda\n",
    "            # get total memory\n",
    "            gpu_total_memory = f\"{gpu.memoryTotal}MB\"\n",
    "            gpu_uuid = gpu.uuid\n",
    "            list_gpus.append(\n",
    "                (\n",
    "                    gpu_id,\n",
    "                    gpu_name,\n",
    "                    driver_version,\n",
    "                    cuda_version,\n",
    "                    gpu_total_memory,\n",
    "                    gpu_uuid,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            tabulate(\n",
    "                list_gpus,\n",
    "                headers=(\n",
    "                    \"id\",\n",
    "                    \"Name\",\n",
    "                    \"Driver Version\",\n",
    "                    \"CUDA Version\",\n",
    "                    \"Total Memory\",\n",
    "                    \"uuid\",\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    except NameError:\n",
    "        print(\"CUDA Version: \", torch.version.cuda)\n",
    "\n",
    "else:\n",
    "    print(\"CUDA not in use, falling back to CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae38f0a-8614-4e0d-a40a-bee5a3585baa",
   "metadata": {
    "tags": [
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Tyler Spears\n",
      "\n",
      "Last updated: 2021-10-27T15:47:32.446382+00:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.8\n",
      "IPython version      : 7.23.1\n",
      "\n",
      "Compiler    : GCC 7.3.0\n",
      "OS          : Linux\n",
      "Release     : 5.4.0-89-generic\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: d256576f3fe98ffb4827d194f1aa31d6bae082c1\n",
      "\n",
      "GPUtil           : 1.4.0\n",
      "torchio          : 0.18.37\n",
      "skimage          : 0.18.1\n",
      "addict           : 2.4.0\n",
      "monai            : 0.7.dev2138\n",
      "seaborn          : 0.11.1\n",
      "matplotlib       : 3.4.1\n",
      "torch            : 1.9.0\n",
      "sklearn          : 0.0\n",
      "scipy            : 1.5.3\n",
      "sys              : 3.8.8 (default, Feb 24 2021, 21:46:12) \n",
      "[GCC 7.3.0]\n",
      "IPython          : 7.23.1\n",
      "pandas           : 1.2.3\n",
      "ants             : 0.2.7\n",
      "dipy             : 1.4.1\n",
      "pytorch_lightning: 1.4.5\n",
      "nibabel          : 3.2.1\n",
      "natsort          : 7.1.1\n",
      "numpy            : 1.20.2\n",
      "box              : 5.4.1\n",
      "json             : 2.0.9\n",
      "\n",
      "================================================== GPU Specs ==================================================\n",
      "  id  Name              Driver Version      CUDA Version  Total Memory    uuid\n",
      "----  ----------------  ----------------  --------------  --------------  ----------------------------------------\n",
      "   0  NVIDIA TITAN RTX  470.57.02                   11.1  24217.0MB       GPU-586d2016-71ef-ebb3-437b-6721a22191ec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cap is defined in an ipython magic command\n",
    "print(cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09447c83-298e-444a-91ca-aa46058eb956",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Variables & Definitions Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc41112b-67dc-4fa3-828e-48a2c88dfb49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up directories\n",
    "data_dir = pathlib.Path(os.environ[\"DATA_DIR\"])\n",
    "\n",
    "processed_data_dir = pathlib.Path(os.environ[\"WRITE_DATA_DIR\"])\n",
    "hcp_processed_data_dir = (\n",
    "    processed_data_dir / \"hcp/derivatives/mean-downsample/scale-1.00mm\"\n",
    ")\n",
    "hcp_orig_scale_data_dir = (\n",
    "    processed_data_dir / \"hcp/derivatives/mean-downsample/scale-orig\"\n",
    ")\n",
    "clinic_processed_data_dir = (\n",
    "    processed_data_dir / \"oasis3/derivatives/mean-downsample/scale-orig\"\n",
    ")\n",
    "assert (\n",
    "    hcp_processed_data_dir.exists()\n",
    "    and clinic_processed_data_dir.exists()\n",
    "    and hcp_orig_scale_data_dir.exists()\n",
    ")\n",
    "results_dir = pathlib.Path(os.environ[\"RESULTS_DIR\"])\n",
    "assert results_dir.exists()\n",
    "tmp_results_dir = pathlib.Path(os.environ[\"TMP_RESULTS_DIR\"])\n",
    "assert tmp_results_dir.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7a7233-48c1-439d-af3f-62f628af527c",
   "metadata": {},
   "source": [
    "### Experiment Logging Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82e4788-bced-42d5-b341-3652317f2427",
   "metadata": {},
   "source": [
    "### Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90b2928-2b91-47f7-9152-038c00d924bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = Box(default_box=True)\n",
    "\n",
    "# Data params.\n",
    "params.num_channels = 1\n",
    "params.hcp.num_subjects = 13\n",
    "params.clinic.num_subjects = 9\n",
    "params.clamp_percentiles = (0.01, 99.0)\n",
    "# params.data_scale_range = None\n",
    "# Scale input data by the valid values of each channel of the vol.\n",
    "# I.e., Dx,x in [0, 1], Dx,y in [-1, 1], Dy,y in [0, 1], Dy,z in [-1, 1], etc.\n",
    "params.data_scale_range = None\n",
    "\n",
    "\n",
    "params.use_grad_penalty = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1009ab0f-71cd-4692-8722-e6fa947366d1",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ea665-dc8b-45b8-be6f-b9cc3c5f62b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transformation pipeline.\n",
    "# The input to the laplacian pyramid must be divisible by 2 for the number of high-\n",
    "# frequency levels in the pyramid.\n",
    "laplace_pyramid_divisible_by_shape = 2 ** 3\n",
    "\n",
    "pre_process_pipeline = monai.transforms.Compose(\n",
    "    [\n",
    "        monai.transforms.CropForegroundd([\"t1w\", \"mask\"], source_key=\"mask\", margin=3),\n",
    "        monai.transforms.DivisiblePadd(\n",
    "            [\"t1w\", \"mask\"], laplace_pyramid_divisible_by_shape\n",
    "        ),\n",
    "        monai.transforms.ToTensord(\"t1w\", dtype=torch.float),\n",
    "        monai.transforms.ToTensord(\"mask\", dtype=torch.float),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03506296-ec92-4083-93c1-83308f181847",
   "metadata": {},
   "source": [
    "### Load and Pre-Process HCP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac5a22-bd1a-41e3-ac99-3d4cdb1ee14c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find data directories for each subject.\n",
    "hcp_subj_dirs: dict = dict()\n",
    "\n",
    "possible_ids = [\n",
    "    \"sub-397154\",\n",
    "    \"sub-224022\",\n",
    "    \"sub-140117\",\n",
    "    \"sub-751348\",\n",
    "    \"sub-894774\",\n",
    "    \"sub-156637\",\n",
    "    \"sub-227432\",\n",
    "    \"sub-303624\",\n",
    "    \"sub-185947\",\n",
    "    \"sub-810439\",\n",
    "    \"sub-753251\",\n",
    "    \"sub-644246\",\n",
    "    \"sub-141422\",\n",
    "    \"sub-135528\",\n",
    "    \"sub-103010\",\n",
    "    \"sub-700634\",\n",
    "]\n",
    "\n",
    "## Sub-set the chosen participants for dev and debugging!\n",
    "selected_ids = random.sample(possible_ids, params.hcp.num_subjects)\n",
    "if params.hcp.num_subjects < len(possible_ids):\n",
    "    warnings.warn(\n",
    "        \"WARNING: Sub-selecting participants for dev and debugging. \"\n",
    "        + f\"Subj IDs selected: {selected_ids}\"\n",
    "    )\n",
    "# ### A nested warning! For debugging only.\n",
    "# warnings.warn(\"WARNING: Mixing training and testing subjects\")\n",
    "# selected_ids.append(selected_ids[0])\n",
    "# ###\n",
    "##\n",
    "\n",
    "selected_ids = natsorted(selected_ids)\n",
    "\n",
    "for subj_id in selected_ids:\n",
    "    hcp_subj_dirs[subj_id] = hcp_processed_data_dir / f\"{subj_id}\"\n",
    "    assert hcp_subj_dirs[subj_id].exists()\n",
    "ppr(hcp_subj_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f30dbd-f4d2-4144-9ba1-6dbf9097b441",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data loading and processing loop.\n",
    "hcp_subj_data = list()\n",
    "# Data reader object for NIFTI files.\n",
    "nib_reader = monai.data.NibabelReader(as_closest_canonical=True)\n",
    "\n",
    "# Directory prefixes for each image to be read.\n",
    "t1w_file_prefix = \"anat\"\n",
    "mask_file_prefix = \"mask\"\n",
    "\n",
    "for subj_id, subj_dir in hcp_subj_dirs.items():\n",
    "    subj_data = dict()\n",
    "    subj_data[\"subj_id\"] = subj_id\n",
    "\n",
    "    # Load the T1s\n",
    "    img_dir = subj_dir / t1w_file_prefix\n",
    "    img_filename = list(img_dir.glob(f\"{subj_id}*T1w*.nii.gz\"))\n",
    "    # Make sure the glob pattern only matches one file.\n",
    "    assert len(img_filename) == 1\n",
    "    img_filename = img_filename[0]\n",
    "    nib_img = nib_reader.read(img_filename)\n",
    "    img, metadata = nib_reader.get_data(nib_img)\n",
    "    # Add channel dimension if not found.\n",
    "    if len(img.shape) == 3:\n",
    "        img = img[\n",
    "            None,\n",
    "        ]\n",
    "    subj_data[\"t1w\"] = img\n",
    "    # The default metadata key name for monai.\n",
    "    subj_data[\"t1w_meta_dict\"] = metadata\n",
    "\n",
    "    # Load masks\n",
    "    img_dir = subj_dir / mask_file_prefix\n",
    "    img_filename = list(img_dir.glob(f\"{subj_id}*mask*.nii.gz\"))\n",
    "    # Make sure the glob pattern only matches one file.\n",
    "    assert len(img_filename) == 1\n",
    "    img_filename = img_filename[0]\n",
    "    nib_img = nib_reader.read(img_filename)\n",
    "    img, metadata = nib_reader.get_data(nib_img)\n",
    "    if len(img.shape) == 3:\n",
    "        img = img[\n",
    "            None,\n",
    "        ]\n",
    "    subj_data[\"mask\"] = img\n",
    "    # The default metadata key name for monai.\n",
    "    subj_data[\"mask_meta_dict\"] = metadata\n",
    "\n",
    "    # Pre-process subject vols.\n",
    "    subj_data = pre_process_pipeline(subj_data)\n",
    "\n",
    "    # Perform scaling of input data?\n",
    "    if params.data_scale_range is not None:\n",
    "        scaler = pitn.data.norm.DTIMinMaxScaler(\n",
    "            params.data_scale_range[0],\n",
    "            params.data_scale_range[1],\n",
    "            quantile_low=params.clamp_percentiles[0] / 100,\n",
    "            quantile_high=params.clamp_percentiles[1] / 100,\n",
    "            dim=(1, 2, 3),\n",
    "            channel_size=params.num_channels,\n",
    "            clip=True,\n",
    "        )\n",
    "        scaled = scaler.scale(subj_data[\"t1w\"] * subj_data[\"mask\"], stateful=True)\n",
    "        subj_data[\"t1w\"] = scaled * subj_data[\"mask\"]\n",
    "        subj_data[\"scaler\"] = scaler\n",
    "\n",
    "    hcp_subj_data.append(subj_data)\n",
    "\n",
    "# Create dataset with all HCP subjects included.\n",
    "hcp_subj_dataset = monai.data.Dataset(hcp_subj_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f3d4d9-333b-464e-be59-485d8de80582",
   "metadata": {},
   "source": [
    "### Load & Pre-Process Clinical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4cda4b-7289-4b78-bb67-88e2c3d3d67d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find data directories for each subject.\n",
    "clinic_subj_dirs: dict = dict()\n",
    "\n",
    "possible_ids = [\n",
    "    \"sub-OAS30188_MR_d3844\",\n",
    "    \"sub-OAS30375_MR_d5792\",\n",
    "    \"sub-OAS30558_MR_d2148\",\n",
    "    \"sub-OAS30643_MR_d0280\",\n",
    "    \"sub-OAS30685_MR_d0032\",\n",
    "    \"sub-OAS30762_MR_d0043\",\n",
    "    \"sub-OAS30770_MR_d1201\",\n",
    "    \"sub-OAS30944_MR_d0089\",\n",
    "    \"sub-OAS31018_MR_d0041\",\n",
    "    \"sub-OAS31157_MR_d4924\",\n",
    "]\n",
    "\n",
    "## Sub-set the chosen participants for dev and debugging!\n",
    "selected_ids = random.sample(possible_ids, params.clinic.num_subjects)\n",
    "if params.clinic.num_subjects < len(possible_ids):\n",
    "    warnings.warn(\n",
    "        \"WARNING: Sub-selecting participants for dev and debugging. \"\n",
    "        + f\"Subj IDs selected: {selected_ids}\"\n",
    "    )\n",
    "# ### A nested warning! For debugging only.\n",
    "# warnings.warn(\"WARNING: Mixing training and testing subjects\")\n",
    "# selected_ids.append(selected_ids[0])\n",
    "# ###\n",
    "##\n",
    "\n",
    "selected_ids = natsorted(selected_ids)\n",
    "\n",
    "for subj_id in selected_ids:\n",
    "    clinic_subj_dirs[subj_id] = clinic_processed_data_dir / f\"{subj_id}\"\n",
    "    assert clinic_subj_dirs[subj_id].exists()\n",
    "ppr(clinic_subj_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed602676-361b-49cb-af88-c7976f653e13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data loading and processing loop.\n",
    "clinic_subj_data = list()\n",
    "# Data reader object for NIFTI files.\n",
    "nib_reader = monai.data.NibabelReader(as_closest_canonical=True)\n",
    "\n",
    "# Directory prefixes for each image to be read.\n",
    "t1w_file_prefix = \"anat\"\n",
    "mask_file_prefix = \"mask\"\n",
    "\n",
    "for subj_id, subj_dir in clinic_subj_dirs.items():\n",
    "    subj_data = dict()\n",
    "    subj_data[\"subj_id\"] = subj_id\n",
    "\n",
    "    # Load the T1s\n",
    "    img_dir = subj_dir / t1w_file_prefix\n",
    "    img_filename = list(img_dir.glob(f\"{subj_id}*T1w*.nii.gz\"))\n",
    "    # Make sure the glob pattern only matches one file.\n",
    "    assert len(img_filename) == 1\n",
    "    img_filename = img_filename[0]\n",
    "    nib_img = nib_reader.read(img_filename)\n",
    "    img, metadata = nib_reader.get_data(nib_img)\n",
    "    # Add channel dimension if not found.\n",
    "    if len(img.shape) == 3:\n",
    "        img = img[\n",
    "            None,\n",
    "        ]\n",
    "    subj_data[\"t1w\"] = img\n",
    "    # The default metadata key name for monai.\n",
    "    subj_data[\"t1w_meta_dict\"] = metadata\n",
    "\n",
    "    # Load masks\n",
    "    img_dir = subj_dir / mask_file_prefix\n",
    "    img_filename = list(img_dir.glob(f\"{subj_id}*mask*.nii.gz\"))\n",
    "    # Make sure the glob pattern only matches one file.\n",
    "    assert len(img_filename) == 1\n",
    "    img_filename = img_filename[0]\n",
    "    nib_img = nib_reader.read(img_filename)\n",
    "    img, metadata = nib_reader.get_data(nib_img)\n",
    "    if len(img.shape) == 3:\n",
    "        img = img[\n",
    "            None,\n",
    "        ]\n",
    "    subj_data[\"mask\"] = img\n",
    "    # The default metadata key name for monai.\n",
    "    subj_data[\"mask_meta_dict\"] = metadata\n",
    "\n",
    "    # Pre-process subject vols.\n",
    "    subj_data = pre_process_pipeline(subj_data)\n",
    "\n",
    "    # Perform scaling of input data?\n",
    "    if params.data_scale_range is not None:\n",
    "        scaler = pitn.data.norm.DTIMinMaxScaler(\n",
    "            params.data_scale_range[0],\n",
    "            params.data_scale_range[1],\n",
    "            quantile_low=params.clamp_percentiles[0] / 100,\n",
    "            quantile_high=params.clamp_percentiles[1] / 100,\n",
    "            dim=(1, 2, 3),\n",
    "            channel_size=params.num_channels,\n",
    "            clip=True,\n",
    "        )\n",
    "        scaled = scaler.scale(subj_data[\"t1w\"] * subj_data[\"mask\"], stateful=True)\n",
    "        subj_data[\"t1w\"] = scaled * subj_data[\"mask\"]\n",
    "        subj_data[\"scaler\"] = scaler\n",
    "\n",
    "    clinic_subj_data.append(subj_data)\n",
    "\n",
    "# Create dataset with all \"clinical quality\" subjects included.\n",
    "clinic_subj_dataset = monai.data.Dataset(clinic_subj_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce8c4de-3cde-44c9-8e4e-8f6b76e5b8ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b38f7db-45b0-42dd-9360-32dca51d201f",
   "metadata": {},
   "source": [
    "### T1 Histograms of Intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cf3d31-aa0b-4157-98fa-b2f12a50fdf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HCP\n",
    "num_hcp_subjs = len(hcp_subj_dataset)\n",
    "nrows = np.ceil(np.sqrt(num_hcp_subjs)).astype(int)\n",
    "ncols = nrows\n",
    "\n",
    "fig = plt.figure(dpi=120, figsize=(7, 5))\n",
    "grid = mpl.gridspec.GridSpec(nrows, ncols, figure=fig, wspace=0.1, hspace=0.1)\n",
    "\n",
    "xlim = [np.inf, -np.inf]\n",
    "ylim = [np.inf, -np.inf]\n",
    "grid_cell_used = collections.defaultdict(lambda: False)\n",
    "\n",
    "for dataset, (i, j) in zip(\n",
    "    hcp_subj_dataset, itertools.product(range(nrows), range(ncols))\n",
    "):\n",
    "\n",
    "    ax = fig.add_subplot(grid[i, j])\n",
    "\n",
    "    ax.hist(dataset[\"t1w\"][dataset[\"mask\"].bool()].flatten().cpu().numpy(), bins=1000)\n",
    "    xlim[0] = min(xlim[0], ax.get_xlim()[0])\n",
    "    xlim[1] = max(xlim[1], ax.get_xlim()[1])\n",
    "    ylim[0] = min(ylim[0], ax.get_ylim()[0])\n",
    "    ylim[1] = max(ylim[1], ax.get_ylim()[1])\n",
    "    grid_cell_used[i, j] = True\n",
    "\n",
    "xlim[1] += 10\n",
    "ylim[1] += 10\n",
    "xticks = np.round(\n",
    "    np.arange(xlim[0], xlim[1], round(xlim[1] // 4, -len(str(round(xlim[1] // 4))) + 1))\n",
    ").astype(int)\n",
    "yticks = np.round(\n",
    "    np.arange(ylim[0], ylim[1], round(ylim[1] // 4, -len(str(round(ylim[1] // 4))) + 1))\n",
    ").astype(int)\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax_spec = ax.get_subplotspec()\n",
    "    i = list(ax_spec.rowspan)[0]\n",
    "    j = list(ax_spec.colspan)[0]\n",
    "\n",
    "    if not grid_cell_used[i + 1, j]:\n",
    "        ax.set_xticks(xticks)\n",
    "        ax.set_xticklabels(xticks, fontdict={\"fontsize\": \"x-small\"}, rotation=30)\n",
    "    else:\n",
    "        ax.set_xticks([])\n",
    "    if not grid_cell_used[i, j - 1]:\n",
    "        ax.set_yticks(yticks)\n",
    "        ax.set_yticklabels(yticks, fontdict={\"fontsize\": \"x-small\"})\n",
    "    else:\n",
    "        ax.set_yticks([])\n",
    "\n",
    "fig.align_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61fb241-59ea-438a-ac11-e6f847a5f882",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clinic\n",
    "num_clinic_subjs = len(clinic_subj_dataset)\n",
    "nrows = np.ceil(np.sqrt(num_clinic_subjs)).astype(int)\n",
    "ncols = nrows\n",
    "\n",
    "fig = plt.figure(dpi=120, figsize=(7, 5))\n",
    "grid = mpl.gridspec.GridSpec(nrows, ncols, figure=fig, wspace=0.1, hspace=0.1)\n",
    "\n",
    "xlim = [np.inf, -np.inf]\n",
    "ylim = [np.inf, -np.inf]\n",
    "grid_cell_used = collections.defaultdict(lambda: False)\n",
    "\n",
    "for dataset, (i, j) in zip(\n",
    "    clinic_subj_dataset, itertools.product(range(nrows), range(ncols))\n",
    "):\n",
    "\n",
    "    ax = fig.add_subplot(grid[i, j])\n",
    "    # ax.hist(dataset[\"t1w\"][dataset[\"mask\"].bool()].flatten().cpu().numpy(), bins=30)\n",
    "    # sns.kdeplot(dataset[\"t1w\"][dataset[\"mask\"].bool()].flatten().cpu().numpy(), ax=ax)\n",
    "    sns.histplot(dataset[\"t1w\"][dataset[\"mask\"].bool()].flatten().cpu().numpy(), ax=ax)\n",
    "    xlim[0] = min(xlim[0], ax.get_xlim()[0])\n",
    "    xlim[1] = max(xlim[1], ax.get_xlim()[1])\n",
    "    ylim[0] = min(ylim[0], ax.get_ylim()[0])\n",
    "    ylim[1] = max(ylim[1], ax.get_ylim()[1])\n",
    "    grid_cell_used[i, j] = True\n",
    "\n",
    "xlim[1] += 10\n",
    "ylim[1] += 10\n",
    "xticks = np.round(\n",
    "    np.arange(xlim[0], xlim[1], round(xlim[1] // 4, -len(str(round(xlim[1] // 4))) + 1))\n",
    ").astype(int)\n",
    "yticks = np.round(\n",
    "    np.arange(ylim[0], ylim[1], round(ylim[1] // 4, -len(str(round(ylim[1] // 4))) + 1))\n",
    ").astype(int)\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax_spec = ax.get_subplotspec()\n",
    "    i = list(ax_spec.rowspan)[0]\n",
    "    j = list(ax_spec.colspan)[0]\n",
    "\n",
    "    if not grid_cell_used[i + 1, j]:\n",
    "        ax.set_xticks(xticks)\n",
    "        ax.set_xticklabels(xticks, fontdict={\"fontsize\": \"x-small\"}, rotation=30)\n",
    "    else:\n",
    "        ax.set_xticks([])\n",
    "    if not grid_cell_used[i, j - 1]:\n",
    "        ax.set_yticks(yticks)\n",
    "        ax.set_yticklabels(yticks, fontdict={\"fontsize\": \"x-small\"})\n",
    "    else:\n",
    "        ax.set_yticks([])\n",
    "        ax.set_ylabel(\"\")\n",
    "\n",
    "fig.align_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42f7854-8716-4f96-8f9e-e13e278f4795",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d = clinic_subj_dataset[-2]\n",
    "t1 = d[\"t1w\"][0]\n",
    "mask = d[\"mask\"][0]\n",
    "\n",
    "intensity_range = (0, 20)\n",
    "filtered_t1 = torch.where(\n",
    "    (t1 >= intensity_range[0]) & (t1 <= intensity_range[1]) & mask.bool(),\n",
    "    t1.double(),\n",
    "    0.0,\n",
    ")\n",
    "dipy.viz.regtools.plot_slices(filtered_t1.cpu().numpy()).set_dpi(120)\n",
    "plt.suptitle(f\"Range [{intensity_range[0]}, {intensity_range[1]}]\")\n",
    "plt.show()\n",
    "\n",
    "intensity_range = (70, 90)\n",
    "filtered_t1 = torch.where(\n",
    "    (t1 >= intensity_range[0]) & (t1 <= intensity_range[1]) & mask.bool(),\n",
    "    t1.double(),\n",
    "    0.0,\n",
    ")\n",
    "dipy.viz.regtools.plot_slices(filtered_t1.cpu().numpy()).set_dpi(120)\n",
    "plt.suptitle(f\"Range [{intensity_range[0]}, {intensity_range[1]}]\")\n",
    "plt.show()\n",
    "\n",
    "intensity_range = (100, 112)\n",
    "filtered_t1 = torch.where(\n",
    "    (t1 >= intensity_range[0]) & (t1 <= intensity_range[1]) & mask.bool(),\n",
    "    t1.double(),\n",
    "    0.0,\n",
    ")\n",
    "dipy.viz.regtools.plot_slices(filtered_t1.cpu().numpy()).set_dpi(120)\n",
    "plt.suptitle(f\"Range [{intensity_range[0]}, {intensity_range[1]}]\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958410bf-5d28-43ff-9546-6003a236cf37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2339855-76a1-4de2-ba85-7e07f5b79e1f",
   "metadata": {},
   "source": [
    "### Peak Finding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b192f05-ba22-4065-82ce-39eeba324e8c",
   "metadata": {},
   "source": [
    "#### HCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bbc33a-939d-404a-a91b-e11e638a2320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Peak width maximization without overlap.\n",
    "def peak_contour_intersection_loss(\n",
    "    rel_height, peaks, signal, lambda_height, lambda_non_curvedness\n",
    "):\n",
    "    try:\n",
    "        if np.isscalar(rel_height):\n",
    "            rel_height = np.repeat([rel_height], len(peaks))\n",
    "\n",
    "        peak_ranges = list()\n",
    "        for rel_h, peak in zip(rel_height, peaks):\n",
    "            _, _, l, r = scipy.signal.peak_widths(signal, np.asarray([peak]), rel_h)\n",
    "            peak_ranges.append([l, r])\n",
    "        peak_ranges = np.asarray(peak_ranges)\n",
    "\n",
    "        intersections = list()\n",
    "        non_curvedness = list()\n",
    "        for i_peak in range(len(peaks)):\n",
    "            for j_peak in range(i_peak + 1, len(peaks)):\n",
    "\n",
    "                p1_range = peak_ranges[i_peak]\n",
    "                p2_range = peak_ranges[j_peak]\n",
    "\n",
    "                # Ensure p1 is the first peak in order.\n",
    "                if p2_range[0] < p1_range[0]:\n",
    "                    p1_range, p2_range = p2_range, p1_range\n",
    "\n",
    "                p1_low, p1_high = p1_range\n",
    "                p2_low, p2_high = p2_range\n",
    "\n",
    "                intersect: float\n",
    "\n",
    "                if i_peak == j_peak:\n",
    "                    intersect = 0\n",
    "                else:\n",
    "                    # p2 starts within p1\n",
    "                    if p2_low < p1_high:\n",
    "                        # p2 is contained in p1\n",
    "                        if p2_high <= p1_high:\n",
    "                            intersect = p2_high - p2_low\n",
    "                        # intersection of p1 and p2 is not the entirety of p2.\n",
    "                        else:\n",
    "                            intersect = p1_high - p2_low\n",
    "                    # Edge case where the low values are equal.\n",
    "                    elif p1_low == p2_low:\n",
    "                        intersect = min((p1_high - p1_low), (p2_high - p2_low))\n",
    "                    else:\n",
    "                        intersect = 0\n",
    "\n",
    "                intersections.append(float(intersect))\n",
    "\n",
    "            # How far does the selected signal range differ from a curve, parameterized by a\n",
    "            # quadratic function?\n",
    "            p_range = np.round(peak_ranges[i_peak]).astype(int).flatten().tolist()\n",
    "            selected_signal = signal[p_range[0] : p_range[1]]\n",
    "            x = np.arange(len(selected_signal))\n",
    "            curve_fit = scipy.interpolate.interp1d(x, selected_signal, kind=\"quadratic\")\n",
    "            pred_curve = curve_fit(x)\n",
    "            curvedness_loss = np.linalg.norm(\n",
    "                selected_signal - pred_curve, ord=2\n",
    "            ) / np.linalg.norm(selected_signal - selected_signal.mean(), ord=2)\n",
    "\n",
    "            non_curvedness.append(curvedness_loss)\n",
    "\n",
    "        intersect_loss = 2 ** (np.asarray(intersections) / len(signal)).mean()\n",
    "        height_loss = (np.abs(1.0 - rel_height)).mean()\n",
    "        non_curve_loss = np.mean(non_curvedness)\n",
    "        # print(\"Intersect \", intersect_loss)\n",
    "        # print(\"Height \", lambda_height * height_loss)\n",
    "        # print(\"Curviture \", lambda_non_curvedness * non_curve_loss)\n",
    "        # print(\"-----\")\n",
    "    except Exception as e:\n",
    "        print(rel_height)\n",
    "        print(e)\n",
    "\n",
    "        raise e\n",
    "    return (\n",
    "        intersect_loss\n",
    "        + lambda_height * height_loss\n",
    "        + lambda_non_curvedness * non_curve_loss\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8313668a-3888-4304-bbc4-cf9f3a10f659",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "size_subsample = 50000\n",
    "t1_sample = list()\n",
    "for d in hcp_subj_dataset:\n",
    "    t1 = d[\"t1w\"][0]\n",
    "    mask = d[\"mask\"][0]\n",
    "    masked_t1 = t1[mask.bool()].flatten().cpu()\n",
    "    # Randomly sample (without replacement) voxels of the T1 image. The KDE estimation does not\n",
    "    # scale well with 1 million + samples.\n",
    "    sample_idx = torch.randperm(len(masked_t1))\n",
    "\n",
    "    masked_t1 = masked_t1[sample_idx[:size_subsample]].numpy()\n",
    "    t1_sample.append(masked_t1)\n",
    "\n",
    "t1_sample = np.concatenate(t1_sample).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca130a45-b209-40ea-9a2e-3df6b7cc6648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(t1_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d191d0f-ecd3-438e-94e4-40322f45ea50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Starting KDE\")\n",
    "# Scott's rule for KDE bandwidth selection. From\n",
    "# <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html>\n",
    "d = 1\n",
    "n = len(t1_sample)\n",
    "bandwidth = n ** (-1.0 / (d + 4))\n",
    "# We want much more smoothness than Scott's gives by default.\n",
    "bandwidth = bandwidth * 10\n",
    "kde = sklearn.neighbors.KernelDensity(bandwidth=bandwidth, atol=1e-8, rtol=1e-5).fit(\n",
    "    t1_sample.reshape(-1, 1)\n",
    ")\n",
    "print(\"Finished KDE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a9e6cf-a504-40b7-9fe2-1857f4aa33b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "intensities = np.linspace(t1_sample.min(), t1_sample.max(), 10000)\n",
    "\n",
    "# Multiply by step size (size of integration) to scale densities such that the integral\n",
    "# over all given intensities is 1.0.\n",
    "integration_step = intensities[1] - intensities[0]\n",
    "log_like = kde.score_samples(intensities.reshape(-1, 1))\n",
    "likelihoods = np.exp(log_like) * integration_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95fe07c-2905-4cbe-b963-734c0be5071e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_peaks = 2\n",
    "plt.figure(dpi=120, figsize=(8, 4))\n",
    "peaks, properties = scipy.signal.find_peaks(likelihoods, rel_height=0.3)\n",
    "prominences = scipy.signal.peak_prominences(likelihoods, peaks)\n",
    "peaks = peaks[np.argsort(prominences[0])][-num_peaks:]\n",
    "# Ensure the peaks are in order of intensity window.\n",
    "peaks = np.sort(peaks)\n",
    "prominences = scipy.signal.peak_prominences(likelihoods, peaks)\n",
    "_, contour_height, peak_left, peak_right = scipy.signal.peak_widths(\n",
    "    likelihoods, peaks, prominence_data=prominences, rel_height=0.5\n",
    ")\n",
    "peak_left = (\n",
    "    peak_left / len(intensities) * (intensities.max() - intensities.min())\n",
    ") + intensities.min()\n",
    "peak_right = (\n",
    "    peak_right / len(intensities) * (intensities.max() - intensities.min())\n",
    ") + intensities.min()\n",
    "\n",
    "plt.plot(intensities, likelihoods)\n",
    "plt.vlines(intensities[peaks], 0, likelihoods.max(), color=\"gray\", alpha=0.5)\n",
    "plt.plot(\n",
    "    np.stack([peak_left, peak_right]),\n",
    "    np.stack([contour_height, contour_height]),\n",
    "    color=\"red\",\n",
    ")\n",
    "plt.xlabel(\"T1w Intensity\")\n",
    "plt.ylabel(\"KDE Likelihood Estimation\")\n",
    "plt.title(\"Intensity Peaks & Widths in HCP Subject Data\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67152198-60b3-409b-bbce-0104ecf4cfb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lambda_height = 0.001\n",
    "# lambda_non_curvedness = 1e11\n",
    "# result = scipy.optimize.minimize(\n",
    "#     functools.partial(\n",
    "#         peak_contour_intersection_loss,\n",
    "#         peaks=peaks,\n",
    "#         signal=likelihoods,\n",
    "#         lambda_height=lambda_height,\n",
    "#         lambda_non_curvedness=lambda_non_curvedness,\n",
    "#     ),\n",
    "#     [0.9, 0.9],\n",
    "#     bounds=((0.1, 1.0), (0.1, 1.0)),\n",
    "#     # bounds=scipy.optimize.Bounds([0.1] * 2, [1.0] * 2, keep_feasible=True),\n",
    "#     # method=\"Powell\",\n",
    "#     # options={\"maxiter\": 100000, \"maxfun\": 100000, \"eps\": 1e-7, \"maxls\": 100, 'gtol':1e-10},\n",
    "# )\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66f9564-59fd-47fe-a1cc-94ffdc124689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The optimization sucks, just set them by hand.\n",
    "result = dict()\n",
    "result[\"x\"] = np.asarray([0.25, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c8e3fd-7372-4c94-b556-3851b9774705",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=120, figsize=(8, 4))\n",
    "\n",
    "# Save the peak ranges for later.\n",
    "peak_ranges = dict()\n",
    "for i_peak, rel_h in enumerate(result[\"x\"]):\n",
    "\n",
    "    p = np.asarray(\n",
    "        [\n",
    "            peaks[i_peak],\n",
    "        ]\n",
    "    )\n",
    "    rel_h = np.asarray(\n",
    "        [\n",
    "            rel_h,\n",
    "        ]\n",
    "    )\n",
    "    prominences = scipy.signal.peak_prominences(likelihoods, p)\n",
    "    _, contour_height, peak_left, peak_right = scipy.signal.peak_widths(\n",
    "        likelihoods, p, rel_height=rel_h\n",
    "    )\n",
    "\n",
    "    peak_left = (\n",
    "        peak_left / len(intensities) * (intensities.max() - intensities.min())\n",
    "    ) + intensities.min()\n",
    "    peak_right = (\n",
    "        peak_right / len(intensities) * (intensities.max() - intensities.min())\n",
    "    ) + intensities.min()\n",
    "    plt.plot(\n",
    "        np.stack([peak_left, peak_right]),\n",
    "        np.stack([contour_height, contour_height]),\n",
    "        color=\"red\",\n",
    "    )\n",
    "\n",
    "    peak_ranges[i_peak] = {\n",
    "        \"range\": np.asarray(\n",
    "            [\n",
    "                [float(peak_left), float(peak_right)],\n",
    "                [float(contour_height), float(contour_height)],\n",
    "            ]\n",
    "        ).T,\n",
    "        \"peak\": float(p),\n",
    "    }\n",
    "\n",
    "\n",
    "plt.plot(intensities, likelihoods)\n",
    "plt.vlines(intensities[peaks], 0, likelihoods.max(), color=\"gray\", alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"T1w Intensity\")\n",
    "plt.ylabel(\"KDE Likelihood Estimation\")\n",
    "plt.title(\"Intensity Peaks & Widths in HCP Subject Data\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c50fc0-91d8-4047-97c5-19e2518db526",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ppr(peak_ranges)\n",
    "\n",
    "d = hcp_subj_dataset[-2]\n",
    "t1 = d[\"t1w\"][0]\n",
    "mask = d[\"mask\"][0]\n",
    "\n",
    "for p in peak_ranges.values():\n",
    "\n",
    "    intensity_range = tuple(p[\"range\"][:, 0])\n",
    "    filtered_t1 = torch.where(\n",
    "        (t1 >= intensity_range[0]) & (t1 <= intensity_range[1]) & mask.bool(),\n",
    "        t1.double(),\n",
    "        0.0,\n",
    "    )\n",
    "    dipy.viz.regtools.plot_slices(filtered_t1.cpu().numpy()).set_dpi(120)\n",
    "    plt.suptitle(f\"Range [{intensity_range[0]}, {intensity_range[1]}]\")\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81f95ae-966f-41a8-9ac8-435bb78063b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Align the zero points, the gray matter peak, the white matter peak, and the max points.\n",
    "peak_intensities = np.unique(\n",
    "    np.sort(\n",
    "        np.asarray(\n",
    "            [\n",
    "                intensities.min(),\n",
    "                intensities.max(),\n",
    "                intensities[np.where(likelihoods == likelihoods.min())][0].item(),\n",
    "                intensities[np.where(likelihoods == likelihoods.max())][0].item(),\n",
    "                *intensities[peaks],\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "peak_likelihoods = likelihoods[\n",
    "    np.isin(intensities, peak_intensities, assume_unique=True)\n",
    "]\n",
    "hcp_peaks = np.stack([peak_intensities, peak_likelihoods], axis=-1)\n",
    "# hcp_peaks = np.stack([hcp_peaks, likelihoods[np.where(intensities ]], axis=-1)\n",
    "ppr(hcp_peaks)\n",
    "ppr(hcp_peaks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6bc837-0633-46d9-be46-0aa4d7477b58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hcp_hist = np.stack([intensities, likelihoods], axis=-1)\n",
    "hcp_kde = kde\n",
    "hcp_samples = t1_sample\n",
    "hcp_peak_ranges = peak_ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60d8f5a-c535-4bc8-9bac-932fb195a62a",
   "metadata": {},
   "source": [
    "#### Clinical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a33029-a047-431a-9b54-9d56a5b9397b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "size_subsample = 50000\n",
    "t1_sample = list()\n",
    "for d in clinic_subj_dataset:\n",
    "    t1 = d[\"t1w\"][0]\n",
    "    mask = d[\"mask\"][0]\n",
    "    masked_t1 = t1[mask.bool()].flatten().cpu()\n",
    "    # Randomly sample (without replacement) voxels of the T1 image. The KDE estimation does not\n",
    "    # scale well with 1 million + samples.\n",
    "    sample_idx = torch.randperm(len(masked_t1))\n",
    "\n",
    "    masked_t1 = masked_t1[sample_idx[:size_subsample]].numpy()\n",
    "    t1_sample.append(masked_t1)\n",
    "\n",
    "t1_sample = np.concatenate(t1_sample).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d309b0-534e-4d60-ac2e-87fa4a9e3a84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Starting KDE\")\n",
    "# Scott's rule for KDE bandwidth selection. From\n",
    "# <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html>\n",
    "d = 1\n",
    "n = len(t1_sample)\n",
    "bandwidth = n ** (-1.0 / (d + 4))\n",
    "# We want much more smoothness than Scott's gives by default.\n",
    "bandwidth = bandwidth * 10\n",
    "kde = sklearn.neighbors.KernelDensity(bandwidth=bandwidth, atol=1e-8, rtol=1e-5).fit(\n",
    "    t1_sample.reshape(-1, 1)\n",
    ")\n",
    "print(\"Finished KDE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6099400e-bb0b-4d52-9141-0e9252b50ab2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "intensities = np.linspace(t1_sample.min(), t1_sample.max(), 10000)\n",
    "# Multiply by step size (size of integration) to scale densities such that the integral\n",
    "# over all given intensities is 1.0.\n",
    "integration_step = intensities[1] - intensities[0]\n",
    "log_like = kde.score_samples(intensities.reshape(-1, 1))\n",
    "likelihoods = np.exp(log_like) * integration_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcba926b-3a1a-412b-9cbd-8d42995264f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_peaks = 2\n",
    "plt.figure(dpi=120, figsize=(8, 4))\n",
    "peaks, properties = scipy.signal.find_peaks(likelihoods, rel_height=0.5)\n",
    "prominences = scipy.signal.peak_prominences(likelihoods, peaks)\n",
    "peaks = peaks[np.argsort(prominences[0])][-num_peaks:]\n",
    "# Ensure the peaks are in order of intensity window.\n",
    "peaks = np.sort(peaks)\n",
    "prominences = scipy.signal.peak_prominences(likelihoods, peaks)\n",
    "_, contour_height, peak_left, peak_right = scipy.signal.peak_widths(\n",
    "    likelihoods, peaks, prominence_data=prominences, rel_height=0.5\n",
    ")\n",
    "peak_left = (\n",
    "    peak_left / len(intensities) * (intensities.max() - intensities.min())\n",
    ") + intensities.min()\n",
    "peak_right = (\n",
    "    peak_right / len(intensities) * (intensities.max() - intensities.min())\n",
    ") + intensities.min()\n",
    "\n",
    "plt.plot(intensities, likelihoods)\n",
    "plt.vlines(intensities[peaks], 0, likelihoods.max(), color=\"gray\", alpha=0.5)\n",
    "plt.plot(\n",
    "    np.stack([peak_left, peak_right]),\n",
    "    np.stack([contour_height, contour_height]),\n",
    "    color=\"red\",\n",
    ")\n",
    "plt.xlabel(\"T1w Intensity\")\n",
    "plt.ylabel(\"KDE Likelihood Estimation\")\n",
    "plt.title(\"Intensity Peaks & Widths in Clinical Subject Data\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5987224f-2ebc-4c5f-bc79-1d577b17e7a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lambda_height = 0.001\n",
    "# lambda_non_curvedness = 1e11\n",
    "# result = scipy.optimize.minimize(\n",
    "#     functools.partial(\n",
    "#         peak_contour_intersection_loss,\n",
    "#         peaks=peaks,\n",
    "#         signal=likelihoods,\n",
    "#         lambda_height=lambda_height,\n",
    "#         lambda_non_curvedness=lambda_non_curvedness,\n",
    "#     ),\n",
    "#     [0.9, 0.9],\n",
    "#     bounds=((0.1, 1.0), (0.1, 1.0)),\n",
    "#     # bounds=scipy.optimize.Bounds([0.1] * 2, [1.0] * 2, keep_feasible=True),\n",
    "#     # method=\"Powell\",\n",
    "#     # options={\"maxiter\": 100000, \"maxfun\": 100000, \"eps\": 1e-7, \"maxls\": 100, 'gtol':1e-10},\n",
    "# )\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545ab74a-f1b5-4cc3-9467-cfb4a0cebe50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Just set the relative heights by hand, the optimization isn't tuned well.\n",
    "result = dict()\n",
    "result[\"x\"] = np.asarray([0.75, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260b553e-1bbd-492e-ae58-30342721698d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=120, figsize=(8, 4))\n",
    "\n",
    "# Save the peak ranges for later.\n",
    "peak_ranges = dict()\n",
    "for i_peak, rel_h in enumerate(result[\"x\"]):\n",
    "\n",
    "    p = np.asarray(\n",
    "        [\n",
    "            peaks[i_peak],\n",
    "        ]\n",
    "    )\n",
    "    rel_h = np.asarray(\n",
    "        [\n",
    "            rel_h,\n",
    "        ]\n",
    "    )\n",
    "    prominences = scipy.signal.peak_prominences(likelihoods, p)\n",
    "    _, contour_height, peak_left, peak_right = scipy.signal.peak_widths(\n",
    "        likelihoods, p, rel_height=rel_h\n",
    "    )\n",
    "\n",
    "    peak_left = (\n",
    "        peak_left / len(intensities) * (intensities.max() - intensities.min())\n",
    "    ) + intensities.min()\n",
    "    peak_right = (\n",
    "        peak_right / len(intensities) * (intensities.max() - intensities.min())\n",
    "    ) + intensities.min()\n",
    "    plt.plot(\n",
    "        np.stack([peak_left, peak_right]),\n",
    "        np.stack([contour_height, contour_height]),\n",
    "        color=\"red\",\n",
    "    )\n",
    "\n",
    "    peak_ranges[i_peak] = {\n",
    "        \"range\": np.asarray(\n",
    "            [\n",
    "                [float(peak_left), float(peak_right)],\n",
    "                [float(contour_height), float(contour_height)],\n",
    "            ]\n",
    "        ).T,\n",
    "        \"peak\": float(p),\n",
    "    }\n",
    "\n",
    "\n",
    "plt.plot(intensities, likelihoods)\n",
    "plt.vlines(intensities[peaks], 0, likelihoods.max(), color=\"gray\", alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"T1w Intensity\")\n",
    "plt.ylabel(\"KDE Likelihood Estimation\")\n",
    "plt.title(\"Intensity Peaks & Widths in Clinical Subject Data\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f6bb25-d67b-4836-8292-9a1e7d7688e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ppr(peak_ranges)\n",
    "\n",
    "d = clinic_subj_dataset[-2]\n",
    "t1 = d[\"t1w\"][0]\n",
    "mask = d[\"mask\"][0]\n",
    "\n",
    "for p in peak_ranges.values():\n",
    "\n",
    "    intensity_range = tuple(p[\"range\"][:, 0])\n",
    "    filtered_t1 = torch.where(\n",
    "        (t1 >= intensity_range[0]) & (t1 <= intensity_range[1]) & mask.bool(),\n",
    "        t1.double(),\n",
    "        0.0,\n",
    "    )\n",
    "    dipy.viz.regtools.plot_slices(filtered_t1.cpu().numpy()).set_dpi(120)\n",
    "    plt.suptitle(f\"Range [{intensity_range[0]}, {intensity_range[1]}]\")\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ac84d-8c3e-4872-ab24-4f8626dd722e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Align the zero points, the gray matter peak, the white matter peak, and the max points.\n",
    "peak_intensities = np.unique(\n",
    "    np.sort(\n",
    "        np.asarray(\n",
    "            [\n",
    "                intensities.min(),\n",
    "                intensities.max(),\n",
    "                intensities[np.where(likelihoods == likelihoods.min())][0].item(),\n",
    "                intensities[np.where(likelihoods == likelihoods.max())][0].item(),\n",
    "                *intensities[peaks],\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "peak_likelihoods = likelihoods[np.isin(intensities, peak_intensities)]\n",
    "clinic_peaks = np.stack([peak_intensities, peak_likelihoods], axis=-1)\n",
    "\n",
    "ppr(clinic_peaks)\n",
    "ppr(clinic_peaks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2487485e-ce0f-4b08-95f8-dc4693276e45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clinic_hist = np.stack([intensities, likelihoods], axis=-1)\n",
    "clinic_kde = kde\n",
    "clinic_samples = t1_sample\n",
    "clinic_peak_ranges = peak_ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b695f9-0ca6-420e-8326-6ced2a09a470",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Histogram Modeling and Interpolation\n",
    "\n",
    "**Ignore this section, models are insufficient.**\n",
    "\n",
    "Tried:\n",
    "- Fitting regression models\n",
    "- Fitting polynomial interpolation models\n",
    "- Fitting the histogram CDFs\n",
    "- Fitting Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ab2312-a561-43d6-b28b-8ae38984a1ae",
   "metadata": {},
   "source": [
    "### sklearn Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17170689-3d9d-4185-bb29-63dc3fc797dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Fit a cubic polynomial to these points by projecting into polynomial feature space,\n",
    "# # then using linear regression.\n",
    "\n",
    "# scaler = sklearn.preprocessing.MinMaxScaler(\n",
    "#     feature_range=(clinic_peaks[:, 0].min(), clinic_peaks[:, 0].max())\n",
    "# )\n",
    "# proj = sklearn.preprocessing.PolynomialFeatures(degree=2)\n",
    "# model = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "# X = hcp_peaks\n",
    "# Y = clinic_peaks\n",
    "# X_p = X.copy()\n",
    "# X_p[:, 0] = scaler.fit_transform(X_p[:, 0].reshape(-1, 1)).flatten()\n",
    "# X_p = proj.fit_transform(X_p)\n",
    "# model = model.fit(X_p, Y)\n",
    "# model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06be01e4-a1d9-4e84-8620-bec27c6ad30d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.figure(dpi=120, figsize=(9, 6))\n",
    "\n",
    "# plt.plot(hcp_peaks[:, 0], hcp_peaks[:, 1], label=\"HCP\", marker=\"o\", ls=\"-\")\n",
    "# plt.plot(clinic_peaks[:, 0], clinic_peaks[:, 1], label=\"Cinical\", marker=\"o\", ls=\"-\")\n",
    "\n",
    "# X = hcp_peaks\n",
    "# X_p = X.copy()\n",
    "# X_p[:, 0] = scaler.fit_transform(X_p[:, 0].reshape(-1, 1)).flatten()\n",
    "# plt.plot(\n",
    "#     X_p[:, 0],\n",
    "#     X_p[:, 1],\n",
    "#     label=\"HCP Scaled to Clinical\",\n",
    "#     marker=\".\",\n",
    "#     ls=\"--\",\n",
    "#     alpha=0.7,\n",
    "# )\n",
    "\n",
    "# X_p = proj.fit_transform(X_p)\n",
    "# adapt_pred = model.predict(X_p)\n",
    "# plt.plot(\n",
    "#     adapt_pred[:, 0],\n",
    "#     adapt_pred[:, 1],\n",
    "#     label=\"HCP adapted to Clinical\",\n",
    "#     marker=\".\",\n",
    "#     ls=\"--\",\n",
    "#     alpha=0.7,\n",
    "# )\n",
    "# plt.xlabel(\"T1w Intensity\")\n",
    "# plt.ylabel(\"Likelihood\")\n",
    "# plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82048e2-7ac7-4001-bb9c-104392e114c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(proj.get_feature_names())\n",
    "# print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5268d7aa-0939-4019-aa88-ec964e4806c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Compare entire distributions.\n",
    "# plt.figure(dpi=100, figsize=(7, 5))\n",
    "# plt.plot(clinic_hist[:, 0], clinic_hist[:, 1], label=\"Clinic Original\")\n",
    "# plt.vlines(\n",
    "#     clinic_peaks[:, 0],\n",
    "#     clinic_hist[:, 1].min(),\n",
    "#     clinic_hist[:, 1].max(),\n",
    "#     ls=\"--\",\n",
    "#     color=\"black\",\n",
    "# )\n",
    "# # plt.plot(hcp_hist[:, 0], hcp_hist[:, 1], label='HCP Original')\n",
    "\n",
    "# X = hcp_hist\n",
    "# X_p = X.copy()\n",
    "# X_p[:, 0] = scaler.fit_transform(X_p[:, 0].reshape(-1, 1)).flatten()\n",
    "# plt.plot(\n",
    "#     X_p[:, 0],\n",
    "#     X_p[:, 1],\n",
    "#     label=\"HCP Scaled to Clinical\",\n",
    "#     alpha=0.7,\n",
    "# )\n",
    "\n",
    "# X_p = proj.fit_transform(X_p)\n",
    "# clinified = model.predict(X_p)\n",
    "# plt.plot(clinified[:, 0], clinified[:, 1], label=\"Clinifed HCP\")\n",
    "\n",
    "# plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aba22e-b3f6-414e-a795-710a7d56dabe",
   "metadata": {},
   "source": [
    "### scipy Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58101c95-0c58-44ca-b7e8-cd4792b04342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Perform 1D interpolation for each axis independently.\n",
    "\n",
    "# X_intens = hcp_peaks[:, 0]\n",
    "# Y_intens = clinic_peaks[:, 0]\n",
    "# f_intens = scipy.interpolate.interp1d(X_intens, Y_intens, kind=\"linear\")\n",
    "\n",
    "# X_likelihood = hcp_peaks[:, 1]\n",
    "# Y_likelihood = clinic_peaks[:, 1]\n",
    "# f_likelihood = scipy.interpolate.interp1d(X_likelihood, Y_likelihood, kind=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00918e67-b9b5-4ad0-8ef4-717e7908c9d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Compare entire distributions.\n",
    "# plt.figure(dpi=100, figsize=(7, 5))\n",
    "# plt.plot(clinic_hist[:, 0], clinic_hist[:, 1], label=\"Clinic Original\")\n",
    "# plt.vlines(\n",
    "#     clinic_peaks[:, 0],\n",
    "#     clinic_hist[:, 1].min(),\n",
    "#     clinic_hist[:, 1].max(),\n",
    "#     ls=\"--\",\n",
    "#     color=\"black\",\n",
    "# )\n",
    "# # plt.plot(hcp_hist[:, 0], hcp_hist[:, 1], label='HCP Original')\n",
    "\n",
    "# plt.plot(\n",
    "#     f_intens(hcp_hist[:, 0]),\n",
    "#     f_likelihood(hcp_hist[:, 1]),\n",
    "#     label=\"HCP Interpolated to Clinic\",\n",
    "# )\n",
    "# plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa53c019-f178-4824-a35f-1581d227266c",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Model Fitting & Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098268ef-538c-4ef6-b507-eaf8997dbb9e",
   "metadata": {},
   "source": [
    "#### HCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745f691a-fa3d-4c1f-ac52-78dbe0b353d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Fit HCP data.\n",
    "# hcp_gmm = sklearn.mixture.BayesianGaussianMixture(\n",
    "#     n_components=50,\n",
    "#     weight_concentration_prior=1e-5,\n",
    "#     weight_concentration_prior_type=\"dirichlet_process\",\n",
    "#     n_init=2,\n",
    "#     # verbose=1,\n",
    "#     max_iter=500,\n",
    "#     tol=1e-2 / 2,\n",
    "# )\n",
    "# X = hcp_samples.reshape(-1, 1)[::50]\n",
    "# hcp_gmm = hcp_gmm.fit(X)\n",
    "# print(f\"Number of Components: {(hcp_gmm.weights_ >= 1e-5).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c5e355-79ca-4904-9b32-c2ee4c0275b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# intensities_to_predict = hcp_samples[::100]\n",
    "# plt.hist(intensities_to_predict, label=\"Original Data\", density=True, bins=100)\n",
    "# intensities_to_predict = np.sort(intensities_to_predict).reshape(-1, 1)\n",
    "# predict_log_likelihoods = hcp_gmm.score_samples(intensities_to_predict).flatten()\n",
    "# predict_likelihoods = np.exp(predict_log_likelihoods)\n",
    "\n",
    "# plt.plot(intensities_to_predict, predict_likelihoods, label=\"GMM Predicted\")\n",
    "# plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0612e0b3-07d9-4794-aa5d-4d6831666648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 1, dpi=100)\n",
    "# x, component_num = hcp_gmm.sample(100000)\n",
    "# x = x.flatten()\n",
    "\n",
    "# plt.hist(\n",
    "#     hcp_samples, label=\"Original Data\", bins=100, density=True, color=\"gray\", alpha=0.2\n",
    "# )\n",
    "# plot_data = pd.DataFrame(\n",
    "#     np.stack([x, component_num], axis=-1), columns=[\"intensity\", \"component\"]\n",
    "# )\n",
    "# sns.histplot(\n",
    "#     plot_data, x=\"intensity\", hue=\"component\", ax=ax, palette=\"tab10\", stat=\"density\"\n",
    "# );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d7252c-ab19-458b-a347-5a7efe09c3c8",
   "metadata": {},
   "source": [
    "#### Clinic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79732c49-3443-413e-b6bc-771e62e22ff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Fit clinic data.\n",
    "# clinic_gmm = sklearn.mixture.BayesianGaussianMixture(\n",
    "#     n_components=50,\n",
    "#     weight_concentration_prior=1e-5,\n",
    "#     weight_concentration_prior_type=\"dirichlet_process\",\n",
    "#     n_init=2,\n",
    "#     # verbose=1,\n",
    "#     max_iter=500,\n",
    "#     tol=1e-2 / 2,\n",
    "# )\n",
    "# X = clinic_samples.reshape(-1, 1)[::50]\n",
    "# clinic_gmm = clinic_gmm.fit(X)\n",
    "# print(f\"Number of Components: {(clinic_gmm.weights_ >= 1e-5).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac2d0f7-ec24-4a39-b55d-2e924ae06d42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# intensities_to_predict = clinic_samples[::10]\n",
    "# plt.hist(intensities_to_predict, label=\"Original Data\", density=True, bins=100)\n",
    "# intensities_to_predict = np.sort(intensities_to_predict).reshape(-1, 1)\n",
    "# predict_log_likelihoods = clinic_gmm.score_samples(intensities_to_predict).flatten()\n",
    "# predict_likelihoods = np.exp(predict_log_likelihoods)\n",
    "\n",
    "# plt.plot(intensities_to_predict, predict_likelihoods, label=\"GMM Predicted\")\n",
    "# plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ff3620-6157-4035-8ee0-4048dbc5cc72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 1, dpi=100)\n",
    "# x, component_num = clinic_gmm.sample(100000)\n",
    "# x = x.flatten()\n",
    "\n",
    "# plt.hist(\n",
    "#     clinic_samples,\n",
    "#     label=\"Original Data\",\n",
    "#     bins=100,\n",
    "#     density=True,\n",
    "#     color=\"gray\",\n",
    "#     alpha=0.2,\n",
    "# )\n",
    "# plot_data = pd.DataFrame(\n",
    "#     np.stack([x, component_num], axis=-1), columns=[\"intensity\", \"component\"]\n",
    "# )\n",
    "# sns.histplot(\n",
    "#     plot_data, x=\"intensity\", hue=\"component\", ax=ax, palette=\"tab10\", stat=\"density\"\n",
    "# );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e18997-95b8-4522-891e-b373d0302bb0",
   "metadata": {},
   "source": [
    "## T1w Intensity CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a540399-7c2a-46b3-94da-3e8ab4edf5a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hcp_cdf = np.zeros_like(hcp_hist)\n",
    "hcp_cdf[:, 0] = hcp_hist[:, 0]\n",
    "hcp_cdf[:, 1] = np.cumsum(hcp_hist[:, 1])\n",
    "hcp_cdf[:, 1] = hcp_cdf[:, 1] / hcp_cdf[:, 1].max()\n",
    "plt.plot(hcp_cdf[:, 0], hcp_cdf[:, 1], label=\"HCP\")\n",
    "\n",
    "hcp_peak_heights = hcp_cdf[:, 1][np.isin(hcp_cdf[:, 0], hcp_peaks[:, 0])]\n",
    "plt.plot(hcp_peaks[:, 0], hcp_peak_heights, \"o\", label=\"HCP\", color=\"C0\")\n",
    "\n",
    "clinic_cdf = np.zeros_like(clinic_hist)\n",
    "clinic_cdf[:, 0] = clinic_hist[:, 0]\n",
    "clinic_cdf[:, 1] = np.cumsum(clinic_hist[:, 1])\n",
    "clinic_cdf[:, 1] = clinic_cdf[:, 1] / clinic_cdf[:, 1].max()\n",
    "plt.plot(clinic_cdf[:, 0], clinic_cdf[:, 1], label=\"clinic\")\n",
    "clinic_peak_heights = clinic_cdf[:, 1][np.isin(clinic_cdf[:, 0], clinic_peaks[:, 0])]\n",
    "plt.plot(clinic_peaks[:, 0], clinic_peak_heights, \"o\", label=\"Clinic\", color=\"C1\")\n",
    "plt.title(\"CDFs of KDEs of HCP & Clinic T1 Histograms\")\n",
    "plt.xlabel(\"Intensity\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfd39dc-e76d-440b-8d0f-88ba04abea34",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Histogram Transformation\n",
    "\n",
    "Based on algorithms in:\n",
    "\n",
    "```L. G. Nyul, J. K. Udupa, and X. Zhang, New variants of a method of MRI scale standardization, IEEE Transactions on Medical Imaging, vol. 19, no. 2, pp. 143150, Feb. 2000, doi: 10.1109/42.836373.```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27df885f-f4bc-4a98-9c88-505615db5ba2",
   "metadata": {},
   "source": [
    "### Determine Landmarks in Source and Target Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cf77de-6213-4481-8721-349d9d2adab2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hcp_cdf = np.stack([hcp_hist[:, 0], np.cumsum(hcp_hist[:, 1])], axis=-1)\n",
    "clinic_cdf = np.stack([clinic_hist[:, 0], np.cumsum(clinic_hist[:, 1])], axis=-1)\n",
    "\n",
    "# Normalize to a sum of 1.0 to account for numerical error.\n",
    "hcp_cdf[:, 1] = hcp_cdf[:, 1] / hcp_cdf[:, 1].max()\n",
    "clinic_cdf[:, 1] = clinic_cdf[:, 1] / clinic_cdf[:, 1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb93483-9ebc-403a-984f-520e750350af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select histogram landmarks that mark a new linear mapping.\n",
    "landmarks = Box(default_box=True)\n",
    "params = Box(default_box=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fc2a1c-6512-4389-af3c-bd70f4c20686",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add min and max intensities as landmarks, take only quantiles to remove tails & noise.\n",
    "min_quantile = 0.00\n",
    "max_quantile = 0.99\n",
    "params.min_quantile = min_quantile\n",
    "params.max_quantile = max_quantile\n",
    "\n",
    "hcp_transform_hist = hcp_hist.copy()\n",
    "hcp_transform_hist[:, 1] = np.where(\n",
    "    (hcp_cdf[:, 1] >= min_quantile) & (hcp_cdf[:, 1] <= max_quantile),\n",
    "    hcp_hist[:, 1],\n",
    "    0.0,\n",
    ")\n",
    "\n",
    "# Renormalize to sum to 1.0.\n",
    "hcp_transform_hist[:, 1] = hcp_transform_hist[:, 1] / hcp_transform_hist[:, 1].sum()\n",
    "\n",
    "landmarks.source.min = hcp_transform_hist[:, 0].min()\n",
    "landmarks.source.max = hcp_transform_hist[:, 0].max()\n",
    "\n",
    "clinic_transform_hist = clinic_hist.copy()\n",
    "clinic_transform_hist[:, 1] = np.where(\n",
    "    (clinic_cdf[:, 1] >= min_quantile) & (clinic_cdf[:, 1] <= max_quantile),\n",
    "    clinic_hist[:, 1],\n",
    "    0.0,\n",
    ")\n",
    "\n",
    "# Renormalize to sum to 1.0.\n",
    "clinic_transform_hist[:, 1] = (\n",
    "    clinic_transform_hist[:, 1] / clinic_transform_hist[:, 1].sum()\n",
    ")\n",
    "\n",
    "landmarks.target.min = clinic_transform_hist[:, 0].min()\n",
    "landmarks.target.max = clinic_transform_hist[:, 0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf940e-612c-46c2-8f21-1678be449841",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select quantiles of the gray matter peak/curve as individual points to match.\n",
    "gm_quantiles = (0.25, 0.5, 0.75)\n",
    "params.gm_quantiles = gm_quantiles\n",
    "\n",
    "gm_clinic_peak = clinic_peak_ranges[0]\n",
    "gm_clinic_curve = np.take(\n",
    "    clinic_transform_hist,\n",
    "    np.where(\n",
    "        (clinic_transform_hist[:, 0] >= gm_clinic_peak[\"range\"][0, 0])\n",
    "        & (clinic_transform_hist[:, 0] <= gm_clinic_peak[\"range\"][1, 0]),\n",
    "    )[0],\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "# Create a CDF *only* for the GM curve, to find quantiles.\n",
    "gm_cdf = np.stack([gm_clinic_curve[:, 0], np.cumsum(gm_clinic_curve[:, 1])], axis=-1)\n",
    "gm_cdf[:, 1] = gm_cdf[:, 1] / gm_cdf[:, 1].max()\n",
    "gm_landmarks = list()\n",
    "for q in gm_quantiles:\n",
    "    intensity = gm_cdf[:, 0][np.argmin(np.abs(q - gm_cdf[:, 1]))]\n",
    "    gm_landmarks.append(intensity)\n",
    "landmarks.target.gm = gm_landmarks\n",
    "\n",
    "###### Do the same for HCP/source images.\n",
    "gm_hcp_peak = hcp_peak_ranges[0]\n",
    "gm_hcp_curve = np.take(\n",
    "    hcp_transform_hist,\n",
    "    np.where(\n",
    "        (hcp_transform_hist[:, 0] >= gm_hcp_peak[\"range\"][0, 0])\n",
    "        & (hcp_transform_hist[:, 0] <= gm_hcp_peak[\"range\"][1, 0]),\n",
    "    )[0],\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "# Create a CDF *only* for the GM curve, to find quantiles.\n",
    "gm_cdf = np.stack([gm_hcp_curve[:, 0], np.cumsum(gm_hcp_curve[:, 1])], axis=-1)\n",
    "gm_cdf[:, 1] = gm_cdf[:, 1] / gm_cdf[:, 1].max()\n",
    "gm_landmarks = list()\n",
    "for q in gm_quantiles:\n",
    "    intensity = gm_cdf[:, 0][np.argmin(np.abs(q - gm_cdf[:, 1]))]\n",
    "    gm_landmarks.append(intensity)\n",
    "landmarks.source.gm = gm_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ae35d2-5b80-410c-bce5-3774821444c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select quantiles of the white matter peak/curve as individual points to match.\n",
    "wm_quantiles = (0.25, 0.5, 0.75)\n",
    "params.wm_quantiles = wm_quantiles\n",
    "\n",
    "wm_clinic_peak = clinic_peak_ranges[1]\n",
    "wm_clinic_curve = np.take(\n",
    "    clinic_transform_hist,\n",
    "    np.where(\n",
    "        (clinic_transform_hist[:, 0] >= wm_clinic_peak[\"range\"][0, 0])\n",
    "        & (clinic_transform_hist[:, 0] <= wm_clinic_peak[\"range\"][1, 0]),\n",
    "    )[0],\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "# Create a CDF *only* for the WM curve, to find quantiles.\n",
    "wm_cdf = np.stack([wm_clinic_curve[:, 0], np.cumsum(wm_clinic_curve[:, 1])], axis=-1)\n",
    "wm_cdf[:, 1] = wm_cdf[:, 1] / wm_cdf[:, 1].max()\n",
    "wm_landmarks = list()\n",
    "for q in wm_quantiles:\n",
    "    # Choose the intensity that is the closest match to this region's quantile.\n",
    "    intensity = wm_cdf[:, 0][np.argmin(np.abs(q - wm_cdf[:, 1]))]\n",
    "    wm_landmarks.append(intensity)\n",
    "landmarks.target.wm = wm_landmarks\n",
    "\n",
    "########## Do the same for HCP/source images\n",
    "wm_hcp_peak = hcp_peak_ranges[1]\n",
    "wm_hcp_curve = np.take(\n",
    "    hcp_transform_hist,\n",
    "    np.where(\n",
    "        (hcp_transform_hist[:, 0] >= wm_hcp_peak[\"range\"][0, 0])\n",
    "        & (hcp_transform_hist[:, 0] <= wm_hcp_peak[\"range\"][1, 0]),\n",
    "    )[0],\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "# Create a CDF *only* for the WM curve, to find quantiles.\n",
    "wm_cdf = np.stack([wm_hcp_curve[:, 0], np.cumsum(wm_hcp_curve[:, 1])], axis=-1)\n",
    "wm_cdf[:, 1] = wm_cdf[:, 1] / wm_cdf[:, 1].max()\n",
    "wm_landmarks = list()\n",
    "for q in wm_quantiles:\n",
    "    # Choose the intensity that is the closest match to this region's quantile.\n",
    "    intensity = wm_cdf[:, 0][np.argmin(np.abs(q - wm_cdf[:, 1]))]\n",
    "    wm_landmarks.append(intensity)\n",
    "landmarks.source.wm = wm_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7f8525-54db-4441-abb2-62a0fdce3d43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ppr(landmarks)\n",
    "ppr(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e390960-a258-4b46-b400-de175b440ae5",
   "metadata": {},
   "source": [
    "### Interpolation of Histogram x-Axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1ab35e-1acb-4559-87a2-6a559e02d3ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create interpolation function to map all intensity values in the source to intensity\n",
    "# values in the taret.\n",
    "x = np.sort(\n",
    "    [\n",
    "        landmarks.source.min,\n",
    "        *landmarks.source.gm,\n",
    "        *landmarks.source.wm,\n",
    "        landmarks.source.max,\n",
    "    ]\n",
    ").flatten()\n",
    "y = np.sort(\n",
    "    [\n",
    "        landmarks.target.min,\n",
    "        *landmarks.target.gm,\n",
    "        *landmarks.target.wm,\n",
    "        landmarks.target.max,\n",
    "    ]\n",
    ").flatten()\n",
    "# Allow out-of-range interpolation as just clamping to the max and min.\n",
    "intensity_map = scipy.interpolate.interp1d(\n",
    "    x, y, kind=\"linear\", bounds_error=False, fill_value=(y.min(), y.max())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd8d61-6868-4adb-b1d6-edfc52540053",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the intensity mapping interpolation.\n",
    "plt.figure(dpi=100)\n",
    "source_intens = np.linspace(-1, hcp_hist[:, 0].max() * 1.1, 10000)\n",
    "plt.plot(source_intens, intensity_map(source_intens))\n",
    "plt.plot(x, y, \".\", color=\"black\")\n",
    "plt.xlabel(\"Original Intensity\")\n",
    "plt.ylabel(\"Mapped Intensity\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac66dd2-a52f-4cd4-8a0b-e9872078a8ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot HCP hist with transformed x-axis.\n",
    "plt.plot(hcp_hist[:, 0], hcp_hist[:, 1], label=\"original\")\n",
    "plt.vlines(x, 0, hcp_hist[:, 1].max() * 1.1, ls=\"--\", color=\"gray\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(intensity_map(hcp_hist[:, 0]), hcp_hist[:, 1], label=\"transformed\")\n",
    "plt.vlines(y, 0, hcp_hist[:, 1].max() * 1.1, ls=\"--\", color=\"gray\")\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9081ce20-cd77-4c2d-a726-d4847a64e61e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaled_hcp_hist = hcp_hist.copy()\n",
    "scaled_hcp_hist[:, 0] = intensity_map(scaled_hcp_hist[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e71d44-34ce-48ee-8556-ba51452280c9",
   "metadata": {},
   "source": [
    "### Create Final Image Voxel LUT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36955676-e9c9-478d-93d0-dbbb8c16080c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(hcp_hist[:, 0], np.cumsum(hcp_hist[:, 1]), label=\"original\")\n",
    "plt.plot(scaled_hcp_hist[:, 0], np.cumsum(scaled_hcp_hist[:, 1]), label=\"transformed\")\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8094dd-b9f6-4a52-b6fa-0ffdb7f410d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mappings on the CDF\n",
    "# Source Pixel intensity -> Source Cumulative Density\n",
    "source_cdf_mapper = scipy.interpolate.interp1d(\n",
    "    scaled_hcp_hist[:, 0],\n",
    "    np.cumsum(scaled_hcp_hist[:, 1]),\n",
    "    kind=\"linear\",\n",
    "    bounds_error=False,\n",
    "    fill_value=(0.0, 1.0),\n",
    ")\n",
    "# Target Cumulative Density -> Target Pixel Intensity\n",
    "target_cdf_inverse_mapper = scipy.interpolate.interp1d(\n",
    "    np.cumsum(clinic_transform_hist[:, 1]),\n",
    "    clinic_transform_hist[:, 0],\n",
    "    kind=\"linear\",\n",
    "    bounds_error=False,\n",
    "    fill_value=(clinic_transform_hist[:, 0].min(), clinic_transform_hist[:, 0].max()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046ea490-52aa-41b1-9092-958a3c67de20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adapt_img(\n",
    "    img, source_hist_intensity_mapper, source_cdf_mapper, target_cdf_inverse_mapper\n",
    "):\n",
    "    shape = img.shape\n",
    "    x = img.flatten()\n",
    "\n",
    "    x_adapt_intens = source_hist_intensity_mapper(x)\n",
    "    x_densities = source_cdf_mapper(x_adapt_intens)\n",
    "    y_intens = target_cdf_inverse_mapper(x_densities)\n",
    "\n",
    "    return y_intens.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8227cb3-d37d-444f-8fa9-8fde010d045f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f_hcp_to_clinic = functools.partial(\n",
    "    adapt_img,\n",
    "    source_hist_intensity_mapper=intensity_map,\n",
    "    source_cdf_mapper=source_cdf_mapper,\n",
    "    target_cdf_inverse_mapper=target_cdf_inverse_mapper,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f62650-f9f7-4099-a9a6-821bed21597a",
   "metadata": {},
   "source": [
    "## Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c5a7e4-702d-4955-b815-0cf2d6c0e017",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=140, constrained_layout=True, figsize=(8, 10))\n",
    "hcp_d = hcp_subj_dataset[-3]\n",
    "hcp_t1 = hcp_d[\"t1w\"][0].cpu().numpy()\n",
    "hcp_mask = hcp_d[\"mask\"][0].cpu().numpy()\n",
    "\n",
    "clinic_d = clinic_subj_dataset[-1]\n",
    "clinic_t1 = clinic_d[\"t1w\"][0].cpu().numpy()\n",
    "clinic_mask = clinic_d[\"mask\"][0].cpu().numpy()\n",
    "\n",
    "hcp_slice = np.index_exp[:, :, 84]\n",
    "clinic_slice = np.index_exp[:, :, 90]\n",
    "imgs = [\n",
    "    hcp_t1[hcp_slice],\n",
    "    intensity_map(hcp_t1[hcp_slice]).reshape(hcp_t1.shape[:2]),\n",
    "    f_hcp_to_clinic(hcp_t1[hcp_slice]),\n",
    "    clinic_t1[clinic_slice],\n",
    "]\n",
    "clinic_bounds = (0, max(map(lambda img: np.quantile(img.flatten(), 0.99), imgs[1:])))\n",
    "bounds = [(0, np.quantile(imgs[0], 0.99))] + list((clinic_bounds,) * (len(imgs) - 1))\n",
    "colors=['orange', 'red', 'blue', 'green']\n",
    "histograms = [\n",
    "    hcp_t1[hcp_mask.astype(bool)],\n",
    "    intensity_map(hcp_t1[hcp_mask.astype(bool)]),\n",
    "    f_hcp_to_clinic(hcp_t1[hcp_mask.astype(bool)]),\n",
    "    clinic_t1[clinic_mask.astype(bool)],\n",
    "]\n",
    "\n",
    "gs = fig.add_gridspec(nrows=len(imgs) + 1, ncols=3, hspace=0.2)\n",
    "\n",
    "for i_row in range(len(imgs)):\n",
    "    img_ax = fig.add_subplot(gs[i_row, 0])\n",
    "    im = img_ax.imshow(\n",
    "        np.rot90(imgs[i_row]), cmap=\"gray\", vmin=bounds[i_row][0], vmax=bounds[i_row][1]\n",
    "    )\n",
    "    img_ax.set_xticks([])\n",
    "    img_ax.set_yticks([])\n",
    "    img_ax.set_xticklabels([])\n",
    "    img_ax.set_yticklabels([])\n",
    "    plt.colorbar(im, ax=img_ax, location=\"left\", shrink=0.85, fraction=0.1, aspect=10)\n",
    "\n",
    "    hist_ax = fig.add_subplot(gs[i_row, 1:])\n",
    "    hist_ax = sns.histplot(\n",
    "        histograms[i_row],\n",
    "        kde=True,\n",
    "        stat=\"probability\",\n",
    "        binrange=bounds[i_row],\n",
    "        ax=hist_ax,\n",
    "        color=colors[i_row],\n",
    "    )\n",
    "    hist_ax.set_xlim(*bounds[i_row])\n",
    "    hist_ax.set_ylim(0, 0.04)\n",
    "\n",
    "combine_hist_ax = fig.add_subplot(gs[-1, :])\n",
    "for i_hist, color, lab in zip(\n",
    "    range(1, 4), [\"red\", \"blue\", \"green\"], [\"Lin Map\", \"CDF Resample\", \"Clinic\"]\n",
    "):\n",
    "    combine_hist_ax = sns.kdeplot(\n",
    "        histograms[i_hist],\n",
    "        ax=combine_hist_ax,\n",
    "        label=lab,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        alpha=0.4,\n",
    "        legend=True,\n",
    "    )\n",
    "    \n",
    "combine_hist_ax.set_xlim(*clinic_bounds)\n",
    "combine_hist_ax.set_ylim(0, 0.04)\n",
    "combine_hist_ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b608a09b-511f-4aea-a6d7-8830ddbed32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in peak_ranges.values():\n",
    "\n",
    "    intensity_range = tuple(p[\"range\"][:, 0])\n",
    "    filtered_t1 = torch.where(\n",
    "        (t1 >= intensity_range[0]) & (t1 <= intensity_range[1]) & mask.bool(),\n",
    "        t1.double(),\n",
    "        0.0,\n",
    "    )\n",
    "    dipy.viz.regtools.plot_slices(filtered_t1.cpu().numpy()).set_dpi(120)\n",
    "    plt.suptitle(f\"Range [{intensity_range[0]}, {intensity_range[1]}]\")\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b096397-39e5-41a5-a44f-0db9ef644505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda-pitn]",
   "language": "python",
   "name": "conda-env-miniconda-pitn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
