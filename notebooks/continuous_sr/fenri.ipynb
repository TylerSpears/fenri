{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3646f639",
   "metadata": {},
   "source": [
    "# Continuous-Space Super-Resolution of fODFs in Diffusion MRI\n",
    "\n",
    "Code by:\n",
    "\n",
    "Tyler Spears - tas6hh@virginia.edu\n",
    "\n",
    "Dr. Tom Fletcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314c19b9",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "753fcb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Automatically re-import project-specific modules.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# imports\n",
    "import collections\n",
    "import copy\n",
    "import datetime\n",
    "import functools\n",
    "import inspect\n",
    "import io\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import pdb\n",
    "import random\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "import typing\n",
    "import warnings\n",
    "import zipfile\n",
    "import concurrent.futures\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from pprint import pprint as ppr\n",
    "\n",
    "import aim\n",
    "import dotenv\n",
    "import einops\n",
    "import lightning\n",
    "\n",
    "# visualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import monai\n",
    "\n",
    "# Data management libraries.\n",
    "import nibabel as nib\n",
    "import nibabel.processing\n",
    "\n",
    "# Computation & ML libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import skimage\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from box import Box\n",
    "from icecream import ic\n",
    "from inr_networks import Decoder, INREncoder, SimplifiedDecoder\n",
    "\n",
    "# from lightning_fabric.fabric import Fabric\n",
    "from natsort import natsorted\n",
    "\n",
    "import pitn\n",
    "\n",
    "plt.rcParams.update({\"figure.autolayout\": True})\n",
    "plt.rcParams.update({\"figure.facecolor\": [1.0, 1.0, 1.0, 1.0]})\n",
    "plt.rcParams.update({\"image.cmap\": \"gray\"})\n",
    "\n",
    "# Set print options for ndarrays/tensors.\n",
    "np.set_printoptions(suppress=True, threshold=100, linewidth=88)\n",
    "torch.set_printoptions(sci_mode=False, threshold=100, linewidth=88)\n",
    "\n",
    "# monai.data.set_track_meta(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b23ac2a5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# MAIN\n",
    "# if __name__ == \"__main__\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e41cbc1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "direnv: loading ~/Projects/pitn/.envrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update notebook's environment variables with direnv.\n",
    "# This requires the python-dotenv package, and direnv be installed on the system\n",
    "# This will not work on Windows.\n",
    "# NOTE: This is kind of hacky, and not necessarily safe. Be careful...\n",
    "# Libraries needed on the python side:\n",
    "# - os\n",
    "# - subprocess\n",
    "# - io\n",
    "# - dotenv\n",
    "\n",
    "# Form command to be run in direnv's context. This command will print out\n",
    "# all environment variables defined in the subprocess/sub-shell.\n",
    "command = \"direnv exec {} /usr/bin/env\".format(os.getcwd())\n",
    "# Run command in a new subprocess.\n",
    "proc = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, cwd=os.getcwd())\n",
    "# Store and format the subprocess' output.\n",
    "proc_out = proc.communicate()[0].strip().decode(\"utf-8\")\n",
    "# Use python-dotenv to load the environment variables by using the output of\n",
    "# 'direnv exec ...' as a 'dummy' .env file.\n",
    "dotenv.load_dotenv(stream=io.StringIO(proc_out), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3e6ce46",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Device IDX  0\n",
      "CUDA Current Device  0\n",
      "CUDA Device properties:  _CudaDeviceProperties(name='NVIDIA RTX A5000', major=8, minor=6, total_memory=24247MB, multi_processor_count=64)\n",
      "CuDNN convolution optimization enabled.\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# torch setup\n",
    "# allow for CUDA usage, if available\n",
    "if torch.cuda.is_available():\n",
    "    # Pick only one device for the default, may use multiple GPUs for training later.\n",
    "    if \"CUDA_PYTORCH_DEVICE_IDX\" in os.environ.keys():\n",
    "        dev_idx = int(os.environ[\"CUDA_PYTORCH_DEVICE_IDX\"])\n",
    "    else:\n",
    "        dev_idx = 0\n",
    "    device = torch.device(f\"cuda:{dev_idx}\")\n",
    "    print(\"CUDA Device IDX \", dev_idx)\n",
    "    torch.cuda.set_device(device)\n",
    "    print(\"CUDA Current Device \", torch.cuda.current_device())\n",
    "    print(\"CUDA Device properties: \", torch.cuda.get_device_properties(device))\n",
    "    # The flag below controls whether to allow TF32 on matmul. This flag defaults to False\n",
    "    # in PyTorch 1.12 and later.\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    # See\n",
    "    # <https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices>\n",
    "    # for details.\n",
    "\n",
    "    # Activate cudnn benchmarking to optimize convolution algorithm speed.\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        print(\"CuDNN convolution optimization enabled.\")\n",
    "        # The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# keep device as the cpu\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52a8f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr cap\n",
    "# Capture output and save to log. Needs to be at the *very first* line of the cell.\n",
    "# Watermark\n",
    "%load_ext watermark\n",
    "%watermark --author \"Tyler Spears\" --updated --iso8601  --python --machine --iversions --githash\n",
    "if torch.cuda.is_available():\n",
    "    # GPU information\n",
    "    try:\n",
    "        gpu_info = pitn.utils.system.get_gpu_specs()\n",
    "        print(gpu_info)\n",
    "    except NameError:\n",
    "        print(\"CUDA Version: \", torch.version.cuda)\n",
    "else:\n",
    "    print(\"CUDA not in use, falling back to CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7f4f6ee",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Tyler Spears\n",
      "\n",
      "Last updated: 2023-10-14T12:28:39.585679-04:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.9\n",
      "IPython version      : 8.4.0\n",
      "\n",
      "Compiler    : GCC 11.3.0\n",
      "OS          : Linux\n",
      "Release     : 5.19.0-46-generic\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 20\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: 0e616caf264d9da0dd908e26e263fea0f006ebb4\n",
      "\n",
      "nibabel   : 5.1.0\n",
      "monai     : 1.2.0\n",
      "seaborn   : 0.12.2\n",
      "skimage   : 0.19.3\n",
      "matplotlib: 3.5.2\n",
      "lightning : 2.0.9.post0\n",
      "einops    : 0.6.0\n",
      "torch     : 2.0.1\n",
      "pandas    : 1.5.2\n",
      "numpy     : 1.24.4\n",
      "pitn      : 0.0.post1.dev291+ge9e5f58.d20230531\n",
      "aim       : 3.14.4\n",
      "sys       : 3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:20:04) [GCC 11.3.0]\n",
      "\n",
      "==================================================GPU Specs==================================================\n",
      "  id  Name              Driver Version      CUDA Version  Total Memory    uuid\n",
      "----  ----------------  ----------------  --------------  --------------  ----------------------------------------\n",
      "   0  NVIDIA RTX A5000  525.105.17                  11.8  24564.0MB       GPU-ed20d87f-e88e-692f-0b56-548b8a05ddea\n",
      "   1  NVIDIA RTX A5000  525.105.17                  11.8  24564.0MB       GPU-0636ee40-2eab-9533-1be7-dbbadade95c4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cap is defined in an ipython magic command\n",
    "try:\n",
    "    print(cap)\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd24b58a",
   "metadata": {},
   "source": [
    "## Experiment & Parameters Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "859488e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Config file not loaded\n"
     ]
    }
   ],
   "source": [
    "p = Box(default_box=True)\n",
    "# Experiment defaults, can be overridden in a config file.\n",
    "\n",
    "# General experiment-wide params\n",
    "###############################################\n",
    "p.experiment_name = \"test_FENRI_preproc_rewrite\"\n",
    "p.override_experiment_name = False\n",
    "p.results_dir = \"/data/srv/outputs/pitn/results/runs\"\n",
    "p.tmp_results_dir = \"/data/srv/outputs/pitn/results/tmp\"\n",
    "p.train_val_test_split_file = (\n",
    "    Path(\"./data_splits\") / \"HCP_train-val-test_split_01.1.csv\"\n",
    ")\n",
    "# p.train_val_test_split_file = random.choice(\n",
    "#     list(Path(\"./data_splits\").glob(\"HCP*train-val-test_split*.csv\"))\n",
    "# )\n",
    "p.aim_logger = dict(\n",
    "    repo=\"aim://dali.cpe.virginia.edu:53800\",\n",
    "    # repo=\"/data/srv/outputs/pitn/results/aim/tmp\",\n",
    "    experiment=\"PITN_FENRI\",\n",
    "    meta_params=dict(run_name=p.experiment_name),\n",
    "    tags=(\"PITN\", \"INR\", \"HCP\", \"super-res\", \"dMRI\", \"FENRI\"),\n",
    ")\n",
    "p.checkpoint_epoch_ratios = (0.5,)\n",
    "###############################################\n",
    "\n",
    "# 1.25mm -> 2.0mm\n",
    "p.preproc_loaded = dict(S0_noise_b0_quantile=0.99, patch_sampling_w_erosion=17)\n",
    "p.baseline_lr_spacing_scale = 1.6\n",
    "p.baseline_snr = 30\n",
    "p.train = dict(\n",
    "    patch_size=(36, 36, 36),\n",
    "    batch_size=6,\n",
    "    samples_per_subj_per_epoch=100,\n",
    "    max_epochs=50,\n",
    "    dwi_recon_epoch_proportion=1 / 99,\n",
    "    # dwi_recon_epoch_proportion=0.0,\n",
    ")\n",
    "p.train.patch_sampling = dict(rng=\"default\")\n",
    "p.train.patch_tf = dict(\n",
    "    downsample_factor_range=(p.baseline_lr_spacing_scale, 2.0),\n",
    "    noise_snr_range=(p.baseline_snr, 35),\n",
    "    prefilter_sigma_scale_coeff=2.0,\n",
    "    rng=\"default\",\n",
    ")\n",
    "\n",
    "# Optimizer kwargs for training.\n",
    "p.train.optim.encoder.lr = 5e-4\n",
    "p.train.optim.decoder.lr = 5e-4\n",
    "p.train.optim.recon_decoder.lr = 1e-3\n",
    "# Train dataloader kwargs.\n",
    "p.train.dataloader = dict(num_workers=17, persistent_workers=True, prefetch_factor=3)\n",
    "# p.train.dataloader = dict(num_workers=0)\n",
    "\n",
    "# Network/model parameters.\n",
    "p.encoder = dict(\n",
    "    interior_channels=80,\n",
    "    out_channels=96,\n",
    "    n_res_units=3,\n",
    "    n_dense_units=3,\n",
    "    activate_fn=\"relu\",\n",
    "    input_coord_channels=True,\n",
    "    post_batch_norm=True,\n",
    ")\n",
    "p.decoder = dict(\n",
    "    context_v_features=96,\n",
    "    out_features=45,\n",
    "    m_encode_num_freqs=36,\n",
    "    sigma_encode_scale=3.0,\n",
    "    n_internal_features=256,\n",
    "    n_internal_layers=3,\n",
    ")\n",
    "p.val.rng_seed = 3967417599011123030\n",
    "p.val.vol_tf = dict(\n",
    "    downsample_factor_range=(p.baseline_lr_spacing_scale, p.baseline_lr_spacing_scale),\n",
    "    noise_snr_range=(p.baseline_snr, p.baseline_snr),\n",
    "    prefilter_sigma_scale_coeff=2.0,\n",
    "    # Manually crop each side by 1 voxel to avoid NaNs in the LR resampling.\n",
    "    manual_crop_lr_sides=((1, 1), (1, 1), (1, 1)),\n",
    "    # rng='default'\n",
    ")\n",
    "\n",
    "# If a config file exists, override the defaults with those values.\n",
    "try:\n",
    "    if \"PITN_CONFIG\" in os.environ.keys():\n",
    "        config_fname = Path(os.environ[\"PITN_CONFIG\"])\n",
    "    else:\n",
    "        config_fname = pitn.utils.system.get_file_glob_unique(Path(\".\"), r\"config.*\")\n",
    "    f_type = config_fname.suffix.casefold()\n",
    "    if f_type in {\".yaml\", \".yml\"}:\n",
    "        f_params = Box.from_yaml(filename=config_fname)\n",
    "    elif f_type == \".json\":\n",
    "        f_params = Box.from_json(filename=config_fname)\n",
    "    elif f_type == \".toml\":\n",
    "        f_params = Box.from_toml(filename=config_fname)\n",
    "    else:\n",
    "        raise RuntimeError()\n",
    "\n",
    "    p.merge_update(f_params)\n",
    "\n",
    "except:\n",
    "    print(\"WARNING: Config file not loaded\")\n",
    "    pass\n",
    "\n",
    "# Remove the default_box behavior now that params have been fully read in.\n",
    "_p = Box(default_box=False)\n",
    "_p.merge_update(p)\n",
    "p = _p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8a421a7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "tvt_split = pd.read_csv(p.train_val_test_split_file)\n",
    "p.train.subj_ids = natsorted(tvt_split[tvt_split.split == \"train\"].subj_id.tolist())\n",
    "if \"val\" not in p.keys():\n",
    "    p.val = dict()\n",
    "p.val.subj_ids = natsorted(tvt_split[tvt_split.split == \"val\"].subj_id.tolist())\n",
    "if \"test\" not in p.keys():\n",
    "    p.test = dict()\n",
    "p.test.subj_ids = natsorted(tvt_split[tvt_split.split == \"test\"].subj_id.tolist())\n",
    "\n",
    "# Ensure that no test subj ids are in either the training or validation sets.\n",
    "# However, we can have overlap between training and validation.\n",
    "assert len(set(p.train.subj_ids) & set(p.test.subj_ids)) == 0\n",
    "assert len(set(p.val.subj_ids) & set(p.test.subj_ids)) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d00d24d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| p.to_dict(): {'aim_logger': {'experiment': 'PITN_FENRI',\n",
      "                                 'meta_params': {'run_name': 'test_FENRI_preproc_rewrite'},\n",
      "                                 'repo': 'aim://dali.cpe.virginia.edu:53800',\n",
      "                                 'tags': ('PITN', 'INR', 'HCP', 'super-res', 'dMRI', 'FENRI')},\n",
      "                  'baseline_lr_spacing_scale': 1.6,\n",
      "                  'baseline_snr': 30,\n",
      "                  'checkpoint_epoch_ratios': (0.5,),\n",
      "                  'decoder': {'context_v_features': 96,\n",
      "                              'm_encode_num_freqs': 36,\n",
      "                              'n_internal_features': 256,\n",
      "                              'n_internal_layers': 3,\n",
      "                              'out_features': 45,\n",
      "                              'sigma_encode_scale': 3.0},\n",
      "                  'encoder': {'activate_fn': 'relu',\n",
      "                              'input_coord_channels': True,\n",
      "                              'interior_channels': 80,\n",
      "                              'n_dense_units': 3,\n",
      "                              'n_res_units': 3,\n",
      "                              'out_channels': 96,\n",
      "                              'post_batch_norm': True},\n",
      "                  'experiment_name': 'test_FENRI_preproc_rewrite',\n",
      "                  'override_experiment_name': False,\n",
      "                  'preproc_loaded': {'S0_noise_b0_quantile': 0.99,\n",
      "                                     'patch_sampling_w_erosion': 17},\n",
      "                  'results_dir':"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " '/data/srv/outputs/pitn/results/runs',\n",
      "                  'test': {'subj_ids': [106824,\n",
      "                                        144125,\n",
      "                                        163331,\n",
      "                                        172433,\n",
      "                                        200513,\n",
      "                                        213522,\n",
      "                                        236130,\n",
      "                                        245333,\n",
      "                                        248339,\n",
      "                                        250932,\n",
      "                                        255639,\n",
      "                                        256540,\n",
      "                                        300719,\n",
      "                                        376247,\n",
      "                                        379657,\n",
      "                                        459453,\n",
      "                                        469961,\n",
      "                                        480141,\n",
      "                                        562446,\n",
      "                                        566454,\n",
      "                                        580751,\n",
      "                                        654754,\n",
      "                                        677968,\n",
      "                                        715647,\n",
      "                                        727654,\n",
      "                                        773257,\n",
      "                                        872562,\n",
      "                                        911849,\n",
      "                                        991267]},\n",
      "                  'tmp_results_dir': '/data/srv/outputs/pitn/results/tmp',\n",
      "                  'train': {'batch_size': 6,\n",
      "                            'dataloader': {'num_workers': 16,\n",
      "                                           'persistent_workers': True,\n",
      "                                           'prefetch_factor': 3},\n",
      "                            'dwi_recon_epoch_proportion': 0.010101010101010102,\n",
      "                            'max_epochs': 40,\n",
      "                            'optim': {'decoder': {'lr': 0.0005},\n",
      "                                      'encoder': {'lr': 0.0005},\n",
      "                                      'recon_decoder': {'lr': 0.001}},\n",
      "                            'patch_sampling': {'rng': 'default'},\n",
      "                            'patch_size': (36, 36, 36),\n",
      "                            'patch_tf': {'downsample_factor_range': (1.6, 2.0),\n",
      "                                         'noise_snr_range': (30, 35),\n",
      "                                         'prefilter_sigma_scale_coeff': 2.0,\n",
      "                                         'rng': 'default'},\n",
      "                            'samples_per_subj_per_epoch': 100,\n",
      "                            'subj_ids': [100206,\n",
      "                                         102109,\n",
      "                                         104820,\n",
      "                                         105115,\n",
      "                                         107422,\n",
      "                                         114419,\n",
      "                                         118124,\n",
      "                                         118730,\n",
      "                                         119833,\n",
      "                                         127832,\n",
      "                                         141422,\n",
      "                                         151425,\n",
      "                                         158540,\n",
      "                                         159946,\n",
      "                                         160931,\n",
      "                                         171532,\n",
      "                                         183741,\n",
      "                                         186545,\n",
      "                                         208226,\n",
      "                                         211922,\n",
      "                                         217429,\n",
      "                                         250427,\n",
      "                                         361941,\n",
      "                                         481042,\n",
      "                                         485757,\n",
      "                                         492754,\n",
      "                                         516742,\n",
      "                                         647858,\n",
      "                                         680452,\n",
      "                                         735148,\n",
      "                                         748662,\n",
      "                                         788674,\n",
      "                                         802844,\n",
      "                                         809252,\n",
      "                                         845458]},\n",
      "                  'train_val_test_split_file': PosixPath('data_splits/HCP_train-val-test_split_03.1.csv'),\n",
      "                  'val': {'rng_seed': 3967417599011123030,\n",
      "                          'subj_ids': [121719, 156637, 168341, 627852, 993675],\n",
      "                          'vol_tf': {'downsample_factor_range': (1.6, 1.6),\n",
      "                                     'manual_crop_lr_sides': ((1, 1), (1, 1), (1, 1)),\n",
      "                                     'noise_snr_range': (30, 30),\n",
      "                                     'prefilter_sigma_scale_coeff': 2.0}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'experiment_name': 'test_FENRI_preproc_rewrite',\n",
       " 'override_experiment_name': False,\n",
       " 'results_dir': '/data/srv/outputs/pitn/results/runs',\n",
       " 'tmp_results_dir': '/data/srv/outputs/pitn/results/tmp',\n",
       " 'train_val_test_split_file': PosixPath('data_splits/HCP_train-val-test_split_03.1.csv'),\n",
       " 'aim_logger': {'repo': 'aim://dali.cpe.virginia.edu:53800',\n",
       "  'experiment': 'PITN_FENRI',\n",
       "  'meta_params': {'run_name': 'test_FENRI_preproc_rewrite'},\n",
       "  'tags': ('PITN', 'INR', 'HCP', 'super-res', 'dMRI', 'FENRI')},\n",
       " 'checkpoint_epoch_ratios': (0.5,),\n",
       " 'preproc_loaded': {'S0_noise_b0_quantile': 0.99,\n",
       "  'patch_sampling_w_erosion': 17},\n",
       " 'baseline_lr_spacing_scale': 1.6,\n",
       " 'baseline_snr': 30,\n",
       " 'train': {'patch_size': (36, 36, 36),\n",
       "  'batch_size': 6,\n",
       "  'samples_per_subj_per_epoch': 100,\n",
       "  'max_epochs': 40,\n",
       "  'dwi_recon_epoch_proportion': 0.010101010101010102,\n",
       "  'patch_sampling': {'rng': 'default'},\n",
       "  'patch_tf': {'downsample_factor_range': (1.6, 2.0),\n",
       "   'noise_snr_range': (30, 35),\n",
       "   'prefilter_sigma_scale_coeff': 2.0,\n",
       "   'rng': 'default'},\n",
       "  'optim': {'encoder': {'lr': 0.0005},\n",
       "   'decoder': {'lr': 0.0005},\n",
       "   'recon_decoder': {'lr': 0.001}},\n",
       "  'dataloader': {'num_workers': 16,\n",
       "   'persistent_workers': True,\n",
       "   'prefetch_factor': 3},\n",
       "  'subj_ids': [100206,\n",
       "   102109,\n",
       "   104820,\n",
       "   105115,\n",
       "   107422,\n",
       "   114419,\n",
       "   118124,\n",
       "   118730,\n",
       "   119833,\n",
       "   127832,\n",
       "   141422,\n",
       "   151425,\n",
       "   158540,\n",
       "   159946,\n",
       "   160931,\n",
       "   171532,\n",
       "   183741,\n",
       "   186545,\n",
       "   208226,\n",
       "   211922,\n",
       "   217429,\n",
       "   250427,\n",
       "   361941,\n",
       "   481042,\n",
       "   485757,\n",
       "   492754,\n",
       "   516742,\n",
       "   647858,\n",
       "   680452,\n",
       "   735148,\n",
       "   748662,\n",
       "   788674,\n",
       "   802844,\n",
       "   809252,\n",
       "   845458]},\n",
       " 'encoder': {'interior_channels': 80,\n",
       "  'out_channels': 96,\n",
       "  'n_res_units': 3,\n",
       "  'n_dense_units': 3,\n",
       "  'activate_fn': 'relu',\n",
       "  'input_coord_channels': True,\n",
       "  'post_batch_norm': True},\n",
       " 'decoder': {'context_v_features': 96,\n",
       "  'out_features': 45,\n",
       "  'm_encode_num_freqs': 36,\n",
       "  'sigma_encode_scale': 3.0,\n",
       "  'n_internal_features': 256,\n",
       "  'n_internal_layers': 3},\n",
       " 'val': {'rng_seed': 3967417599011123030,\n",
       "  'vol_tf': {'downsample_factor_range': (1.6, 1.6),\n",
       "   'noise_snr_range': (30, 30),\n",
       "   'prefilter_sigma_scale_coeff': 2.0,\n",
       "   'manual_crop_lr_sides': ((1, 1), (1, 1), (1, 1))},\n",
       "  'subj_ids': [121719, 156637, 168341, 627852, 993675]},\n",
       " 'test': {'subj_ids': [106824,\n",
       "   144125,\n",
       "   163331,\n",
       "   172433,\n",
       "   200513,\n",
       "   213522,\n",
       "   236130,\n",
       "   245333,\n",
       "   248339,\n",
       "   250932,\n",
       "   255639,\n",
       "   256540,\n",
       "   300719,\n",
       "   376247,\n",
       "   379657,\n",
       "   459453,\n",
       "   469961,\n",
       "   480141,\n",
       "   562446,\n",
       "   566454,\n",
       "   580751,\n",
       "   654754,\n",
       "   677968,\n",
       "   715647,\n",
       "   727654,\n",
       "   773257,\n",
       "   872562,\n",
       "   911849,\n",
       "   991267]}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(p.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddac0231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which parameters to store in the aim meta-params.\n",
    "p.aim_logger.meta_params.hparams = dict(\n",
    "    batch_size=p.train.batch_size,\n",
    "    patch_spatial_size=p.train.patch_size,\n",
    "    samples_per_subj_per_epoch=p.train.samples_per_subj_per_epoch,\n",
    "    max_epochs=p.train.max_epochs,\n",
    ")\n",
    "p.aim_logger.meta_params.data = dict(\n",
    "    train_subj_ids=p.train.subj_ids,\n",
    "    val_subj_ids=p.val.subj_ids,\n",
    "    test_subj_ids=p.test.subj_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "173f55e9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def fork_rng(rng: torch.Generator) -> torch.Generator:\n",
    "    rng_fork = torch.Generator(device=rng.device)\n",
    "    rng_fork.set_state(rng.get_state())\n",
    "    return rng_fork\n",
    "\n",
    "\n",
    "rng = fork_rng(torch.default_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d31c8f",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc9aa666",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "num_load_and_tf_workers = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3590a6d4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "hcp_data_root_dir = Path(\"/data/srv/outputs/pitn/hcp\")\n",
    "\n",
    "assert hcp_data_root_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02a6eb1f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Set paths relative to the subj id root dir for each required image/file.\n",
    "rel_dwi_path = Path(\"ras/diffusion/dwi_norm.nii.gz\")\n",
    "rel_grad_table_path = Path(\"ras/diffusion/ras_grad_mrtrix.b\")\n",
    "rel_odf_path = Path(\"ras/odf/wm_msmt_csd_norm_odf.nii.gz\")\n",
    "rel_fivett_seg_path = Path(\"ras/segmentation/fivett_dwi-space_segmentation.nii.gz\")\n",
    "rel_brain_mask_path = Path(\"ras/brain_mask.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bce90ce",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Set a common target set of gradient directions/strengths based on the standard HCP\n",
    "# protocol.\n",
    "first_n_b0s = 9\n",
    "first_n_b1000s = 45\n",
    "# Remove the b2000s entirely.\n",
    "first_n_b3000s = 45\n",
    "\n",
    "target_grad_table = pitn.data.HCP_STANDARD_3T_GRAD_MRTRIX_TABLE\n",
    "\n",
    "shells = target_grad_table.b.to_numpy().round(-2)\n",
    "row_select_template = np.zeros_like(shells).astype(bool)\n",
    "# Take first N b0s\n",
    "b0_idx = (np.where(shells == 0)[0][:first_n_b0s],)\n",
    "b0_select = row_select_template.copy()\n",
    "b0_select[b0_idx] = True\n",
    "# Take first N b1000s\n",
    "b1000_idx = (np.where(shells == 1000)[0][:first_n_b1000s],)\n",
    "b1000_select = row_select_template.copy()\n",
    "b1000_select[b1000_idx] = True\n",
    "# Take first N b3000s\n",
    "b3000_idx = (np.where(shells == 3000)[0][:first_n_b3000s],)\n",
    "b3000_select = row_select_template.copy()\n",
    "b3000_select[b3000_idx] = True\n",
    "\n",
    "dwi_select_mask = b0_select | b1000_select | b3000_select\n",
    "target_grad_table = target_grad_table.loc[dwi_select_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "951e44d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_loaded_kwargs = dict(\n",
    "    S0_noise_b0_quantile=p.preproc_loaded.S0_noise_b0_quantile,\n",
    "    patch_sampling_w_erosion=p.preproc_loaded.patch_sampling_w_erosion,\n",
    "    resample_target_grad_table=target_grad_table,\n",
    ")\n",
    "\n",
    "# Worker function as a single-argument callable.\n",
    "def load_and_deterministic_tf_subj(subj_files: dict):\n",
    "    print(\n",
    "        f\"{os.getpid()} : Loading subj {subj_files['subj_id']}...\\n\",\n",
    "        end=\"\",\n",
    "        flush=True,\n",
    "    )\n",
    "    s = pitn.data.load_super_res_subj_sample(**subj_files)\n",
    "    print(\n",
    "        f\"{os.getpid()} : Finished loading subj {subj_files['subj_id']}\\n\",\n",
    "        end=\"\",\n",
    "        flush=True,\n",
    "    )\n",
    "    return pitn.data.preproc.preproc_loaded_super_res_subj(s, **preproc_loaded_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbaf649",
   "metadata": {},
   "source": [
    "### Training Dataset of Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3f873a9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1686985 : Loading subj 100206...\n",
      "1686986 : Loading subj 102109...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1686986 : Finished loading subj 102109\n",
      "1686985 : Finished loading subj 100206\n",
      "Done loading training subject data\n"
     ]
    }
   ],
   "source": [
    "# DEBUG_TRAIN_DATA_SUBJS = 2\n",
    "\n",
    "train_subj_ids = p.train.subj_ids\n",
    "# train_subj_ids = p.train.subj_ids[:DEBUG_TRAIN_DATA_SUBJS]  #!DEBUG\n",
    "\n",
    "train_subj_dicts = list()\n",
    "for subj_id in train_subj_ids:\n",
    "    root_dir = hcp_data_root_dir / str(subj_id)\n",
    "    d = dict(\n",
    "        subj_id=str(subj_id),\n",
    "        dwi_f=root_dir / rel_dwi_path,\n",
    "        grad_mrtrix_f=root_dir / rel_grad_table_path,\n",
    "        odf_f=root_dir / rel_odf_path,\n",
    "        brain_mask_f=root_dir / rel_brain_mask_path,\n",
    "        fivett_seg_f=root_dir / rel_fivett_seg_path,\n",
    "    )\n",
    "    train_subj_dicts.append(d)\n",
    "\n",
    "# Load & run non-random transforms on training subjects in parallel.\n",
    "preproc_train_dataset = list()\n",
    "with concurrent.futures.ProcessPoolExecutor(\n",
    "    max_workers=num_load_and_tf_workers\n",
    ") as executor:\n",
    "    subj_data_futures = executor.map(load_and_deterministic_tf_subj, train_subj_dicts)\n",
    "    for subj_data in subj_data_futures:\n",
    "        preproc_train_dataset.append(subj_data)\n",
    "\n",
    "print(\"Done loading training subject data\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82a921f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset that randomly samples and (ramdonly) transforms patches.\n",
    "train_patch_fn = partial(\n",
    "    pitn.data.preproc.lazy_sample_patch_from_super_res_sample,\n",
    "    patch_size=p.train.patch_size,\n",
    "    num_samples=p.train.samples_per_subj_per_epoch,\n",
    "    rng=\"default\",\n",
    ")\n",
    "train_patch_tf = partial(\n",
    "    pitn.data.preproc.preproc_super_res_sample, **p.train.patch_tf.to_dict()\n",
    ")\n",
    "train_dataset = monai.data.PatchDataset(\n",
    "    data=preproc_train_dataset,\n",
    "    patch_func=train_patch_fn,\n",
    "    samples_per_image=p.train.samples_per_subj_per_epoch,\n",
    "    transform=train_patch_tf,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c048a20d",
   "metadata": {},
   "source": [
    "### Validation Dataset of Whole Volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8454f7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1687045 : Loading subj 121719...\n",
      "1687046 : Loading subj 156637...\n",
      "1687046 : Loading subj 156637...\n",
      "1687045 : Finished loading subj 121719\n",
      "1687046 : Finished loading subj 156637\n",
      "Done loading validation subject data\n"
     ]
    }
   ],
   "source": [
    "# DEBUG_VAL_DATA_SUBJS = 2\n",
    "\n",
    "val_subj_ids = p.val.subj_ids\n",
    "# val_subj_ids = p.val.subj_ids[:DEBUG_VAL_DATA_SUBJS]  #!DEBUG\n",
    "\n",
    "val_subj_dicts = list()\n",
    "for subj_id in val_subj_ids:\n",
    "    root_dir = hcp_data_root_dir / str(subj_id)\n",
    "    d = dict(\n",
    "        subj_id=str(subj_id),\n",
    "        dwi_f=root_dir / rel_dwi_path,\n",
    "        grad_mrtrix_f=root_dir / rel_grad_table_path,\n",
    "        odf_f=root_dir / rel_odf_path,\n",
    "        brain_mask_f=root_dir / rel_brain_mask_path,\n",
    "        fivett_seg_f=root_dir / rel_fivett_seg_path,\n",
    "    )\n",
    "    val_subj_dicts.append(d)\n",
    "\n",
    "# Load & run non-random transforms on subjects in parallel.\n",
    "preproc_val_dataset = list()\n",
    "with concurrent.futures.ProcessPoolExecutor(\n",
    "    max_workers=num_load_and_tf_workers\n",
    ") as executor:\n",
    "    subj_data_futures = executor.map(load_and_deterministic_tf_subj, val_subj_dicts)\n",
    "    for subj_data in subj_data_futures:\n",
    "        preproc_val_dataset.append(subj_data)\n",
    "\n",
    "print(\"Done loading validation subject data\", flush=True)\n",
    "\n",
    "\n",
    "# Seed the validation set generator for deterministic transforms of test samples between\n",
    "# networks,trilinear interpolation, etc.\n",
    "# The seed is set as 'val_seed_int XOR subj_id_int'.\n",
    "val_dataset = monai.data.Dataset(\n",
    "    [\n",
    "        pitn.data.preproc.preproc_super_res_sample(\n",
    "            v,\n",
    "            **p.val.vol_tf.to_dict(),\n",
    "            rng=torch.Generator(device=rng.device).manual_seed(\n",
    "                int(p.val.rng_seed) ^ int(v[\"subj_id\"])\n",
    "            ),\n",
    "        )\n",
    "        for v in preproc_val_dataset\n",
    "    ]\n",
    ")\n",
    "del preproc_val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de803e6",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1fd010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger_run(run_kwargs: dict, logger_meta_params: dict, logger_tags: list):\n",
    "    aim_run = aim.Run(\n",
    "        system_tracking_interval=None,\n",
    "        log_system_params=True,\n",
    "        capture_terminal_logs=True,\n",
    "        **run_kwargs,\n",
    "    )\n",
    "    for k, v in logger_meta_params.items():\n",
    "        aim_run[k] = v\n",
    "    for v in logger_tags:\n",
    "        aim_run.add_tag(v)\n",
    "\n",
    "    return aim_run\n",
    "\n",
    "\n",
    "def batchwise_masked_mse(y_pred, y, mask):\n",
    "    masked_y_pred = y_pred.clone()\n",
    "    masked_y = y.clone()\n",
    "    masked_y_pred[~mask] = torch.nan\n",
    "    masked_y[~mask] = torch.nan\n",
    "    se = F.mse_loss(masked_y_pred, masked_y, reduction=\"none\")\n",
    "    se = se.reshape(se.shape[0], -1)\n",
    "    mse = torch.nanmean(se, dim=1)\n",
    "    return mse\n",
    "\n",
    "\n",
    "def validate_stage(\n",
    "    fabric,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    val_dataloader,\n",
    "    step: int,\n",
    "    epoch: int,\n",
    "    aim_run,\n",
    "    val_viz_subj_id,\n",
    "):\n",
    "    encoder_was_training = encoder.training\n",
    "    decoder_was_training = decoder.training\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        # Set up validation metrics to track for this validation run.\n",
    "        val_metrics = {\"mse\": list()}\n",
    "        for batch_dict in val_dataloader:\n",
    "            subj_id = batch_dict[\"subj_id\"]\n",
    "            if len(subj_id) == 1:\n",
    "                subj_id = subj_id[0]\n",
    "            if val_viz_subj_id is None:\n",
    "                val_viz_subj_id = subj_id\n",
    "\n",
    "            x = batch_dict[\"lr_dwi\"]\n",
    "            batch_size = x.shape[0]\n",
    "            x_affine_vox2real = batch_dict[\"affine_lr_vox2real\"].to(x.dtype)\n",
    "            x_spacing = batch_dict[\"lr_spacing\"]\n",
    "            x_coords = einops.rearrange(\n",
    "                batch_dict[\"lr_real_coords\"], \"b coord x y z -> b x y z coord\"\n",
    "            )\n",
    "\n",
    "            y = batch_dict[\"odf\"]\n",
    "            y_mask = batch_dict[\"brain_mask\"].bool()\n",
    "            y_spacing = batch_dict[\"full_res_spacing\"]\n",
    "            y_coords = einops.rearrange(\n",
    "                batch_dict[\"full_res_real_coords\"], \"b coord x y z -> b x y z coord\"\n",
    "            )\n",
    "\n",
    "            if batch_size == 1:\n",
    "                if x_coords.shape[0] != 1:\n",
    "                    x_coords.unsqueeze_(0)\n",
    "                if y_coords.shape[0] != 1:\n",
    "                    y_coords.unsqueeze_(0)\n",
    "\n",
    "            # Append LR coordinates to the end of the input LR DWIs.\n",
    "            x = torch.cat(\n",
    "                [x, einops.rearrange(x_coords, \"b x y z coord -> b coord x y z\")], dim=1\n",
    "            )\n",
    "            ctx_v = encoder(x)\n",
    "\n",
    "            # Whole-volume inference is memory-prohibitive, so use a sliding\n",
    "            # window inference method on the encoded volume.\n",
    "            # Transform y_coords into a coordinates-first shape, for the interface, and\n",
    "            # attach the mask for compatibility with the sliding inference function.\n",
    "            y_slide_window = torch.cat(\n",
    "                [\n",
    "                    einops.rearrange(y_coords, \"b x y z coord -> b coord x y z\"),\n",
    "                    y_mask.to(y_coords),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "            fn_coordify = lambda x: einops.rearrange(\n",
    "                x, \"b coord x y z -> b x y z coord\"\n",
    "            )\n",
    "\n",
    "            pred_fodf = monai.inferers.sliding_window_inference(\n",
    "                y_slide_window,\n",
    "                roi_size=(52, 52, 52),\n",
    "                sw_batch_size=y_coords.shape[0],\n",
    "                predictor=lambda q: decoder(\n",
    "                    # Rearrange back into coord-last format.\n",
    "                    query_real_coords=fn_coordify(q[:, :-1]),\n",
    "                    query_coords_mask=fn_coordify(q[:, -1:].bool()),\n",
    "                    context_v=ctx_v,\n",
    "                    context_real_coords=x_coords,\n",
    "                    affine_context_vox2real=x_affine_vox2real,\n",
    "                    context_spacing=x_spacing,\n",
    "                    query_spacing=y_spacing,\n",
    "                ),\n",
    "                overlap=0,\n",
    "                padding_mode=\"replicate\",\n",
    "            )\n",
    "\n",
    "            y_mask_broad = torch.broadcast_to(y_mask, y.shape)\n",
    "            # Calculate performance metrics\n",
    "            mse_loss = batchwise_masked_mse(pred_fodf, y, mask=y_mask_broad)\n",
    "            val_metrics[\"mse\"].append(mse_loss.detach().cpu().flatten())\n",
    "\n",
    "            # If visualization subj_id is in this batch, create the visual and log it.\n",
    "            if subj_id == val_viz_subj_id:\n",
    "                with mpl.rc_context({\"font.size\": 6.0}):\n",
    "                    fig = plt.figure(dpi=175, figsize=(10, 6))\n",
    "                    # Reorient from RAS to IPR for visualization purposes.\n",
    "                    ipr_pred_fodf = einops.rearrange(\n",
    "                        pred_fodf, \"b c x y z -> b c z y x\"\n",
    "                    ).flip(2, 3)\n",
    "                    ipr_y = einops.rearrange(y, \"b c x y z -> b c z y x\").flip(2, 3)\n",
    "                    ipr_y_mask_broad = einops.rearrange(\n",
    "                        y_mask_broad, \"b c x y z -> b c z y x\"\n",
    "                    ).flip(2, 3)\n",
    "                    # Select a mix of zonal and non-zonal harmonics for viz.\n",
    "                    fodf_coeff_idx = (0, 4, 8, 26, 30)\n",
    "                    h_degrees = list(range(0, 9, 2))\n",
    "                    zh_coeff_idx = (0, 3, 10, 21, 36)\n",
    "                    # Generate row headers.\n",
    "                    row_headers = list()\n",
    "                    for i in range(len(fodf_coeff_idx)):\n",
    "                        coeff_idx = fodf_coeff_idx[i]\n",
    "                        deg = h_degrees[i]\n",
    "                        order = int(coeff_idx - zh_coeff_idx[i])\n",
    "                        row_headers.append(f\"Deg {deg} order {order}\")\n",
    "\n",
    "                    fig = pitn.viz.plot_fodf_coeff_slices(\n",
    "                        ipr_pred_fodf,\n",
    "                        ipr_y,\n",
    "                        torch.abs(ipr_pred_fodf - ipr_y) * ipr_y_mask_broad,\n",
    "                        # pred_fodf,\n",
    "                        # y,\n",
    "                        # torch.abs(pred_fodf - y) * y_mask_broad,\n",
    "                        slice_idx=(0.5, 0.55, 0.45),\n",
    "                        col_headers=(\"Predicted\",) * 3\n",
    "                        + (\"Target\",) * 3\n",
    "                        + (\"|Pred - GT|\",) * 3,\n",
    "                        fodf_coeff_idx=fodf_coeff_idx,\n",
    "                        # row_headers=[f\"z-harm deg {i}\" for i in range(0, 9, 2)],\n",
    "                        row_headers=row_headers,\n",
    "                        colorbars=\"rows\",\n",
    "                        fig=fig,\n",
    "                        interpolation=\"nearest\",\n",
    "                        cmap=\"gray\",\n",
    "                    )\n",
    "\n",
    "                    aim_run.track(\n",
    "                        aim.Image(\n",
    "                            fig,\n",
    "                            caption=f\"Val Subj {subj_id}, \"\n",
    "                            + f\"MSE = {val_metrics['mse'][-1].item()}\",\n",
    "                            optimize=True,\n",
    "                            quality=100,\n",
    "                            format=\"png\",\n",
    "                        ),\n",
    "                        name=\"sh_whole_volume\",\n",
    "                        context={\"subset\": \"val\"},\n",
    "                        epoch=epoch,\n",
    "                        step=step,\n",
    "                    )\n",
    "                    plt.close(fig)\n",
    "                    del ipr_pred_fodf, ipr_y, ipr_y_mask_broad\n",
    "\n",
    "                # Plot MSE as distributed over the SH orders.\n",
    "                sh_coeff_labels = {\n",
    "                    \"idx\": list(range(0, 45)),\n",
    "                    \"l\": np.concatenate(\n",
    "                        list(\n",
    "                            map(\n",
    "                                lambda x: np.array([x] * (2 * x + 1)),\n",
    "                                range(0, 9, 2),\n",
    "                            )\n",
    "                        ),\n",
    "                        dtype=int,\n",
    "                    ).flatten(),\n",
    "                }\n",
    "                error_fodf = F.mse_loss(pred_fodf, y, reduction=\"none\")\n",
    "                error_fodf = einops.rearrange(\n",
    "                    error_fodf, \"b sh_idx x y z -> b x y z sh_idx\"\n",
    "                )\n",
    "                error_fodf = error_fodf[\n",
    "                    y_mask[:, 0, ..., None].broadcast_to(error_fodf.shape)\n",
    "                ]\n",
    "                error_fodf = einops.rearrange(\n",
    "                    error_fodf, \"(elem sh_idx) -> elem sh_idx\", sh_idx=45\n",
    "                )\n",
    "                error_fodf = error_fodf.flatten().detach().cpu().numpy()\n",
    "                error_df = pd.DataFrame.from_dict(\n",
    "                    {\n",
    "                        \"MSE\": error_fodf,\n",
    "                        \"SH_idx\": np.tile(\n",
    "                            sh_coeff_labels[\"idx\"], error_fodf.shape[0] // 45\n",
    "                        ),\n",
    "                        \"L Order\": np.tile(\n",
    "                            sh_coeff_labels[\"l\"], error_fodf.shape[0] // 45\n",
    "                        ),\n",
    "                    }\n",
    "                )\n",
    "                with mpl.rc_context({\"font.size\": 6.0}):\n",
    "                    fig = plt.figure(dpi=100, figsize=(6, 2))\n",
    "                    sns.boxplot(\n",
    "                        data=error_df,\n",
    "                        x=\"SH_idx\",\n",
    "                        y=\"MSE\",\n",
    "                        hue=\"L Order\",\n",
    "                        linewidth=0.8,\n",
    "                        showfliers=False,\n",
    "                        width=0.85,\n",
    "                        dodge=False,\n",
    "                    )\n",
    "                    aim_run.track(\n",
    "                        aim.Image(fig, caption=\"MSE over SH orders\", optimize=True),\n",
    "                        name=\"mse_over_sh_orders\",\n",
    "                        epoch=epoch,\n",
    "                        step=step,\n",
    "                    )\n",
    "                    plt.close(fig)\n",
    "            fabric.print(f\"MSE {val_metrics['mse'][-1].item()}\")\n",
    "            fabric.print(\"Finished validation subj \", subj_id)\n",
    "            del pred_fodf\n",
    "\n",
    "    val_metrics[\"mse\"] = torch.cat(val_metrics[\"mse\"])\n",
    "    # Log metrics\n",
    "    aim_run.track(\n",
    "        {\"mse\": val_metrics[\"mse\"].mean().numpy()},\n",
    "        context={\"subset\": \"val\"},\n",
    "        step=step,\n",
    "        epoch=epoch,\n",
    "    )\n",
    "\n",
    "    encoder.train(mode=encoder_was_training)\n",
    "    decoder.train(mode=decoder_was_training)\n",
    "    return aim_run, val_viz_subj_id, val_metrics[\"mse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "face5de2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "ts = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "# Break ISO format because many programs don't like having colons ':' in a filename.\n",
    "ts = ts.replace(\":\", \"_\")\n",
    "tmp_res_dir = Path(p.tmp_results_dir) / ts\n",
    "tmp_res_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e24d732",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mambaforge/envs/pitn/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning:\n",
      "\n",
      "Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experiment_name': 'test_FENRI_preproc_rewrite', 'override_experiment_name': False, 'results_dir': '/data/srv/outputs/pitn/results/runs', 'tmp_results_dir': '/data/srv/outputs/pitn/results/tmp', 'train_val_test_split_file': PosixPath('data_splits/HCP_train-val-test_split_03.1.csv'), 'aim_logger': {'repo': 'aim://dali.cpe.virginia.edu:53800', 'experiment': 'PITN_FENRI', 'meta_params': {'run_name': 'test_FENRI_preproc_rewrite', 'hparams': {'batch_size': 6, 'patch_spatial_size': (36, 36, 36), 'samples_per_subj_per_epoch': 100, 'max_epochs': 40}, 'data': {'train_subj_ids': [100206, 102109, 104820, 105115, 107422, 114419, 118124, 118730, 119833, 127832, 141422, 151425, 158540, 159946, 160931, 171532, 183741, 186545, 208226, 211922, 217429, 250427, 361941, 481042, 485757, 492754, 516742, 647858, 680452, 735148, 748662, 788674, 802844, 809252, 845458], 'val_subj_ids': [121719, 156637, 168341, 627852, 993675], 'test_subj_ids': [106824, 144125, 163331, 172433, 200513, 213522, 236130, 245333, 248339, 250932, 255639, 256540, 300719, 376247, 379657, 459453, 469961, 480141, 562446, 566454, 580751, 654754, 677968, 715647, 727654, 773257, 872562, 911849, 991267]}}, 'tags': ('PITN', 'INR', 'HCP', 'super-res', 'dMRI', 'FENRI')}, 'checkpoint_epoch_ratios': (0.5,), 'preproc_loaded': {'S0_noise_b0_quantile': 0.99, 'patch_sampling_w_erosion': 17}, 'baseline_lr_spacing_scale': 1.6, 'baseline_snr': 30, 'train': {'patch_size': (36, 36, 36), 'batch_size': 6, 'samples_per_subj_per_epoch': 100, 'max_epochs': 40, 'dwi_recon_epoch_proportion': 0.010101010101010102, 'patch_sampling': {'rng': 'default'}, 'patch_tf': {'downsample_factor_range': (1.6, 2.0), 'noise_snr_range': (30, 35), 'prefilter_sigma_scale_coeff': 2.0, 'rng': 'default'}, 'optim': {'encoder': {'lr': 0.0005}, 'decoder': {'lr': 0.0005}, 'recon_decoder': {'lr': 0.001}}, 'dataloader': {'num_workers': 16, 'persistent_workers': True, 'prefetch_factor': 3}, 'subj_ids': [100206, 102109, 104820, 105115, 107422, 114419, 118124, 118730, 119833, 127832, 141422, 151425, 158540, 159946, 160931, 171532, 183741, 186545, 208226, 211922, 217429, 250427, 361941, 481042, 485757, 492754, 516742, 647858, 680452, 735148, 748662, 788674, 802844, 809252, 845458]}, 'encoder': {'interior_channels': 80, 'out_channels': 96, 'n_res_units': 3, 'n_dense_units': 3, 'activate_fn': 'relu', 'input_coord_channels': True, 'post_batch_norm': True}, 'decoder': {'context_v_features': 96, 'out_features': 45, 'm_encode_num_freqs': 36, 'sigma_encode_scale': 3.0, 'n_internal_features': 256, 'n_internal_layers': 3}, 'val': {'rng_seed': 3967417599011123030, 'vol_tf': {'downsample_factor_range': (1.6, 1.6), 'noise_snr_range': (30, 30), 'prefilter_sigma_scale_coeff': 2.0, 'manual_crop_lr_sides': ((1, 1), (1, 1), (1, 1))}, 'subj_ids': [121719, 156637, 168341, 627852, 993675]}, 'test': {'subj_ids': [106824, 144125, 163331, 172433, 200513, 213522, 236130, 245333, 248339, 250932, 255639, 256540, 300719, 376247, 379657, 459453, 469961, 480141, 562446, 566454, 580751, 654754, 677968, 715647, 727654, 773257, 872562, 911849, 991267]}}\n",
      "INREncoder(\n",
      "  (activate_fn): ReLU()\n",
      "  (pre_conv): Sequential(\n",
      "    (0): Conv3d(102, 102, kernel_size=(1, 1, 1), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "    (1): ReLU()\n",
      "    (2): Conv3d(102, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "  )\n",
      "  (cascade): DenseCascadeBlock3d(\n",
      "    (base_layers): ModuleList(\n",
      "      (0-2): 3 x DenseCascadeBlock3d(\n",
      "        (base_layers): ModuleList(\n",
      "          (0-2): 3 x ResBlock3dNoBN(\n",
      "            (conv1): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (conv2): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (active_fn): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (combiner_convs): ModuleList(\n",
      "          (0): Conv3d(160, 80, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "          (1): Conv3d(240, 80, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "          (2): Conv3d(320, 80, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (combiner_convs): ModuleList(\n",
      "      (0): Conv3d(160, 80, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (1): Conv3d(240, 80, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (2): Conv3d(320, 80, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "    )\n",
      "  )\n",
      "  (post_conv): Sequential(\n",
      "    (0): Conv3d(80, 80, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "    (1): ReLU()\n",
      "    (2): Conv3d(80, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "    (3): ReLU()\n",
      "    (4): ReplicationPad3d((1, 0, 1, 0, 1, 0))\n",
      "    (5): AvgPool3d(kernel_size=2, stride=1, padding=0)\n",
      "    (6): Conv3d(96, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "  )\n",
      "  (output_batch_norm): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "SimplifiedDecoder(\n",
      "  (activate_fn): SiLU(inplace=True)\n",
      "  (linear_layers): Sequential(\n",
      "    (0): Linear(in_features=318, out_features=256, bias=True)\n",
      "    (1): SiLU(inplace=True)\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): SiLU(inplace=True)\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (5): SiLU(inplace=True)\n",
      "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (7): SiLU(inplace=True)\n",
      "    (8): Linear(in_features=256, out_features=45, bias=True)\n",
      "  )\n",
      ")\n",
      "INREncoder(\n",
      "  (activate_fn): ReLU()\n",
      "  (pre_conv): Sequential(\n",
      "    (0): Conv3d(96, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "    (1): ReLU()\n",
      "    (2): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "  )\n",
      "  (cascade): DenseCascadeBlock3d(\n",
      "    (base_layers): ModuleList(\n",
      "      (0-1): 2 x DenseCascadeBlock3d(\n",
      "        (base_layers): ModuleList(\n",
      "          (0-1): 2 x ResBlock3dNoBN(\n",
      "            (conv1): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (conv2): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (active_fn): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (combiner_convs): ModuleList(\n",
      "          (0): Conv3d(96, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "          (1): Conv3d(144, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (combiner_convs): ModuleList(\n",
      "      (0): Conv3d(96, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (1): Conv3d(144, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "    )\n",
      "  )\n",
      "  (post_conv): Sequential(\n",
      "    (0): Conv3d(48, 48, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "    (1): ReLU()\n",
      "    (2): Conv3d(48, 9, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "    (3): ReLU()\n",
      "    (4): ReplicationPad3d((1, 0, 1, 0, 1, 0))\n",
      "    (5): AvgPool3d(kernel_size=2, stride=1, padding=0)\n",
      "    (6): Conv3d(9, 9, kernel_size=(1, 1, 1), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "  )\n",
      "  (output_batch_norm): Identity()\n",
      ")\n",
      "Encoder num params: 4591146\n",
      "Decoder num params: 290605\n",
      "Recon decoder num params: 966483\n",
      "\n",
      "Epoch 0\n",
      " ==========\n",
      "| 5.741753578186035 | 3.686112403869629 | 2.125011920928955 | 1.733633041381836 | 1.3836324214935303 | 1.5747408866882324 | 1.3050378561019897 | 1.122988224029541 | 1.1518691778182983 | 1.0344538688659668 | 1.0297590494155884 | 0.9530758261680603 | 1.0297454595565796 | 0.9872437715530396 | 1.0842366218566895 | 0.8100681304931641 | 0.9703553318977356 | 0.8718632459640503 | 0.9096095561981201 | 0.8004576563835144 | 0.710759699344635 | 0.9172687530517578 | 0.6493791341781616 | 0.7616626620292664 | 0.7507967352867126 | 0.8313857316970825 | 0.8247262835502625 | 0.6628283262252808 | 0.8493858575820923 | 0.6399852633476257 | 0.739933967590332 | 0.7356434464454651 | 0.7506338953971863 | 0.8228006362915039 \n",
      "==Validation==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mambaforge/envs/pitn/lib/python3.10/site-packages/aim/sdk/objects/image.py:257: UserWarning:\n",
      "\n",
      "This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.0013913540169596672\n",
      "Finished validation subj  121719\n",
      "MSE 0.0014154238160699606\n",
      "Finished validation subj  156637\n",
      "0.001403388916514814\n",
      "\n",
      "Epoch 1\n",
      " ==========\n",
      "| 0.8414884209632874 | 0.701795756816864 | 0.6678855419158936 | 0.7947455644607544 | 0.8149698376655579 | 0.8074823617935181 | 0.7506694793701172 | 0.8219672441482544 | 0.7458412647247314 | 0.6770784854888916 | 0.7551950812339783 | 0.7058492302894592 | 0.6273735761642456 | 0.7935634255409241 | 0.8204122185707092 | 0.7145586609840393 | 0.6780732870101929 | 0.7831396460533142 | 0.7505689263343811 | 0.6943125128746033 | 0.7384729981422424 | 0.5421141982078552 | 0.700857400894165 | 0.6401545405387878 | 0.7216776609420776 | 0.5929803848266602 | 0.7416458129882812 | 0.6271793246269226 | 0.5627123117446899 | 0.5944715142250061 | 0.6540064811706543 | 0.7908726930618286 | 0.7464737892150879 | 0.551296055316925 \n",
      "==Validation==\n",
      "MSE 0.0012558887246996164\n",
      "Finished validation subj  156637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mambaforge/envs/pitn/lib/python3.10/site-packages/aim/sdk/objects/image.py:257: UserWarning:\n",
      "\n",
      "This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.001251955982297659\n",
      "Finished validation subj  121719\n",
      "0.0012539223534986377\n",
      "\n",
      "Epoch 2\n",
      " ==========\n",
      "| 0.6830394864082336 | 0.6467061638832092 | 0.8460617065429688 | 0.7161509990692139 | 0.6574587225914001 | 0.7080872058868408 | 0.7813021540641785 | 0.7345115542411804 | 0.725163996219635 | 0.7687106728553772 | 0.6188287138938904 | 0.7428200840950012 | 0.6886027455329895 | 0.6882544755935669 | 0.6656602025032043 | 0.6855745911598206 | 0.8440973162651062 | 0.7339818477630615 | 0.7715484499931335 | 0.721413791179657 | 0.6893589496612549 | 0.723884105682373 | 0.5716910362243652 | 0.7231716513633728 | 0.6737591624259949 | 0.6907608509063721 | 0.6102041006088257 | 0.8371809124946594 | 0.8326345682144165 | 0.8491932153701782 | 0.6202498078346252 | 0.729773998260498 | 0.6765401363372803 | 0.7731353044509888 \n",
      "==Validation==\n",
      "MSE 0.0012785998405888677\n",
      "Finished validation subj  156637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mambaforge/envs/pitn/lib/python3.10/site-packages/aim/sdk/objects/image.py:257: UserWarning:\n",
      "\n",
      "This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb Cell 29\u001b[0m line \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=364'>365</a>\u001b[0m         aim_run\u001b[39m.\u001b[39madd_tag(\u001b[39m\"\u001b[39m\u001b[39mSTOPPED\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=365'>366</a>\u001b[0m         (tmp_res_dir \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSTOPPED\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mtouch()\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=366'>367</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=367'>368</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=368'>369</a>\u001b[0m     \u001b[39mif\u001b[39;00m fabric\u001b[39m.\u001b[39mis_global_zero:\n",
      "\u001b[1;32m/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb Cell 29\u001b[0m line \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=297'>298</a>\u001b[0m fabric\u001b[39m.\u001b[39mbarrier()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=298'>299</a>\u001b[0m \u001b[39mif\u001b[39;00m fabric\u001b[39m.\u001b[39mis_global_zero:\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=299'>300</a>\u001b[0m     aim_run, val_viz_subj_id, val_scores \u001b[39m=\u001b[39m validate_stage(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=300'>301</a>\u001b[0m         fabric,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=301'>302</a>\u001b[0m         encoder,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=302'>303</a>\u001b[0m         decoder,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=303'>304</a>\u001b[0m         val_dataloader\u001b[39m=\u001b[39;49mval_dataloader,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=304'>305</a>\u001b[0m         step\u001b[39m=\u001b[39;49mstep,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=305'>306</a>\u001b[0m         epoch\u001b[39m=\u001b[39;49mepoch,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=306'>307</a>\u001b[0m         aim_run\u001b[39m=\u001b[39;49maim_run,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=307'>308</a>\u001b[0m         val_viz_subj_id\u001b[39m=\u001b[39;49mval_viz_subj_id,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=308'>309</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=309'>310</a>\u001b[0m     curr_val_score \u001b[39m=\u001b[39m val_scores\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mitem()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=310'>311</a>\u001b[0m     fabric\u001b[39m.\u001b[39mprint(\u001b[39mstr\u001b[39m(curr_val_score))\n",
      "\u001b[1;32m/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb Cell 29\u001b[0m line \u001b[0;36mvalidate_stage\u001b[0;34m(fabric, encoder, decoder, val_dataloader, step, epoch, aim_run, val_viz_subj_id)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=211'>212</a>\u001b[0m \u001b[39mwith\u001b[39;00m mpl\u001b[39m.\u001b[39mrc_context({\u001b[39m\"\u001b[39m\u001b[39mfont.size\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m6.0\u001b[39m}):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=212'>213</a>\u001b[0m     fig \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mfigure(dpi\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m6\u001b[39m, \u001b[39m2\u001b[39m))\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=213'>214</a>\u001b[0m     sns\u001b[39m.\u001b[39;49mboxplot(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=214'>215</a>\u001b[0m         data\u001b[39m=\u001b[39;49merror_df,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=215'>216</a>\u001b[0m         x\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mSH_idx\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=216'>217</a>\u001b[0m         y\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mMSE\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=217'>218</a>\u001b[0m         hue\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mL Order\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=218'>219</a>\u001b[0m         linewidth\u001b[39m=\u001b[39;49m\u001b[39m0.8\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=219'>220</a>\u001b[0m         showfliers\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=220'>221</a>\u001b[0m         width\u001b[39m=\u001b[39;49m\u001b[39m0.85\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=221'>222</a>\u001b[0m         dodge\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=222'>223</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=223'>224</a>\u001b[0m     aim_run\u001b[39m.\u001b[39mtrack(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=224'>225</a>\u001b[0m         aim\u001b[39m.\u001b[39mImage(fig, caption\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMSE over SH orders\u001b[39m\u001b[39m\"\u001b[39m, optimize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=225'>226</a>\u001b[0m         name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmse_over_sh_orders\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=226'>227</a>\u001b[0m         epoch\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=227'>228</a>\u001b[0m         step\u001b[39m=\u001b[39mstep,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=228'>229</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/fenri.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=229'>230</a>\u001b[0m     plt\u001b[39m.\u001b[39mclose(fig)\n",
      "File \u001b[0;32m/opt/mambaforge/envs/pitn/lib/python3.10/site-packages/seaborn/categorical.py:2231\u001b[0m, in \u001b[0;36mboxplot\u001b[0;34m(data, x, y, hue, order, hue_order, orient, color, palette, saturation, width, dodge, fliersize, linewidth, whis, ax, **kwargs)\u001b[0m\n\u001b[1;32m   2224\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mboxplot\u001b[39m(\n\u001b[1;32m   2225\u001b[0m     data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m, x\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, hue\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, order\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, hue_order\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2226\u001b[0m     orient\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, color\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, palette\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, saturation\u001b[39m=\u001b[39m\u001b[39m.75\u001b[39m, width\u001b[39m=\u001b[39m\u001b[39m.8\u001b[39m,\n\u001b[1;32m   2227\u001b[0m     dodge\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, fliersize\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, linewidth\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, whis\u001b[39m=\u001b[39m\u001b[39m1.5\u001b[39m, ax\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2228\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m   2229\u001b[0m ):\n\u001b[0;32m-> 2231\u001b[0m     plotter \u001b[39m=\u001b[39m _BoxPlotter(x, y, hue, data, order, hue_order,\n\u001b[1;32m   2232\u001b[0m                           orient, color, palette, saturation,\n\u001b[1;32m   2233\u001b[0m                           width, dodge, fliersize, linewidth)\n\u001b[1;32m   2235\u001b[0m     \u001b[39mif\u001b[39;00m ax \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2236\u001b[0m         ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mgca()\n",
      "File \u001b[0;32m/opt/mambaforge/envs/pitn/lib/python3.10/site-packages/seaborn/categorical.py:785\u001b[0m, in \u001b[0;36m_BoxPlotter.__init__\u001b[0;34m(self, x, y, hue, data, order, hue_order, orient, color, palette, saturation, width, dodge, fliersize, linewidth)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, x, y, hue, data, order, hue_order,\n\u001b[1;32m    782\u001b[0m              orient, color, palette, saturation,\n\u001b[1;32m    783\u001b[0m              width, dodge, fliersize, linewidth):\n\u001b[0;32m--> 785\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestablish_variables(x, y, hue, data, orient, order, hue_order)\n\u001b[1;32m    786\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestablish_colors(color, palette, saturation)\n\u001b[1;32m    788\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdodge \u001b[39m=\u001b[39m dodge\n",
      "File \u001b[0;32m/opt/mambaforge/envs/pitn/lib/python3.10/site-packages/seaborn/categorical.py:608\u001b[0m, in \u001b[0;36m_CategoricalPlotter.establish_variables\u001b[0;34m(self, x, y, hue, data, orient, order, hue_order, units)\u001b[0m\n\u001b[1;32m    605\u001b[0m     hue_names \u001b[39m=\u001b[39m categorical_order(hue, hue_order)\n\u001b[1;32m    607\u001b[0m     \u001b[39m# Group the hue data\u001b[39;00m\n\u001b[0;32m--> 608\u001b[0m     plot_hues, hue_title \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_group_longform(hue, groups,\n\u001b[1;32m    609\u001b[0m                                                 group_names)\n\u001b[1;32m    611\u001b[0m \u001b[39m# Now handle the units for nested observations\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[39mif\u001b[39;00m units \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/mambaforge/envs/pitn/lib/python3.10/site-packages/seaborn/categorical.py:645\u001b[0m, in \u001b[0;36m_CategoricalPlotter._group_longform\u001b[0;34m(self, vals, grouper, order)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m order:\n\u001b[1;32m    644\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 645\u001b[0m         g_vals \u001b[39m=\u001b[39m grouped_vals\u001b[39m.\u001b[39;49mget_group(g)\n\u001b[1;32m    646\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m    647\u001b[0m         g_vals \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([])\n",
      "File \u001b[0;32m/opt/mambaforge/envs/pitn/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:809\u001b[0m, in \u001b[0;36mBaseGroupBy.get_group\u001b[0;34m(self, name, obj)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    807\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_selected_obj\n\u001b[0;32m--> 809\u001b[0m inds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_index(name)\n\u001b[1;32m    810\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(inds):\n\u001b[1;32m    811\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(name)\n",
      "File \u001b[0;32m/opt/mambaforge/envs/pitn/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:737\u001b[0m, in \u001b[0;36mBaseGroupBy._get_index\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[1;32m    733\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_index\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[1;32m    734\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    735\u001b[0m \u001b[39m    Safe get index, translate keys for datelike to underlying repr.\u001b[39;00m\n\u001b[1;32m    736\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_indices([name])[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/mambaforge/envs/pitn/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:701\u001b[0m, in \u001b[0;36mBaseGroupBy._get_indices\u001b[0;34m(self, names)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(names) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m []\n\u001b[0;32m--> 701\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    702\u001b[0m     index_sample \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices))\n\u001b[1;32m    703\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/mambaforge/envs/pitn/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:679\u001b[0m, in \u001b[0;36mBaseGroupBy.indices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[1;32m    674\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mindices\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m[Hashable, npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39mintp]]:\n\u001b[1;32m    676\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[39m    Dict {group name -> group indices}.\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 679\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrouper\u001b[39m.\u001b[39;49mindices\n",
      "File \u001b[0;32m/opt/mambaforge/envs/pitn/lib/python3.10/site-packages/pandas/_libs/properties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/mambaforge/envs/pitn/lib/python3.10/site-packages/pandas/core/groupby/ops.py:865\u001b[0m, in \u001b[0;36mBaseGrouper.indices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    863\u001b[0m codes_list \u001b[39m=\u001b[39m [ping\u001b[39m.\u001b[39mcodes \u001b[39mfor\u001b[39;00m ping \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroupings]\n\u001b[1;32m    864\u001b[0m keys \u001b[39m=\u001b[39m [ping\u001b[39m.\u001b[39mgroup_index \u001b[39mfor\u001b[39;00m ping \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroupings]\n\u001b[0;32m--> 865\u001b[0m \u001b[39mreturn\u001b[39;00m get_indexer_dict(codes_list, keys)\n",
      "File \u001b[0;32m/opt/mambaforge/envs/pitn/lib/python3.10/site-packages/pandas/core/sorting.py:629\u001b[0m, in \u001b[0;36mget_indexer_dict\u001b[0;34m(label_list, keys)\u001b[0m\n\u001b[1;32m    621\u001b[0m ngroups \u001b[39m=\u001b[39m (\n\u001b[1;32m    622\u001b[0m     ((group_index\u001b[39m.\u001b[39msize \u001b[39mand\u001b[39;00m group_index\u001b[39m.\u001b[39mmax()) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m    623\u001b[0m     \u001b[39mif\u001b[39;00m is_int64_overflow_possible(shape)\n\u001b[1;32m    624\u001b[0m     \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mprod(shape, dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mi8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    625\u001b[0m )\n\u001b[1;32m    627\u001b[0m sorter \u001b[39m=\u001b[39m get_group_index_sorter(group_index, ngroups)\n\u001b[0;32m--> 629\u001b[0m sorted_labels \u001b[39m=\u001b[39m [lab\u001b[39m.\u001b[39mtake(sorter) \u001b[39mfor\u001b[39;00m lab \u001b[39min\u001b[39;00m label_list]\n\u001b[1;32m    630\u001b[0m group_index \u001b[39m=\u001b[39m group_index\u001b[39m.\u001b[39mtake(sorter)\n\u001b[1;32m    632\u001b[0m \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39mindices_fast(sorter, group_index, keys, sorted_labels)\n",
      "File \u001b[0;32m/opt/mambaforge/envs/pitn/lib/python3.10/site-packages/pandas/core/sorting.py:629\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    621\u001b[0m ngroups \u001b[39m=\u001b[39m (\n\u001b[1;32m    622\u001b[0m     ((group_index\u001b[39m.\u001b[39msize \u001b[39mand\u001b[39;00m group_index\u001b[39m.\u001b[39mmax()) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m    623\u001b[0m     \u001b[39mif\u001b[39;00m is_int64_overflow_possible(shape)\n\u001b[1;32m    624\u001b[0m     \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mprod(shape, dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mi8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    625\u001b[0m )\n\u001b[1;32m    627\u001b[0m sorter \u001b[39m=\u001b[39m get_group_index_sorter(group_index, ngroups)\n\u001b[0;32m--> 629\u001b[0m sorted_labels \u001b[39m=\u001b[39m [lab\u001b[39m.\u001b[39;49mtake(sorter) \u001b[39mfor\u001b[39;00m lab \u001b[39min\u001b[39;00m label_list]\n\u001b[1;32m    630\u001b[0m group_index \u001b[39m=\u001b[39m group_index\u001b[39m.\u001b[39mtake(sorter)\n\u001b[1;32m    632\u001b[0m \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39mindices_fast(sorter, group_index, keys, sorted_labels)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fabric = lightning.Fabric(accelerator=\"gpu\", devices=1, precision=32)\n",
    "# fabric = lightning.Fabric(accelerator=\"cpu\", devices=1, precision=32)\n",
    "fabric.launch()\n",
    "device = fabric.device\n",
    "\n",
    "aim_run = setup_logger_run(\n",
    "    run_kwargs={\n",
    "        k: p.aim_logger[k] for k in set(p.aim_logger.keys()) - {\"meta_params\", \"tags\"}\n",
    "    },\n",
    "    logger_meta_params=p.aim_logger.meta_params.to_dict(),\n",
    "    logger_tags=p.aim_logger.tags,\n",
    ")\n",
    "if \"in_channels\" not in p.encoder:\n",
    "    in_channels = int(train_dataset[0][\"lr_dwi\"].shape[0]) + 3\n",
    "else:\n",
    "    in_channels = p.encoder.in_channels\n",
    "\n",
    "# Wrap the entire training & validation loop in a try...except statement.\n",
    "try:\n",
    "\n",
    "    LOSS_ODF_COEFF_MEANS = torch.from_numpy(\n",
    "        np.array([0.17] + [0.002] * 5 + [0.002] * 9 + [0.0] * 13 + [0.0] * 17)\n",
    "    )\n",
    "    LOSS_ODF_COEFF_STDS = torch.from_numpy(\n",
    "        np.array([0.05] + [0.1] * 5 + [0.06] * 9 + [0.03] * 13 + [0.01] * 17)\n",
    "    )\n",
    "    LOSS_ODF_COEFF_MEANS = LOSS_ODF_COEFF_MEANS[None, :, None, None, None].to(device)\n",
    "    LOSS_ODF_COEFF_STDS = LOSS_ODF_COEFF_STDS[None, :, None, None, None].to(device)\n",
    "\n",
    "    encoder = INREncoder(**{**p.encoder.to_dict(), **{\"in_channels\": in_channels}})\n",
    "    # Initialize weight shape for the encoder.\n",
    "    with torch.no_grad():\n",
    "        encoder(torch.randn(1, in_channels, 20, 20, 20))\n",
    "    decoder = SimplifiedDecoder(**p.decoder.to_dict())\n",
    "\n",
    "    decoder = SimplifiedDecoder(**p.decoder.to_dict())\n",
    "    recon_decoder = INREncoder(\n",
    "        in_channels=encoder.out_channels,\n",
    "        interior_channels=48,\n",
    "        out_channels=9,\n",
    "        n_res_units=2,\n",
    "        n_dense_units=2,\n",
    "        activate_fn=p.encoder.activate_fn,\n",
    "        input_coord_channels=False,\n",
    "    )\n",
    "    # Initialize weight shape for the recon decoder.\n",
    "    with torch.no_grad():\n",
    "        recon_decoder(torch.randn(1, recon_decoder.in_channels, 20, 20, 20))\n",
    "    fabric.print(p.to_dict())\n",
    "    fabric.print(encoder)\n",
    "    fabric.print(decoder)\n",
    "    fabric.print(recon_decoder)\n",
    "    fabric.print(\"Encoder num params:\", sum([p.numel() for p in encoder.parameters()]))\n",
    "    fabric.print(\"Decoder num params:\", sum([p.numel() for p in decoder.parameters()]))\n",
    "    fabric.print(\n",
    "        \"Recon decoder num params:\",\n",
    "        sum([p.numel() for p in recon_decoder.parameters()]),\n",
    "    )\n",
    "\n",
    "    optim_encoder = torch.optim.AdamW(\n",
    "        encoder.parameters(), **p.train.optim.encoder.to_dict()\n",
    "    )\n",
    "    encoder, optim_encoder = fabric.setup(encoder, optim_encoder)\n",
    "    optim_decoder = torch.optim.AdamW(\n",
    "        decoder.parameters(), **p.train.optim.decoder.to_dict()\n",
    "    )\n",
    "    decoder, optim_decoder = fabric.setup(decoder, optim_decoder)\n",
    "    optim_recon_decoder = torch.optim.AdamW(\n",
    "        recon_decoder.parameters(), **p.train.optim.recon_decoder.to_dict()\n",
    "    )\n",
    "    recon_decoder, optim_recon_decoder = fabric.setup(\n",
    "        recon_decoder, optim_recon_decoder\n",
    "    )\n",
    "    loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
    "    recon_loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "    # Padding in collation will only occurr in the high-res/target volumes, which are\n",
    "    # processed voxel-wise. So, padding at the end does not change the behavior of the\n",
    "    # conv layers and does not change the vox-to-real coordinate affine transform.\n",
    "    collate_fn = partial(pitn.data.preproc.pad_list_data_collate_tensor, method=\"end\")\n",
    "    train_dataloader = monai.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=p.train.batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn,\n",
    "        **p.train.dataloader.to_dict(),\n",
    "    )\n",
    "    val_dataloader = monai.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=1,\n",
    "        # num_workers=0,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    train_dataloader, val_dataloader = fabric.setup_dataloaders(\n",
    "        train_dataloader, val_dataloader\n",
    "    )\n",
    "    val_viz_subj_id = None\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    recon_decoder.train()\n",
    "    losses = dict(\n",
    "        loss=list(),\n",
    "        epoch=list(),\n",
    "        step=list(),\n",
    "    )\n",
    "    step = 1\n",
    "    train_dwi_recon_epoch_proportion = p.train.dwi_recon_epoch_proportion\n",
    "    train_recon = False\n",
    "\n",
    "    epochs = p.train.max_epochs\n",
    "    curr_best_val_score = 1e8\n",
    "    checkpoint_epochs = np.floor(np.array(p.checkpoint_epoch_ratios) * epochs)\n",
    "    checkpoint_epochs = set(checkpoint_epochs.astype(int).tolist())\n",
    "    curr_checkpoint = 0\n",
    "    for epoch in range(epochs):\n",
    "        fabric.print(f\"\\nEpoch {epoch}\\n\", \"=\" * 10)\n",
    "        if epoch < math.floor(epochs * train_dwi_recon_epoch_proportion):\n",
    "            if not train_recon:\n",
    "                train_recon = True\n",
    "        else:\n",
    "            if train_recon:\n",
    "                train_recon = False\n",
    "                fabric.barrier()\n",
    "                if fabric.is_global_zero:\n",
    "                    torch.save(\n",
    "                        {\n",
    "                            \"epoch\": epoch,\n",
    "                            \"step\": step,\n",
    "                            \"aim_run_hash\": aim_run.hash,\n",
    "                            \"recon_decoder\": recon_decoder.state_dict(),\n",
    "                            \"optim_recon_decoder\": optim_recon_decoder.state_dict(),\n",
    "                        },\n",
    "                        Path(tmp_res_dir)\n",
    "                        / f\"recon_decoder_state_dict_epoch_{epoch}_step_{step}.pt\",\n",
    "                    )\n",
    "                    # Replace the recon network and optimization model with dummies, to\n",
    "                    # release gpu memory.\n",
    "                    del recon_decoder\n",
    "                    del optim_recon_decoder\n",
    "                    recon_decoder = torch.nn.Linear(1, 1, bias=False)\n",
    "                    optim_recon_decoder = torch.optim.SGD(\n",
    "                        recon_decoder.parameters(), 1e-3\n",
    "                    )\n",
    "                    recon_decoder, optim_recon_decoder = fabric.setup(\n",
    "                        recon_decoder, optim_recon_decoder\n",
    "                    )\n",
    "                    fabric.barrier()\n",
    "\n",
    "        for batch_dict in train_dataloader:\n",
    "\n",
    "            x = batch_dict[\"lr_dwi\"]\n",
    "            x_affine_vox2real = batch_dict[\"affine_lr_vox2real\"].to(x.dtype)\n",
    "            x_spacing = batch_dict[\"lr_spacing\"]\n",
    "            # Coordinates must be in coordinate-first shape to be batched by monai's\n",
    "            # collate function, so undo that here.\n",
    "            x_coords = einops.rearrange(\n",
    "                batch_dict[\"lr_real_coords\"], \"b coord x y z -> b x y z coord\"\n",
    "            )\n",
    "\n",
    "            y = batch_dict[\"odf\"]\n",
    "            y_mask = batch_dict[\"brain_mask\"].bool()\n",
    "            y_spacing = batch_dict[\"full_res_spacing\"]\n",
    "            y_coords = einops.rearrange(\n",
    "                batch_dict[\"full_res_real_coords\"], \"b coord x y z -> b x y z coord\"\n",
    "            )\n",
    "\n",
    "            optim_encoder.zero_grad()\n",
    "            optim_decoder.zero_grad()\n",
    "            optim_recon_decoder.zero_grad()\n",
    "\n",
    "            # Append LR coordinates to the end of the input LR DWIs.\n",
    "            x = torch.cat(\n",
    "                [x, einops.rearrange(x_coords, \"b x y z coord -> b coord x y z\")], dim=1\n",
    "            )\n",
    "            ctx_v = encoder(x)\n",
    "\n",
    "            if not train_recon:\n",
    "                y_mask_broad = torch.broadcast_to(y_mask, y.shape)\n",
    "                y_coord_mask = einops.rearrange(y_mask, \"b 1 x y z -> b x y z 1\")\n",
    "                pred_fodf = decoder(\n",
    "                    context_v=ctx_v,\n",
    "                    context_real_coords=x_coords,\n",
    "                    query_real_coords=y_coords,\n",
    "                    query_coords_mask=y_coord_mask,\n",
    "                    affine_context_vox2real=x_affine_vox2real,\n",
    "                    context_spacing=x_spacing,\n",
    "                    query_spacing=y_spacing,\n",
    "                )\n",
    "\n",
    "                # Scale prediction and target to weigh the loss.\n",
    "                # Transform coefficients to a standard normal distribution.\n",
    "                # 45 x n_voxels\n",
    "                pred_fodf_standardized = (\n",
    "                    pred_fodf - LOSS_ODF_COEFF_MEANS\n",
    "                ) / LOSS_ODF_COEFF_STDS\n",
    "                y_standardized = (y - LOSS_ODF_COEFF_MEANS) / LOSS_ODF_COEFF_STDS\n",
    "                # Tissue weights\n",
    "                tissue_weight_mask = y_mask.to(pred_fodf)\n",
    "                tissue_weight_mask[batch_dict[\"gm_mask\"]] = 0.3\n",
    "                tissue_weight_mask[batch_dict[\"csf_mask\"]] = 0.1\n",
    "                tissue_weight_mask[batch_dict[\"wm_mask\"]] = 1.0\n",
    "                pred_fodf_standardized *= tissue_weight_mask\n",
    "                y_standardized *= tissue_weight_mask\n",
    "                # Calculate loss over weighted prediction and target.\n",
    "                loss_fodf = loss_fn(\n",
    "                    pred_fodf_standardized[y_mask_broad], y_standardized[y_mask_broad]\n",
    "                )\n",
    "\n",
    "                # loss_fodf = loss_fn(pred_fodf[y_mask_broad], y[y_mask_broad])\n",
    "                loss_recon = y.new_zeros(1)\n",
    "                recon_pred = None\n",
    "            else:\n",
    "                recon_pred = recon_decoder(ctx_v)\n",
    "                # Index bvals to be 2 b=0s, 2 b=1000s, and 2 b=3000s.\n",
    "                recon_y = torch.cat(\n",
    "                    [x[:, (0, 1, 2, 11, 12, 13)], x_coords.movedim(-1, 1)], dim=1\n",
    "                )\n",
    "                loss_recon = recon_loss_fn(recon_pred, recon_y)\n",
    "                loss_fodf = recon_y.new_zeros(1)\n",
    "                pred_fodf = None\n",
    "\n",
    "            loss = loss_fodf + loss_recon\n",
    "\n",
    "            fabric.backward(loss)\n",
    "            for model, model_optim in zip(\n",
    "                (encoder, decoder, recon_decoder),\n",
    "                (optim_encoder, optim_decoder, optim_recon_decoder),\n",
    "            ):\n",
    "                if train_recon and model is decoder:\n",
    "                    continue\n",
    "                elif not train_recon and model is recon_decoder:\n",
    "                    continue\n",
    "                fabric.clip_gradients(\n",
    "                    model,\n",
    "                    model_optim,\n",
    "                    max_norm=5.0,\n",
    "                    norm_type=2,\n",
    "                    error_if_nonfinite=True,\n",
    "                )\n",
    "\n",
    "            optim_encoder.step()\n",
    "            optim_decoder.step()\n",
    "            optim_recon_decoder.step()\n",
    "\n",
    "            to_track = {\n",
    "                \"loss\": loss.detach().cpu().item(),\n",
    "            }\n",
    "            # Depending on whether or not the reconstruction decoder is training,\n",
    "            # select which metrics to track at this time.\n",
    "            if train_recon:\n",
    "                to_track = {\n",
    "                    **to_track,\n",
    "                    **{\n",
    "                        \"loss_recon\": loss_recon.detach().cpu().item(),\n",
    "                    },\n",
    "                }\n",
    "            else:\n",
    "                to_track = {\n",
    "                    **to_track,\n",
    "                    **{\n",
    "                        \"loss_pred_fodf\": loss_fodf.detach().cpu().item(),\n",
    "                    },\n",
    "                }\n",
    "            if fabric.is_global_zero:\n",
    "                aim_run.track(\n",
    "                    to_track,\n",
    "                    context={\n",
    "                        \"subset\": \"train\",\n",
    "                    },\n",
    "                    step=step,\n",
    "                    epoch=epoch,\n",
    "                )\n",
    "                losses[\"loss\"].append(loss.detach().cpu().item())\n",
    "                losses[\"epoch\"].append(epoch)\n",
    "                losses[\"step\"].append(step)\n",
    "\n",
    "            fabric.print(\n",
    "                f\"| {loss.detach().cpu().item()}\",\n",
    "                end=\" \",\n",
    "                flush=(step % 10) == 0,\n",
    "            )\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        optim_encoder.zero_grad(set_to_none=True)\n",
    "        optim_decoder.zero_grad(set_to_none=True)\n",
    "        optim_recon_decoder.zero_grad(set_to_none=True)\n",
    "        # Delete some training inputs to relax memory constraints in whole-\n",
    "        # volume inference inside validation step.\n",
    "        del x, x_coords, y, y_coords, pred_fodf, recon_pred\n",
    "\n",
    "        fabric.print(\"\\n==Validation==\", flush=True)\n",
    "        fabric.barrier()\n",
    "        if fabric.is_global_zero:\n",
    "            aim_run, val_viz_subj_id, val_scores = validate_stage(\n",
    "                fabric,\n",
    "                encoder,\n",
    "                decoder,\n",
    "                val_dataloader=val_dataloader,\n",
    "                step=step,\n",
    "                epoch=epoch,\n",
    "                aim_run=aim_run,\n",
    "                val_viz_subj_id=val_viz_subj_id,\n",
    "            )\n",
    "            curr_val_score = val_scores.detach().cpu().mean().item()\n",
    "            fabric.print(str(curr_val_score))\n",
    "        fabric.barrier()\n",
    "\n",
    "        # Start saving best performing models if the previous best val score was\n",
    "        # surpassed, and the current number of epcohs is >= half the total training\n",
    "        # amount.\n",
    "        if (curr_val_score < (curr_best_val_score - (0.05 * curr_best_val_score))) and (\n",
    "            epoch >= epochs / 3\n",
    "        ):\n",
    "            fabric.print(\"Saving new best validation score\")\n",
    "            fabric.barrier()\n",
    "            if fabric.is_global_zero:\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"encoder\": encoder.state_dict(),\n",
    "                        \"decoder\": decoder.state_dict(),\n",
    "                        \"epoch\": epoch,\n",
    "                        \"step\": step,\n",
    "                        \"aim_run_hash\": aim_run.hash,\n",
    "                        \"optim_encoder\": optim_encoder.state_dict(),\n",
    "                        \"optim_decoder\": optim_decoder.state_dict(),\n",
    "                        \"recon_decoder\": recon_decoder.state_dict(),\n",
    "                        \"optim_recon_decoder\": optim_recon_decoder.state_dict(),\n",
    "                    },\n",
    "                    Path(tmp_res_dir)\n",
    "                    / f\"best_val_score_state_dict_epoch_{epoch}_step_{step}.pt\",\n",
    "                )\n",
    "                curr_best_val_score = curr_val_score\n",
    "            fabric.barrier()\n",
    "\n",
    "        if epoch in checkpoint_epochs:\n",
    "            fabric.print(f\"Saving checkpoint {curr_checkpoint}\")\n",
    "            fabric.barrier()\n",
    "            if fabric.is_global_zero:\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"encoder\": encoder.state_dict(),\n",
    "                        \"decoder\": decoder.state_dict(),\n",
    "                        \"epoch\": epoch,\n",
    "                        \"step\": step,\n",
    "                        \"aim_run_hash\": aim_run.hash,\n",
    "                        \"optim_encoder\": optim_encoder.state_dict(),\n",
    "                        \"optim_decoder\": optim_decoder.state_dict(),\n",
    "                        \"recon_decoder\": recon_decoder.state_dict(),\n",
    "                        \"optim_recon_decoder\": optim_recon_decoder.state_dict(),\n",
    "                    },\n",
    "                    Path(tmp_res_dir)\n",
    "                    / f\"checkpoint_{curr_checkpoint}_state_dict_epoch_{epoch}_step_{step}.pt\",\n",
    "                )\n",
    "            fabric.barrier()\n",
    "            curr_checkpoint += 1\n",
    "\n",
    "except KeyboardInterrupt as e:\n",
    "    if fabric.is_global_zero:\n",
    "        aim_run.add_tag(\"STOPPED\")\n",
    "        (tmp_res_dir / \"STOPPED\").touch()\n",
    "    raise e\n",
    "except Exception as e:\n",
    "    if fabric.is_global_zero:\n",
    "        aim_run.add_tag(\"FAILED\")\n",
    "        (tmp_res_dir / \"FAILED\").touch()\n",
    "    raise e\n",
    "finally:\n",
    "    if fabric.is_global_zero:\n",
    "        aim_run.close()\n",
    "\n",
    "# Sync all pytorch-lightning processes.\n",
    "fabric.barrier()\n",
    "if fabric.is_global_zero:\n",
    "    fabric.print(\"Saving model state dict\")\n",
    "    torch.save(\n",
    "        {\n",
    "            \"encoder\": encoder.state_dict(),\n",
    "            \"decoder\": decoder.state_dict(),\n",
    "            \"recon_decoder\": recon_decoder.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"step\": step,\n",
    "            \"aim_run_hash\": aim_run.hash,\n",
    "            \"optim_encoder\": optim_encoder.state_dict(),\n",
    "            \"optim_decoder\": optim_decoder.state_dict(),\n",
    "            \"optim_recon_decoder\": optim_recon_decoder.state_dict(),\n",
    "        },\n",
    "        Path(tmp_res_dir) / f\"final_state_dict_epoch_{epoch}_step_{step}.pt\",\n",
    "    )\n",
    "    fabric.print(\"=\" * 40)\n",
    "    losses = pd.DataFrame.from_dict(losses)\n",
    "    losses.to_csv(Path(tmp_res_dir) / \"train_losses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35b2f33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "comment_magics": true,
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "pitn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
