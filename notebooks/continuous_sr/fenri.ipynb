{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3646f639",
   "metadata": {},
   "source": [
    "# Continuous-Space Super-Resolution of fODFs in Diffusion MRI\n",
    "\n",
    "Code by:\n",
    "\n",
    "Tyler Spears - tas6hh@virginia.edu\n",
    "\n",
    "Dr. Tom Fletcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314c19b9",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753fcb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Automatically re-import project-specific modules.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# imports\n",
    "import collections\n",
    "import copy\n",
    "import datetime\n",
    "import functools\n",
    "import inspect\n",
    "import io\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import pdb\n",
    "import random\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "import typing\n",
    "import warnings\n",
    "import zipfile\n",
    "import concurrent.futures\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from pprint import pprint as ppr\n",
    "\n",
    "import aim\n",
    "import dotenv\n",
    "import einops\n",
    "import lightning\n",
    "\n",
    "# visualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import monai\n",
    "\n",
    "# Data management libraries.\n",
    "import nibabel as nib\n",
    "import nibabel.processing\n",
    "\n",
    "# Computation & ML libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import skimage\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from box import Box\n",
    "from icecream import ic\n",
    "from inr_networks import Decoder, INREncoder, SimplifiedDecoder\n",
    "\n",
    "# from lightning_fabric.fabric import Fabric\n",
    "from natsort import natsorted\n",
    "\n",
    "import pitn\n",
    "\n",
    "plt.rcParams.update({\"figure.autolayout\": True})\n",
    "plt.rcParams.update({\"figure.facecolor\": [1.0, 1.0, 1.0, 1.0]})\n",
    "plt.rcParams.update({\"image.cmap\": \"gray\"})\n",
    "\n",
    "# Set print options for ndarrays/tensors.\n",
    "np.set_printoptions(suppress=True, threshold=100, linewidth=88)\n",
    "torch.set_printoptions(sci_mode=False, threshold=100, linewidth=88)\n",
    "\n",
    "# monai.data.set_track_meta(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23ac2a5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# MAIN\n",
    "# if __name__ == \"__main__\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e41cbc1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Update notebook's environment variables with direnv.\n",
    "# This requires the python-dotenv package, and direnv be installed on the system\n",
    "# This will not work on Windows.\n",
    "# NOTE: This is kind of hacky, and not necessarily safe. Be careful...\n",
    "# Libraries needed on the python side:\n",
    "# - os\n",
    "# - subprocess\n",
    "# - io\n",
    "# - dotenv\n",
    "\n",
    "# Form command to be run in direnv's context. This command will print out\n",
    "# all environment variables defined in the subprocess/sub-shell.\n",
    "command = \"direnv exec {} /usr/bin/env\".format(os.getcwd())\n",
    "# Run command in a new subprocess.\n",
    "proc = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, cwd=os.getcwd())\n",
    "# Store and format the subprocess' output.\n",
    "proc_out = proc.communicate()[0].strip().decode(\"utf-8\")\n",
    "# Use python-dotenv to load the environment variables by using the output of\n",
    "# 'direnv exec ...' as a 'dummy' .env file.\n",
    "dotenv.load_dotenv(stream=io.StringIO(proc_out), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e6ce46",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# torch setup\n",
    "# allow for CUDA usage, if available\n",
    "if torch.cuda.is_available():\n",
    "    # Pick only one device for the default, may use multiple GPUs for training later.\n",
    "    if \"CUDA_PYTORCH_DEVICE_IDX\" in os.environ.keys():\n",
    "        dev_idx = int(os.environ[\"CUDA_PYTORCH_DEVICE_IDX\"])\n",
    "    else:\n",
    "        dev_idx = 0\n",
    "    device = torch.device(f\"cuda:{dev_idx}\")\n",
    "    print(\"CUDA Device IDX \", dev_idx)\n",
    "    torch.cuda.set_device(device)\n",
    "    print(\"CUDA Current Device \", torch.cuda.current_device())\n",
    "    print(\"CUDA Device properties: \", torch.cuda.get_device_properties(device))\n",
    "    # The flag below controls whether to allow TF32 on matmul. This flag defaults to False\n",
    "    # in PyTorch 1.12 and later.\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    # See\n",
    "    # <https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices>\n",
    "    # for details.\n",
    "\n",
    "    # Activate cudnn benchmarking to optimize convolution algorithm speed.\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        print(\"CuDNN convolution optimization enabled.\")\n",
    "        # The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# keep device as the cpu\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a8f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr cap\n",
    "# Capture output and save to log. Needs to be at the *very first* line of the cell.\n",
    "# Watermark\n",
    "%load_ext watermark\n",
    "%watermark --author \"Tyler Spears\" --updated --iso8601  --python --machine --iversions --githash\n",
    "if torch.cuda.is_available():\n",
    "    # GPU information\n",
    "    try:\n",
    "        gpu_info = pitn.utils.system.get_gpu_specs()\n",
    "        print(gpu_info)\n",
    "    except NameError:\n",
    "        print(\"CUDA Version: \", torch.version.cuda)\n",
    "else:\n",
    "    print(\"CUDA not in use, falling back to CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f4f6ee",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# cap is defined in an ipython magic command\n",
    "try:\n",
    "    print(cap)\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd24b58a",
   "metadata": {},
   "source": [
    "## Experiment & Parameters Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859488e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Box(default_box=True)\n",
    "# Experiment defaults, can be overridden in a config file.\n",
    "\n",
    "# General experiment-wide params\n",
    "###############################################\n",
    "p.experiment_name = \"FENRI_test_fov-resize_rerun\"\n",
    "p.override_experiment_name = False\n",
    "p.results_dir = \"/data/srv/outputs/pitn/results/runs\"\n",
    "p.tmp_results_dir = \"/data/srv/outputs/pitn/results/tmp\"\n",
    "p.train_val_test_split_file = (\n",
    "    Path(\"./data_splits\") / \"HCP_train-val-test_split_01.1.csv\"\n",
    ")\n",
    "# p.train_val_test_split_file = random.choice(\n",
    "#     list(Path(\"./data_splits\").glob(\"HCP*train-val-test_split*.csv\"))\n",
    "# )\n",
    "p.aim_logger = dict(\n",
    "    repo=\"aim://dali.cpe.virginia.edu:53800\",\n",
    "    # repo=\"/data/srv/outputs/pitn/results/aim/tmp\",\n",
    "    experiment=\"PITN_FENRI\",\n",
    "    meta_params=dict(run_name=p.experiment_name),\n",
    "    tags=(\"PITN\", \"INR\", \"HCP\", \"super-res\", \"dMRI\", \"FENRI\"),\n",
    ")\n",
    "p.checkpoint_epoch_ratios = (0.5,)\n",
    "###############################################\n",
    "\n",
    "# 1.25mm -> 2.0mm\n",
    "p.preproc_loaded = dict(S0_noise_b0_quantile=0.99, patch_sampling_w_erosion=17)\n",
    "p.baseline_lr_spacing_scale = 1.6\n",
    "p.baseline_snr = 30\n",
    "p.train = dict(\n",
    "    patch_size=(36, 36, 36),\n",
    "    batch_size=6,\n",
    "    samples_per_subj_per_epoch=100,\n",
    "    max_epochs=50,\n",
    "    # dwi_recon_epoch_proportion=1 / 99,\n",
    "    dwi_recon_epoch_proportion=0.0,\n",
    ")\n",
    "p.train.patch_sampling = dict(rng=\"default\")\n",
    "p.train.patch_tf = dict(\n",
    "    downsample_factor_range=(p.baseline_lr_spacing_scale, 2.0),\n",
    "    noise_snr_range=(p.baseline_snr, 35),\n",
    "    prefilter_sigma_scale_coeff=2.0,\n",
    "    rng=\"default\",\n",
    ")\n",
    "\n",
    "# Optimizer kwargs for training.\n",
    "p.train.optim.encoder.lr = 5e-4\n",
    "p.train.optim.decoder.lr = 5e-4\n",
    "p.train.optim.recon_decoder.lr = 1e-3\n",
    "# Train dataloader kwargs.\n",
    "p.train.dataloader = dict(num_workers=17, persistent_workers=True, prefetch_factor=3)\n",
    "# p.train.dataloader = dict(num_workers=0)\n",
    "\n",
    "# Network/model parameters.\n",
    "p.encoder = dict(\n",
    "    interior_channels=80,\n",
    "    out_channels=96,\n",
    "    n_res_units=3,\n",
    "    n_dense_units=3,\n",
    "    activate_fn=\"relu\",\n",
    "    input_coord_channels=True,\n",
    "    post_batch_norm=True,\n",
    ")\n",
    "p.decoder = dict(\n",
    "    context_v_features=96,\n",
    "    out_features=45,\n",
    "    m_encode_num_freqs=36,\n",
    "    sigma_encode_scale=3.0,\n",
    "    n_internal_features=256,\n",
    "    n_internal_layers=3,\n",
    ")\n",
    "p.val.rng_seed = 3967417599011123030\n",
    "p.val.vol_tf = dict(\n",
    "    downsample_factor_range=(p.baseline_lr_spacing_scale, p.baseline_lr_spacing_scale),\n",
    "    noise_snr_range=(p.baseline_snr, p.baseline_snr),\n",
    "    prefilter_sigma_scale_coeff=2.0,\n",
    "    # Manually crop each side by 1 voxel to avoid NaNs in the LR resampling.\n",
    "    manual_crop_lr_sides=((1, 1), (1, 1), (1, 1)),\n",
    ")\n",
    "\n",
    "# If a config file exists, override the defaults with those values.\n",
    "try:\n",
    "    if \"PITN_CONFIG\" in os.environ.keys():\n",
    "        config_fname = Path(os.environ[\"PITN_CONFIG\"])\n",
    "    else:\n",
    "        config_fname = pitn.utils.system.get_file_glob_unique(Path(\".\"), r\"config.*\")\n",
    "    f_type = config_fname.suffix.casefold()\n",
    "    if f_type in {\".yaml\", \".yml\"}:\n",
    "        f_params = Box.from_yaml(filename=config_fname)\n",
    "    elif f_type == \".json\":\n",
    "        f_params = Box.from_json(filename=config_fname)\n",
    "    elif f_type == \".toml\":\n",
    "        f_params = Box.from_toml(filename=config_fname)\n",
    "    else:\n",
    "        raise RuntimeError()\n",
    "\n",
    "    p.merge_update(f_params)\n",
    "\n",
    "except:\n",
    "    print(\"WARNING: Config file not loaded\")\n",
    "    pass\n",
    "\n",
    "# Remove the default_box behavior now that params have been fully read in.\n",
    "_p = Box(default_box=False)\n",
    "_p.merge_update(p)\n",
    "p = _p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a421a7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "tvt_split = pd.read_csv(p.train_val_test_split_file)\n",
    "p.train.subj_ids = natsorted(tvt_split[tvt_split.split == \"train\"].subj_id.tolist())\n",
    "if \"val\" not in p.keys():\n",
    "    p.val = dict()\n",
    "p.val.subj_ids = natsorted(tvt_split[tvt_split.split == \"val\"].subj_id.tolist())\n",
    "if \"test\" not in p.keys():\n",
    "    p.test = dict()\n",
    "p.test.subj_ids = natsorted(tvt_split[tvt_split.split == \"test\"].subj_id.tolist())\n",
    "\n",
    "# Ensure that no test subj ids are in either the training or validation sets.\n",
    "# However, we can have overlap between training and validation.\n",
    "assert len(set(p.train.subj_ids) & set(p.test.subj_ids)) == 0\n",
    "assert len(set(p.val.subj_ids) & set(p.test.subj_ids)) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d00d24d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "ic(p.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddac0231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which parameters to store in the aim meta-params.\n",
    "p.aim_logger.meta_params.hparams = dict(\n",
    "    batch_size=p.train.batch_size,\n",
    "    patch_spatial_size=p.train.patch_size,\n",
    "    samples_per_subj_per_epoch=p.train.samples_per_subj_per_epoch,\n",
    "    max_epochs=p.train.max_epochs,\n",
    ")\n",
    "p.aim_logger.meta_params.data = dict(\n",
    "    train_subj_ids=p.train.subj_ids,\n",
    "    val_subj_ids=p.val.subj_ids,\n",
    "    test_subj_ids=p.test.subj_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173f55e9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def fork_rng(rng: torch.Generator) -> torch.Generator:\n",
    "    rng_fork = torch.Generator(device=rng.device)\n",
    "    rng_fork.set_state(rng.get_state())\n",
    "    return rng_fork\n",
    "\n",
    "\n",
    "rng = fork_rng(torch.default_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d31c8f",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9aa666",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "num_load_and_tf_workers = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3590a6d4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "hcp_data_root_dir = Path(\"/data/srv/outputs/pitn/hcp\")\n",
    "\n",
    "assert hcp_data_root_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a6eb1f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Set paths relative to the subj id root dir for each required image/file.\n",
    "rel_dwi_path = Path(\"ras/diffusion/dwi_norm.nii.gz\")\n",
    "rel_grad_table_path = Path(\"ras/diffusion/ras_grad_mrtrix.b\")\n",
    "rel_odf_path = Path(\"ras/odf/wm_msmt_csd_norm_odf.nii.gz\")\n",
    "rel_fivett_seg_path = Path(\"ras/segmentation/fivett_dwi-space_segmentation.nii.gz\")\n",
    "rel_brain_mask_path = Path(\"ras/brain_mask.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bce90ce",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Set a common target set of gradient directions/strengths based on the standard HCP\n",
    "# protocol.\n",
    "first_n_b0s = 9\n",
    "first_n_b1000s = 45\n",
    "# Remove the b2000s entirely.\n",
    "first_n_b3000s = 45\n",
    "\n",
    "target_grad_table = pitn.data.HCP_STANDARD_3T_GRAD_MRTRIX_TABLE\n",
    "\n",
    "shells = target_grad_table.b.to_numpy().round(-2)\n",
    "row_select_template = np.zeros_like(shells).astype(bool)\n",
    "# Take first N b0s\n",
    "b0_idx = (np.where(shells == 0)[0][:first_n_b0s],)\n",
    "b0_select = row_select_template.copy()\n",
    "b0_select[b0_idx] = True\n",
    "# Take first N b1000s\n",
    "b1000_idx = (np.where(shells == 1000)[0][:first_n_b1000s],)\n",
    "b1000_select = row_select_template.copy()\n",
    "b1000_select[b1000_idx] = True\n",
    "# Take first N b3000s\n",
    "b3000_idx = (np.where(shells == 3000)[0][:first_n_b3000s],)\n",
    "b3000_select = row_select_template.copy()\n",
    "b3000_select[b3000_idx] = True\n",
    "\n",
    "dwi_select_mask = b0_select | b1000_select | b3000_select\n",
    "target_grad_table = target_grad_table.loc[dwi_select_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951e44d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_loaded_kwargs = dict(\n",
    "    S0_noise_b0_quantile=p.preproc_loaded.S0_noise_b0_quantile,\n",
    "    patch_sampling_w_erosion=p.preproc_loaded.patch_sampling_w_erosion,\n",
    "    resample_target_grad_table=target_grad_table,\n",
    ")\n",
    "\n",
    "# Worker function as a single-argument callable.\n",
    "def load_and_deterministic_tf_subj(subj_files: dict):\n",
    "    print(\n",
    "        f\"{os.getpid()} : Loading subj {subj_files['subj_id']}...\\n\",\n",
    "        end=\"\",\n",
    "        flush=True,\n",
    "    )\n",
    "    s = pitn.data.load_super_res_subj_sample(**subj_files)\n",
    "    print(\n",
    "        f\"{os.getpid()} : Finished loading subj {subj_files['subj_id']}\\n\",\n",
    "        end=\"\",\n",
    "        flush=True,\n",
    "    )\n",
    "    return pitn.data.preproc.preproc_loaded_super_res_subj(s, **preproc_loaded_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbaf649",
   "metadata": {},
   "source": [
    "### Training Dataset of Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f873a9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# DEBUG_TRAIN_DATA_SUBJS = 2\n",
    "\n",
    "train_subj_ids = p.train.subj_ids\n",
    "# train_subj_ids = p.train.subj_ids[:DEBUG_TRAIN_DATA_SUBJS]  #!DEBUG\n",
    "\n",
    "train_subj_dicts = list()\n",
    "for subj_id in train_subj_ids:\n",
    "    root_dir = hcp_data_root_dir / str(subj_id)\n",
    "    d = dict(\n",
    "        subj_id=str(subj_id),\n",
    "        dwi_f=root_dir / rel_dwi_path,\n",
    "        grad_mrtrix_f=root_dir / rel_grad_table_path,\n",
    "        odf_f=root_dir / rel_odf_path,\n",
    "        brain_mask_f=root_dir / rel_brain_mask_path,\n",
    "        fivett_seg_f=root_dir / rel_fivett_seg_path,\n",
    "    )\n",
    "    train_subj_dicts.append(d)\n",
    "\n",
    "# Load & run non-random transforms on training subjects in parallel.\n",
    "preproc_train_dataset = list()\n",
    "with concurrent.futures.ProcessPoolExecutor(\n",
    "    max_workers=num_load_and_tf_workers\n",
    ") as executor:\n",
    "    subj_data_futures = executor.map(load_and_deterministic_tf_subj, train_subj_dicts)\n",
    "    for subj_data in subj_data_futures:\n",
    "        preproc_train_dataset.append(subj_data)\n",
    "\n",
    "print(\"Done loading training subject data\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a921f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset that randomly samples and (ramdonly) transforms patches.\n",
    "train_patch_fn = partial(\n",
    "    pitn.data.preproc.lazy_sample_patch_from_super_res_sample,\n",
    "    patch_size=p.train.patch_size,\n",
    "    num_samples=p.train.samples_per_subj_per_epoch,\n",
    "    rng=\"default\",\n",
    ")\n",
    "train_patch_tf = partial(\n",
    "    pitn.data.preproc.preproc_super_res_sample, **p.train.patch_tf.to_dict()\n",
    ")\n",
    "train_dataset = monai.data.PatchDataset(\n",
    "    data=preproc_train_dataset,\n",
    "    patch_func=train_patch_fn,\n",
    "    samples_per_image=p.train.samples_per_subj_per_epoch,\n",
    "    transform=train_patch_tf,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c048a20d",
   "metadata": {},
   "source": [
    "### Validation Dataset of Whole Volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8454f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG_VAL_DATA_SUBJS = 2\n",
    "\n",
    "val_subj_ids = p.val.subj_ids\n",
    "# val_subj_ids = p.val.subj_ids[:DEBUG_VAL_DATA_SUBJS]  #!DEBUG\n",
    "\n",
    "val_subj_dicts = list()\n",
    "for subj_id in val_subj_ids:\n",
    "    root_dir = hcp_data_root_dir / str(subj_id)\n",
    "    d = dict(\n",
    "        subj_id=str(subj_id),\n",
    "        dwi_f=root_dir / rel_dwi_path,\n",
    "        grad_mrtrix_f=root_dir / rel_grad_table_path,\n",
    "        odf_f=root_dir / rel_odf_path,\n",
    "        brain_mask_f=root_dir / rel_brain_mask_path,\n",
    "        fivett_seg_f=root_dir / rel_fivett_seg_path,\n",
    "    )\n",
    "    val_subj_dicts.append(d)\n",
    "\n",
    "# Load & run non-random transforms on subjects in parallel.\n",
    "preproc_val_dataset = list()\n",
    "with concurrent.futures.ProcessPoolExecutor(\n",
    "    max_workers=num_load_and_tf_workers\n",
    ") as executor:\n",
    "    subj_data_futures = executor.map(load_and_deterministic_tf_subj, val_subj_dicts)\n",
    "    for subj_data in subj_data_futures:\n",
    "        preproc_val_dataset.append(subj_data)\n",
    "\n",
    "print(\"Done loading validation subject data\", flush=True)\n",
    "\n",
    "\n",
    "# Seed the validation set generator for deterministic transforms of test samples between\n",
    "# networks,trilinear interpolation, etc.\n",
    "# The seed is set as 'val_seed_int XOR subj_id_int'.\n",
    "val_dataset = monai.data.Dataset(\n",
    "    [\n",
    "        pitn.data.preproc.preproc_super_res_sample(\n",
    "            v,\n",
    "            **p.val.vol_tf.to_dict(),\n",
    "            rng=torch.Generator(device=rng.device).manual_seed(\n",
    "                int(p.val.rng_seed) ^ int(v[\"subj_id\"])\n",
    "            ),\n",
    "        )\n",
    "        for v in preproc_val_dataset\n",
    "    ]\n",
    ")\n",
    "del preproc_val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de803e6",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger_run(run_kwargs: dict, logger_meta_params: dict, logger_tags: list):\n",
    "    aim_run = aim.Run(\n",
    "        system_tracking_interval=None,\n",
    "        log_system_params=True,\n",
    "        capture_terminal_logs=True,\n",
    "        **run_kwargs,\n",
    "    )\n",
    "    for k, v in logger_meta_params.items():\n",
    "        aim_run[k] = v\n",
    "    for v in logger_tags:\n",
    "        aim_run.add_tag(v)\n",
    "\n",
    "    return aim_run\n",
    "\n",
    "\n",
    "def batchwise_masked_mse(y_pred, y, mask):\n",
    "    masked_y_pred = y_pred.clone()\n",
    "    masked_y = y.clone()\n",
    "    masked_y_pred[~mask] = torch.nan\n",
    "    masked_y[~mask] = torch.nan\n",
    "    se = F.mse_loss(masked_y_pred, masked_y, reduction=\"none\")\n",
    "    se = se.reshape(se.shape[0], -1)\n",
    "    mse = torch.nanmean(se, dim=1)\n",
    "    return mse\n",
    "\n",
    "\n",
    "def validate_stage(\n",
    "    fabric,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    val_dataloader,\n",
    "    step: int,\n",
    "    epoch: int,\n",
    "    aim_run,\n",
    "    val_viz_subj_id,\n",
    "):\n",
    "    encoder_was_training = encoder.training\n",
    "    decoder_was_training = decoder.training\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        # Set up validation metrics to track for this validation run.\n",
    "        val_metrics = {\"mse\": list()}\n",
    "        for batch_dict in val_dataloader:\n",
    "            subj_id = batch_dict[\"subj_id\"]\n",
    "            if len(subj_id) == 1:\n",
    "                subj_id = subj_id[0]\n",
    "            if val_viz_subj_id is None:\n",
    "                val_viz_subj_id = subj_id\n",
    "\n",
    "            x = batch_dict[\"lr_dwi\"]\n",
    "            batch_size = x.shape[0]\n",
    "            x_affine_vox2real = batch_dict[\"affine_lr_vox2real\"].to(x.dtype)\n",
    "            x_spacing = batch_dict[\"lr_spacing\"]\n",
    "            x_coords = einops.rearrange(\n",
    "                batch_dict[\"lr_real_coords\"], \"b coord x y z -> b x y z coord\"\n",
    "            )\n",
    "\n",
    "            y = batch_dict[\"odf\"]\n",
    "            y_mask = batch_dict[\"brain_mask\"].bool()\n",
    "            y_spacing = batch_dict[\"full_res_spacing\"]\n",
    "            y_coords = einops.rearrange(\n",
    "                batch_dict[\"full_res_real_coords\"], \"b coord x y z -> b x y z coord\"\n",
    "            )\n",
    "\n",
    "            if batch_size == 1:\n",
    "                if x_coords.shape[0] != 1:\n",
    "                    x_coords.unsqueeze_(0)\n",
    "                if y_coords.shape[0] != 1:\n",
    "                    y_coords.unsqueeze_(0)\n",
    "\n",
    "            # Append LR coordinates to the end of the input LR DWIs.\n",
    "            x = torch.cat(\n",
    "                [x, einops.rearrange(x_coords, \"b x y z coord -> b coord x y z\")], dim=1\n",
    "            )\n",
    "            ctx_v = encoder(x)\n",
    "\n",
    "            # Whole-volume inference is memory-prohibitive, so use a sliding\n",
    "            # window inference method on the encoded volume.\n",
    "            # Transform y_coords into a coordinates-first shape, for the interface, and\n",
    "            # attach the mask for compatibility with the sliding inference function.\n",
    "            y_slide_window = torch.cat(\n",
    "                [\n",
    "                    einops.rearrange(y_coords, \"b x y z coord -> b coord x y z\"),\n",
    "                    y_mask.to(y_coords),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "            fn_coordify = lambda x: einops.rearrange(\n",
    "                x, \"b coord x y z -> b x y z coord\"\n",
    "            )\n",
    "\n",
    "            pred_fodf = monai.inferers.sliding_window_inference(\n",
    "                y_slide_window,\n",
    "                roi_size=(52, 52, 52),\n",
    "                sw_batch_size=y_coords.shape[0],\n",
    "                predictor=lambda q: decoder(\n",
    "                    # Rearrange back into coord-last format.\n",
    "                    query_real_coords=fn_coordify(q[:, :-1]),\n",
    "                    query_coords_mask=fn_coordify(q[:, -1:].bool()),\n",
    "                    context_v=ctx_v,\n",
    "                    context_real_coords=x_coords,\n",
    "                    affine_context_vox2real=x_affine_vox2real,\n",
    "                    context_spacing=x_spacing,\n",
    "                    query_spacing=y_spacing,\n",
    "                ),\n",
    "                overlap=0,\n",
    "                padding_mode=\"replicate\",\n",
    "            )\n",
    "\n",
    "            y_mask_broad = torch.broadcast_to(y_mask, y.shape)\n",
    "            # Calculate performance metrics\n",
    "            mse_loss = batchwise_masked_mse(pred_fodf, y, mask=y_mask_broad)\n",
    "            val_metrics[\"mse\"].append(mse_loss.detach().cpu().flatten())\n",
    "\n",
    "            # If visualization subj_id is in this batch, create the visual and log it.\n",
    "            if subj_id == val_viz_subj_id:\n",
    "                with mpl.rc_context({\"font.size\": 6.0}):\n",
    "                    fig = plt.figure(dpi=175, figsize=(10, 6))\n",
    "                    # Reorient from RAS to IPR for visualization purposes.\n",
    "                    ipr_pred_fodf = einops.rearrange(\n",
    "                        pred_fodf, \"b c x y z -> b c z y x\"\n",
    "                    ).flip(2, 3)\n",
    "                    ipr_y = einops.rearrange(y, \"b c x y z -> b c z y x\").flip(2, 3)\n",
    "                    ipr_y_mask_broad = einops.rearrange(\n",
    "                        y_mask_broad, \"b c x y z -> b c z y x\"\n",
    "                    ).flip(2, 3)\n",
    "                    # Select a mix of zonal and non-zonal harmonics for viz.\n",
    "                    fodf_coeff_idx = (0, 4, 8, 26, 30)\n",
    "                    h_degrees = list(range(0, 9, 2))\n",
    "                    zh_coeff_idx = (0, 3, 10, 21, 36)\n",
    "                    # Generate row headers.\n",
    "                    row_headers = list()\n",
    "                    for i in range(len(fodf_coeff_idx)):\n",
    "                        coeff_idx = fodf_coeff_idx[i]\n",
    "                        deg = h_degrees[i]\n",
    "                        order = int(coeff_idx - zh_coeff_idx[i])\n",
    "                        row_headers.append(f\"Deg {deg} order {order}\")\n",
    "\n",
    "                    fig = pitn.viz.plot_fodf_coeff_slices(\n",
    "                        ipr_pred_fodf,\n",
    "                        ipr_y,\n",
    "                        torch.abs(ipr_pred_fodf - ipr_y) * ipr_y_mask_broad,\n",
    "                        # pred_fodf,\n",
    "                        # y,\n",
    "                        # torch.abs(pred_fodf - y) * y_mask_broad,\n",
    "                        slice_idx=(0.5, 0.55, 0.45),\n",
    "                        col_headers=(\"Predicted\",) * 3\n",
    "                        + (\"Target\",) * 3\n",
    "                        + (\"|Pred - GT|\",) * 3,\n",
    "                        fodf_coeff_idx=fodf_coeff_idx,\n",
    "                        # row_headers=[f\"z-harm deg {i}\" for i in range(0, 9, 2)],\n",
    "                        row_headers=row_headers,\n",
    "                        colorbars=\"rows\",\n",
    "                        fig=fig,\n",
    "                        interpolation=\"nearest\",\n",
    "                        cmap=\"gray\",\n",
    "                    )\n",
    "\n",
    "                    aim_run.track(\n",
    "                        aim.Image(\n",
    "                            fig,\n",
    "                            caption=f\"Val Subj {subj_id}, \"\n",
    "                            + f\"MSE = {val_metrics['mse'][-1].item()}\",\n",
    "                            optimize=True,\n",
    "                            quality=100,\n",
    "                            format=\"png\",\n",
    "                        ),\n",
    "                        name=\"sh_whole_volume\",\n",
    "                        context={\"subset\": \"val\"},\n",
    "                        epoch=epoch,\n",
    "                        step=step,\n",
    "                    )\n",
    "                    plt.close(fig)\n",
    "                    del ipr_pred_fodf, ipr_y, ipr_y_mask_broad\n",
    "\n",
    "                # Plot MSE as distributed over the SH orders.\n",
    "                sh_coeff_labels = {\n",
    "                    \"idx\": list(range(0, 45)),\n",
    "                    \"l\": np.concatenate(\n",
    "                        list(\n",
    "                            map(\n",
    "                                lambda x: np.array([x] * (2 * x + 1)),\n",
    "                                range(0, 9, 2),\n",
    "                            )\n",
    "                        ),\n",
    "                        dtype=int,\n",
    "                    ).flatten(),\n",
    "                }\n",
    "                error_fodf = F.mse_loss(pred_fodf, y, reduction=\"none\")\n",
    "                error_fodf = einops.rearrange(\n",
    "                    error_fodf, \"b sh_idx x y z -> b x y z sh_idx\"\n",
    "                )\n",
    "                error_fodf = error_fodf[\n",
    "                    y_mask[:, 0, ..., None].broadcast_to(error_fodf.shape)\n",
    "                ]\n",
    "                error_fodf = einops.rearrange(\n",
    "                    error_fodf, \"(elem sh_idx) -> elem sh_idx\", sh_idx=45\n",
    "                )\n",
    "                error_fodf = error_fodf.flatten().detach().cpu().numpy()\n",
    "                error_df = pd.DataFrame.from_dict(\n",
    "                    {\n",
    "                        \"MSE\": error_fodf,\n",
    "                        \"SH_idx\": np.tile(\n",
    "                            sh_coeff_labels[\"idx\"], error_fodf.shape[0] // 45\n",
    "                        ),\n",
    "                        \"L Order\": np.tile(\n",
    "                            sh_coeff_labels[\"l\"], error_fodf.shape[0] // 45\n",
    "                        ),\n",
    "                    }\n",
    "                )\n",
    "                with mpl.rc_context({\"font.size\": 6.0}):\n",
    "                    fig = plt.figure(dpi=100, figsize=(6, 2))\n",
    "                    sns.boxplot(\n",
    "                        data=error_df,\n",
    "                        x=\"SH_idx\",\n",
    "                        y=\"MSE\",\n",
    "                        hue=\"L Order\",\n",
    "                        linewidth=0.8,\n",
    "                        showfliers=False,\n",
    "                        width=0.85,\n",
    "                        dodge=False,\n",
    "                    )\n",
    "                    aim_run.track(\n",
    "                        aim.Image(fig, caption=\"MSE over SH orders\", optimize=True),\n",
    "                        name=\"mse_over_sh_orders\",\n",
    "                        epoch=epoch,\n",
    "                        step=step,\n",
    "                    )\n",
    "                    plt.close(fig)\n",
    "            fabric.print(f\"MSE {val_metrics['mse'][-1].item()}\")\n",
    "            fabric.print(\"Finished validation subj \", subj_id)\n",
    "            del pred_fodf\n",
    "\n",
    "    val_metrics[\"mse\"] = torch.cat(val_metrics[\"mse\"])\n",
    "    # Log metrics\n",
    "    aim_run.track(\n",
    "        {\"mse\": val_metrics[\"mse\"].mean().numpy()},\n",
    "        context={\"subset\": \"val\"},\n",
    "        step=step,\n",
    "        epoch=epoch,\n",
    "    )\n",
    "\n",
    "    encoder.train(mode=encoder_was_training)\n",
    "    decoder.train(mode=decoder_was_training)\n",
    "    return aim_run, val_viz_subj_id, val_metrics[\"mse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "face5de2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "ts = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "# Break ISO format because many programs don't like having colons ':' in a filename.\n",
    "ts = ts.replace(\":\", \"_\")\n",
    "tmp_res_dir = Path(p.tmp_results_dir) / ts\n",
    "tmp_res_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e24d732",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "fabric = lightning.Fabric(accelerator=\"gpu\", devices=1, precision=32)\n",
    "# fabric = lightning.Fabric(accelerator=\"cpu\", devices=1, precision=32)\n",
    "fabric.launch()\n",
    "device = fabric.device\n",
    "\n",
    "aim_run = setup_logger_run(\n",
    "    run_kwargs={\n",
    "        k: p.aim_logger[k] for k in set(p.aim_logger.keys()) - {\"meta_params\", \"tags\"}\n",
    "    },\n",
    "    logger_meta_params=p.aim_logger.meta_params.to_dict(),\n",
    "    logger_tags=p.aim_logger.tags,\n",
    ")\n",
    "if \"in_channels\" not in p.encoder:\n",
    "    in_channels = int(train_dataset[0][\"lr_dwi\"].shape[0]) + 3\n",
    "else:\n",
    "    in_channels = p.encoder.in_channels\n",
    "\n",
    "# Wrap the entire training & validation loop in a try...except statement.\n",
    "try:\n",
    "\n",
    "    LOSS_ODF_COEFF_MEANS = torch.from_numpy(\n",
    "        np.array([0.17] + [0.002] * 5 + [0.002] * 9 + [0.0] * 13 + [0.0] * 17)\n",
    "    )\n",
    "    LOSS_ODF_COEFF_STDS = torch.from_numpy(\n",
    "        np.array([0.05] + [0.1] * 5 + [0.06] * 9 + [0.03] * 13 + [0.01] * 17)\n",
    "    )\n",
    "    LOSS_ODF_COEFF_MEANS = LOSS_ODF_COEFF_MEANS[None, :, None, None, None].to(device)\n",
    "    LOSS_ODF_COEFF_STDS = LOSS_ODF_COEFF_STDS[None, :, None, None, None].to(device)\n",
    "\n",
    "    encoder = INREncoder(**{**p.encoder.to_dict(), **{\"in_channels\": in_channels}})\n",
    "    # Initialize weight shape for the encoder.\n",
    "    with torch.no_grad():\n",
    "        encoder(torch.randn(1, in_channels, 20, 20, 20))\n",
    "    decoder = SimplifiedDecoder(**p.decoder.to_dict())\n",
    "\n",
    "    decoder = SimplifiedDecoder(**p.decoder.to_dict())\n",
    "    recon_decoder = INREncoder(\n",
    "        in_channels=encoder.out_channels,\n",
    "        interior_channels=48,\n",
    "        out_channels=9,\n",
    "        n_res_units=2,\n",
    "        n_dense_units=2,\n",
    "        activate_fn=p.encoder.activate_fn,\n",
    "        input_coord_channels=False,\n",
    "    )\n",
    "    # Initialize weight shape for the recon decoder.\n",
    "    with torch.no_grad():\n",
    "        recon_decoder(torch.randn(1, recon_decoder.in_channels, 20, 20, 20))\n",
    "    fabric.print(p.to_dict())\n",
    "    fabric.print(encoder)\n",
    "    fabric.print(decoder)\n",
    "    fabric.print(recon_decoder)\n",
    "    fabric.print(\"Encoder num params:\", sum([p.numel() for p in encoder.parameters()]))\n",
    "    fabric.print(\"Decoder num params:\", sum([p.numel() for p in decoder.parameters()]))\n",
    "    fabric.print(\n",
    "        \"Recon decoder num params:\",\n",
    "        sum([p.numel() for p in recon_decoder.parameters()]),\n",
    "    )\n",
    "\n",
    "    optim_encoder = torch.optim.AdamW(\n",
    "        encoder.parameters(), **p.train.optim.encoder.to_dict()\n",
    "    )\n",
    "    encoder, optim_encoder = fabric.setup(encoder, optim_encoder)\n",
    "    optim_decoder = torch.optim.AdamW(\n",
    "        decoder.parameters(), **p.train.optim.decoder.to_dict()\n",
    "    )\n",
    "    decoder, optim_decoder = fabric.setup(decoder, optim_decoder)\n",
    "    optim_recon_decoder = torch.optim.AdamW(\n",
    "        recon_decoder.parameters(), **p.train.optim.recon_decoder.to_dict()\n",
    "    )\n",
    "    recon_decoder, optim_recon_decoder = fabric.setup(\n",
    "        recon_decoder, optim_recon_decoder\n",
    "    )\n",
    "    loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
    "    recon_loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "    # Padding in collation will only occurr in the high-res/target volumes, which are\n",
    "    # processed voxel-wise. So, padding at the end does not change the behavior of the\n",
    "    # conv layers and does not change the vox-to-real coordinate affine transform.\n",
    "    collate_fn = partial(pitn.data.preproc.pad_list_data_collate_tensor, method=\"end\")\n",
    "    train_dataloader = monai.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=p.train.batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn,\n",
    "        **p.train.dataloader.to_dict(),\n",
    "    )\n",
    "    val_dataloader = monai.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=1,\n",
    "        # num_workers=0,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    train_dataloader, val_dataloader = fabric.setup_dataloaders(\n",
    "        train_dataloader, val_dataloader\n",
    "    )\n",
    "    val_viz_subj_id = None\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    recon_decoder.train()\n",
    "    losses = dict(\n",
    "        loss=list(),\n",
    "        epoch=list(),\n",
    "        step=list(),\n",
    "    )\n",
    "    step = 1\n",
    "    train_dwi_recon_epoch_proportion = p.train.dwi_recon_epoch_proportion\n",
    "    train_recon = False\n",
    "\n",
    "    epochs = p.train.max_epochs\n",
    "    curr_best_val_score = 1e8\n",
    "    checkpoint_epochs = np.floor(np.array(p.checkpoint_epoch_ratios) * epochs)\n",
    "    checkpoint_epochs = set(checkpoint_epochs.astype(int).tolist())\n",
    "    curr_checkpoint = 0\n",
    "    for epoch in range(epochs):\n",
    "        fabric.print(f\"\\nEpoch {epoch}\\n\", \"=\" * 10)\n",
    "        if epoch < math.floor(epochs * train_dwi_recon_epoch_proportion):\n",
    "            if not train_recon:\n",
    "                train_recon = True\n",
    "        else:\n",
    "            if train_recon:\n",
    "                train_recon = False\n",
    "                fabric.barrier()\n",
    "                if fabric.is_global_zero:\n",
    "                    torch.save(\n",
    "                        {\n",
    "                            \"epoch\": epoch,\n",
    "                            \"step\": step,\n",
    "                            \"aim_run_hash\": aim_run.hash,\n",
    "                            \"recon_decoder\": recon_decoder.state_dict(),\n",
    "                            \"optim_recon_decoder\": optim_recon_decoder.state_dict(),\n",
    "                        },\n",
    "                        Path(tmp_res_dir)\n",
    "                        / f\"recon_decoder_state_dict_epoch_{epoch}_step_{step}.pt\",\n",
    "                    )\n",
    "                    # Replace the recon network and optimization model with dummies, to\n",
    "                    # release gpu memory.\n",
    "                    del recon_decoder\n",
    "                    del optim_recon_decoder\n",
    "                    recon_decoder = torch.nn.Linear(1, 1, bias=False)\n",
    "                    optim_recon_decoder = torch.optim.SGD(\n",
    "                        recon_decoder.parameters(), 1e-3\n",
    "                    )\n",
    "                    recon_decoder, optim_recon_decoder = fabric.setup(\n",
    "                        recon_decoder, optim_recon_decoder\n",
    "                    )\n",
    "                    fabric.barrier()\n",
    "\n",
    "        for batch_dict in train_dataloader:\n",
    "\n",
    "            x = batch_dict[\"lr_dwi\"]\n",
    "            x_affine_vox2real = batch_dict[\"affine_lr_vox2real\"].to(x.dtype)\n",
    "            x_spacing = batch_dict[\"lr_spacing\"]\n",
    "            # Coordinates must be in coordinate-first shape to be batched by monai's\n",
    "            # collate function, so undo that here.\n",
    "            x_coords = einops.rearrange(\n",
    "                batch_dict[\"lr_real_coords\"], \"b coord x y z -> b x y z coord\"\n",
    "            )\n",
    "\n",
    "            y = batch_dict[\"odf\"]\n",
    "            y_mask = batch_dict[\"brain_mask\"].bool()\n",
    "            y_spacing = batch_dict[\"full_res_spacing\"]\n",
    "            y_coords = einops.rearrange(\n",
    "                batch_dict[\"full_res_real_coords\"], \"b coord x y z -> b x y z coord\"\n",
    "            )\n",
    "\n",
    "            optim_encoder.zero_grad()\n",
    "            optim_decoder.zero_grad()\n",
    "            optim_recon_decoder.zero_grad()\n",
    "\n",
    "            # Append LR coordinates to the end of the input LR DWIs.\n",
    "            x = torch.cat(\n",
    "                [x, einops.rearrange(x_coords, \"b x y z coord -> b coord x y z\")], dim=1\n",
    "            )\n",
    "            ctx_v = encoder(x)\n",
    "\n",
    "            if not train_recon:\n",
    "                y_mask_broad = torch.broadcast_to(y_mask, y.shape)\n",
    "                y_coord_mask = einops.rearrange(y_mask, \"b 1 x y z -> b x y z 1\")\n",
    "                pred_fodf = decoder(\n",
    "                    context_v=ctx_v,\n",
    "                    context_real_coords=x_coords,\n",
    "                    query_real_coords=y_coords,\n",
    "                    query_coords_mask=y_coord_mask,\n",
    "                    affine_context_vox2real=x_affine_vox2real,\n",
    "                    context_spacing=x_spacing,\n",
    "                    query_spacing=y_spacing,\n",
    "                )\n",
    "\n",
    "                # Scale prediction and target to weigh the loss.\n",
    "                # Transform coefficients to a standard normal distribution.\n",
    "                # 45 x n_voxels\n",
    "                pred_fodf_standardized = (\n",
    "                    pred_fodf - LOSS_ODF_COEFF_MEANS\n",
    "                ) / LOSS_ODF_COEFF_STDS\n",
    "                y_standardized = (y - LOSS_ODF_COEFF_MEANS) / LOSS_ODF_COEFF_STDS\n",
    "                # Tissue weights\n",
    "                tissue_weight_mask = y_mask.to(pred_fodf)\n",
    "                tissue_weight_mask[batch_dict[\"gm_mask\"]] = 0.3\n",
    "                tissue_weight_mask[batch_dict[\"csf_mask\"]] = 0.1\n",
    "                tissue_weight_mask[batch_dict[\"wm_mask\"]] = 1.0\n",
    "                pred_fodf_standardized *= tissue_weight_mask\n",
    "                y_standardized *= tissue_weight_mask\n",
    "                # Calculate loss over weighted prediction and target.\n",
    "                loss_fodf = loss_fn(\n",
    "                    pred_fodf_standardized[y_mask_broad], y_standardized[y_mask_broad]\n",
    "                )\n",
    "\n",
    "                # loss_fodf = loss_fn(pred_fodf[y_mask_broad], y[y_mask_broad])\n",
    "                loss_recon = y.new_zeros(1)\n",
    "                recon_pred = None\n",
    "            else:\n",
    "                recon_pred = recon_decoder(ctx_v)\n",
    "                # Index bvals to be 2 b=0s, 2 b=1000s, and 2 b=3000s.\n",
    "                recon_y = torch.cat(\n",
    "                    [x[:, (0, 1, 2, 11, 12, 13)], x_coords.movedim(-1, 1)], dim=1\n",
    "                )\n",
    "                loss_recon = recon_loss_fn(recon_pred, recon_y)\n",
    "                loss_fodf = recon_y.new_zeros(1)\n",
    "                pred_fodf = None\n",
    "\n",
    "            loss = loss_fodf + loss_recon\n",
    "\n",
    "            fabric.backward(loss)\n",
    "            for model, model_optim in zip(\n",
    "                (encoder, decoder, recon_decoder),\n",
    "                (optim_encoder, optim_decoder, optim_recon_decoder),\n",
    "            ):\n",
    "                if train_recon and model is decoder:\n",
    "                    continue\n",
    "                elif not train_recon and model is recon_decoder:\n",
    "                    continue\n",
    "                fabric.clip_gradients(\n",
    "                    model,\n",
    "                    model_optim,\n",
    "                    max_norm=5.0,\n",
    "                    norm_type=2,\n",
    "                    error_if_nonfinite=True,\n",
    "                )\n",
    "\n",
    "            optim_encoder.step()\n",
    "            optim_decoder.step()\n",
    "            optim_recon_decoder.step()\n",
    "\n",
    "            to_track = {\n",
    "                \"loss\": loss.detach().cpu().item(),\n",
    "            }\n",
    "            # Depending on whether or not the reconstruction decoder is training,\n",
    "            # select which metrics to track at this time.\n",
    "            if train_recon:\n",
    "                to_track = {\n",
    "                    **to_track,\n",
    "                    **{\n",
    "                        \"loss_recon\": loss_recon.detach().cpu().item(),\n",
    "                    },\n",
    "                }\n",
    "            else:\n",
    "                to_track = {\n",
    "                    **to_track,\n",
    "                    **{\n",
    "                        \"loss_pred_fodf\": loss_fodf.detach().cpu().item(),\n",
    "                    },\n",
    "                }\n",
    "            if fabric.is_global_zero:\n",
    "                aim_run.track(\n",
    "                    to_track,\n",
    "                    context={\n",
    "                        \"subset\": \"train\",\n",
    "                    },\n",
    "                    step=step,\n",
    "                    epoch=epoch,\n",
    "                )\n",
    "                losses[\"loss\"].append(loss.detach().cpu().item())\n",
    "                losses[\"epoch\"].append(epoch)\n",
    "                losses[\"step\"].append(step)\n",
    "\n",
    "            fabric.print(\n",
    "                f\"| {loss.detach().cpu().item()}\",\n",
    "                end=\" \",\n",
    "                flush=(step % 10) == 0,\n",
    "            )\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        optim_encoder.zero_grad(set_to_none=True)\n",
    "        optim_decoder.zero_grad(set_to_none=True)\n",
    "        optim_recon_decoder.zero_grad(set_to_none=True)\n",
    "        # Delete some training inputs to relax memory constraints in whole-\n",
    "        # volume inference inside validation step.\n",
    "        del x, x_coords, y, y_coords, pred_fodf, recon_pred\n",
    "\n",
    "        fabric.print(\"\\n==Validation==\", flush=True)\n",
    "        fabric.barrier()\n",
    "        if fabric.is_global_zero:\n",
    "            aim_run, val_viz_subj_id, val_scores = validate_stage(\n",
    "                fabric,\n",
    "                encoder,\n",
    "                decoder,\n",
    "                val_dataloader=val_dataloader,\n",
    "                step=step,\n",
    "                epoch=epoch,\n",
    "                aim_run=aim_run,\n",
    "                val_viz_subj_id=val_viz_subj_id,\n",
    "            )\n",
    "            curr_val_score = val_scores.detach().cpu().mean().item()\n",
    "            fabric.print(str(curr_val_score))\n",
    "        fabric.barrier()\n",
    "\n",
    "        # Start saving best performing models if the previous best val score was\n",
    "        # surpassed, and the current number of epcohs is >= half the total training\n",
    "        # amount.\n",
    "        if (curr_val_score < (curr_best_val_score - (0.05 * curr_best_val_score))) and (\n",
    "            epoch >= epochs / 3\n",
    "        ):\n",
    "            fabric.print(\"Saving new best validation score\")\n",
    "            fabric.barrier()\n",
    "            if fabric.is_global_zero:\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"encoder\": encoder.state_dict(),\n",
    "                        \"decoder\": decoder.state_dict(),\n",
    "                        \"epoch\": epoch,\n",
    "                        \"step\": step,\n",
    "                        \"aim_run_hash\": aim_run.hash,\n",
    "                        \"optim_encoder\": optim_encoder.state_dict(),\n",
    "                        \"optim_decoder\": optim_decoder.state_dict(),\n",
    "                        \"recon_decoder\": recon_decoder.state_dict(),\n",
    "                        \"optim_recon_decoder\": optim_recon_decoder.state_dict(),\n",
    "                    },\n",
    "                    Path(tmp_res_dir)\n",
    "                    / f\"best_val_score_state_dict_epoch_{epoch}_step_{step}.pt\",\n",
    "                )\n",
    "                curr_best_val_score = curr_val_score\n",
    "            fabric.barrier()\n",
    "\n",
    "        if epoch in checkpoint_epochs:\n",
    "            fabric.print(f\"Saving checkpoint {curr_checkpoint}\")\n",
    "            fabric.barrier()\n",
    "            if fabric.is_global_zero:\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"encoder\": encoder.state_dict(),\n",
    "                        \"decoder\": decoder.state_dict(),\n",
    "                        \"epoch\": epoch,\n",
    "                        \"step\": step,\n",
    "                        \"aim_run_hash\": aim_run.hash,\n",
    "                        \"optim_encoder\": optim_encoder.state_dict(),\n",
    "                        \"optim_decoder\": optim_decoder.state_dict(),\n",
    "                        \"recon_decoder\": recon_decoder.state_dict(),\n",
    "                        \"optim_recon_decoder\": optim_recon_decoder.state_dict(),\n",
    "                    },\n",
    "                    Path(tmp_res_dir)\n",
    "                    / f\"checkpoint_{curr_checkpoint}_state_dict_epoch_{epoch}_step_{step}.pt\",\n",
    "                )\n",
    "            fabric.barrier()\n",
    "            curr_checkpoint += 1\n",
    "\n",
    "except KeyboardInterrupt as e:\n",
    "    if fabric.is_global_zero:\n",
    "        aim_run.add_tag(\"STOPPED\")\n",
    "        (tmp_res_dir / \"STOPPED\").touch()\n",
    "    raise e\n",
    "except Exception as e:\n",
    "    if fabric.is_global_zero:\n",
    "        aim_run.add_tag(\"FAILED\")\n",
    "        (tmp_res_dir / \"FAILED\").touch()\n",
    "    raise e\n",
    "finally:\n",
    "    if fabric.is_global_zero:\n",
    "        aim_run.close()\n",
    "\n",
    "# Sync all pytorch-lightning processes.\n",
    "fabric.barrier()\n",
    "if fabric.is_global_zero:\n",
    "    fabric.print(\"Saving model state dict\")\n",
    "    torch.save(\n",
    "        {\n",
    "            \"encoder\": encoder.state_dict(),\n",
    "            \"decoder\": decoder.state_dict(),\n",
    "            \"recon_decoder\": recon_decoder.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"step\": step,\n",
    "            \"aim_run_hash\": aim_run.hash,\n",
    "            \"optim_encoder\": optim_encoder.state_dict(),\n",
    "            \"optim_decoder\": optim_decoder.state_dict(),\n",
    "            \"optim_recon_decoder\": optim_recon_decoder.state_dict(),\n",
    "        },\n",
    "        Path(tmp_res_dir) / f\"final_state_dict_epoch_{epoch}_step_{step}.pt\",\n",
    "    )\n",
    "    fabric.print(\"=\" * 40)\n",
    "    losses = pd.DataFrame.from_dict(losses)\n",
    "    losses.to_csv(Path(tmp_res_dir) / \"train_losses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35b2f33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "comment_magics": true,
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "pitn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
