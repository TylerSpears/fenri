{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73408619",
   "metadata": {},
   "source": [
    "# Continuous-Space Super-Resolution of fODFs in Diffusion MRI\n",
    "\n",
    "Code by:\n",
    "\n",
    "Tyler Spears - tas6hh@virginia.edu\n",
    "\n",
    "Dr. Tom Fletcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64ef468",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8d7897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Automatically re-import project-specific modules.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# imports\n",
    "import collections\n",
    "import copy\n",
    "import datetime\n",
    "import functools\n",
    "import inspect\n",
    "import io\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import pdb\n",
    "import random\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "import typing\n",
    "import warnings\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from pprint import pprint as ppr\n",
    "\n",
    "import aim\n",
    "import dotenv\n",
    "import einops\n",
    "\n",
    "# visualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import monai\n",
    "\n",
    "# Data management libraries.\n",
    "import nibabel as nib\n",
    "import nibabel.processing\n",
    "\n",
    "# Computation & ML libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import skimage\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchinfo\n",
    "from box import Box\n",
    "from icecream import ic\n",
    "from lightning_fabric.fabric import Fabric\n",
    "from natsort import natsorted\n",
    "\n",
    "import pitn\n",
    "\n",
    "plt.rcParams.update({\"figure.autolayout\": True})\n",
    "plt.rcParams.update({\"figure.facecolor\": [1.0, 1.0, 1.0, 1.0]})\n",
    "plt.rcParams.update({\"image.cmap\": \"gray\"})\n",
    "\n",
    "# Set print options for ndarrays/tensors.\n",
    "np.set_printoptions(suppress=True, threshold=100, linewidth=88)\n",
    "torch.set_printoptions(sci_mode=False, threshold=100, linewidth=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c68d20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "direnv: loading ~/Projects/pitn/.envrc\n",
      "\n",
      "EnvironmentNameNotFound: Could not find conda environment: pitn\n",
      "You can list all discoverable environments with `conda info --envs`.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update notebook's environment variables with direnv.\n",
    "# This requires the python-dotenv package, and direnv be installed on the system\n",
    "# This will not work on Windows.\n",
    "# NOTE: This is kind of hacky, and not necessarily safe. Be careful...\n",
    "# Libraries needed on the python side:\n",
    "# - os\n",
    "# - subprocess\n",
    "# - io\n",
    "# - dotenv\n",
    "\n",
    "# Form command to be run in direnv's context. This command will print out\n",
    "# all environment variables defined in the subprocess/sub-shell.\n",
    "command = \"direnv exec {} /usr/bin/env\".format(os.getcwd())\n",
    "# Run command in a new subprocess.\n",
    "proc = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, cwd=os.getcwd())\n",
    "# Store and format the subprocess' output.\n",
    "proc_out = proc.communicate()[0].strip().decode(\"utf-8\")\n",
    "# Use python-dotenv to load the environment variables by using the output of\n",
    "# 'direnv exec ...' as a 'dummy' .env file.\n",
    "dotenv.load_dotenv(stream=io.StringIO(proc_out), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4126c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Device IDX  0\n",
      "CUDA Current Device  0\n",
      "CUDA Device properties:  _CudaDeviceProperties(name='NVIDIA RTX A5000', major=8, minor=6, total_memory=24247MB, multi_processor_count=64)\n",
      "CuDNN convolution optimization enabled.\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# torch setup\n",
    "# allow for CUDA usage, if available\n",
    "if torch.cuda.is_available():\n",
    "    # Pick only one device for the default, may use multiple GPUs for training later.\n",
    "    if \"CUDA_PYTORCH_DEVICE_IDX\" in os.environ.keys():\n",
    "        dev_idx = int(os.environ[\"CUDA_PYTORCH_DEVICE_IDX\"])\n",
    "    else:\n",
    "        dev_idx = 0\n",
    "    device = torch.device(f\"cuda:{dev_idx}\")\n",
    "    print(\"CUDA Device IDX \", dev_idx)\n",
    "    torch.cuda.set_device(device)\n",
    "    print(\"CUDA Current Device \", torch.cuda.current_device())\n",
    "    print(\"CUDA Device properties: \", torch.cuda.get_device_properties(device))\n",
    "    # The flag below controls whether to allow TF32 on matmul. This flag defaults to False\n",
    "    # in PyTorch 1.12 and later.\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    # See\n",
    "    # <https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices>\n",
    "    # for details.\n",
    "\n",
    "    # Activate cudnn benchmarking to optimize convolution algorithm speed.\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        print(\"CuDNN convolution optimization enabled.\")\n",
    "        # The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# keep device as the cpu\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d3a5c6",
   "metadata": {},
   "source": [
    "## Experiment & Parameters Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cd75175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Config file not loaded\n"
     ]
    }
   ],
   "source": [
    "p = Box(default_box=True)\n",
    "# Experiment defaults, can be overridden in a config file.\n",
    "\n",
    "p.results_dir = \"/data/srv/outputs/pitn/results/runs\"\n",
    "p.tmp_results_dir = \"/data/srv/outputs/pitn/results/tmp\"\n",
    "# p.test.subj_ids = [\"299154\"]\n",
    "p.test.subj_ids = list(\n",
    "    map(\n",
    "        str,\n",
    "        [\n",
    "            # 100307,\n",
    "            # 104820,\n",
    "            # 110613,\n",
    "            # 118730,\n",
    "            # 121618,\n",
    "            # 122317,\n",
    "            # 122822,\n",
    "            # 123925,\n",
    "            # 124422,\n",
    "            # 127933,\n",
    "            # 131924,\n",
    "            # 133827,\n",
    "            # 144428,\n",
    "            # 145127,\n",
    "            # 147737,\n",
    "            # 149236,\n",
    "            # 154532,\n",
    "            # 164939,\n",
    "            # 165032,\n",
    "            # 169545,\n",
    "            # 173536,\n",
    "            # 173940,\n",
    "            # 178849,\n",
    "            # 179346,\n",
    "            # 180836,\n",
    "            # 185038,\n",
    "            # 195647,\n",
    "            # 195849,\n",
    "            # 202113,\n",
    "            # 204016,\n",
    "            # 275645,\n",
    "            # 293748,\n",
    "            # 298051,\n",
    "            # 300618,\n",
    "            # 353740,\n",
    "            # 385450,\n",
    "            # 419239,\n",
    "            # 445543,\n",
    "            # 465852,\n",
    "            # 495255,\n",
    "            # 500222,\n",
    "            # 510225,\n",
    "            # 518746,\n",
    "            # 529549,\n",
    "            # 645450,\n",
    "            # 654350,\n",
    "            749058,\n",
    "            # 767464,\n",
    "            # 800941,\n",
    "            # 803240,\n",
    "            # 812746,\n",
    "            # 825048,\n",
    "            # 843151,\n",
    "            # 849971,\n",
    "            # 896778,\n",
    "            # 929464,\n",
    "            # 933253,\n",
    "            # 965367,\n",
    "            # 987074,\n",
    "            # 993675,\n",
    "            # 994273,\n",
    "            # 581450,\n",
    "            # 251833,\n",
    "            # 191336,\n",
    "            # 126426,\n",
    "            # 859671,\n",
    "            # 200210,\n",
    "            # 360030,\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "p.model_weight_f = str(\n",
    "    Path(p.tmp_results_dir) / \"2023-02-22T01_29_25/state_dict_epoch_249_step_50000.pt\"\n",
    ")\n",
    "# p.model_weight_f = str(\n",
    "#     Path(p.tmp_results_dir) / \"2023-02-09T21_09_47/state_dict_epoch_174_step_35000.pt\"\n",
    "# )\n",
    "p.target_vox_size = 2.0\n",
    "###############################################\n",
    "# Network/model parameters.\n",
    "p.encoder = dict(\n",
    "    interior_channels=80,\n",
    "    out_channels=128,\n",
    "    n_res_units=3,\n",
    "    n_dense_units=3,\n",
    "    activate_fn=\"relu\",\n",
    ")\n",
    "p.decoder = dict(\n",
    "    context_v_features=128,\n",
    "    in_features=p.encoder.out_channels,\n",
    "    out_features=45,\n",
    "    m_encode_num_freqs=36,\n",
    "    sigma_encode_scale=3.0,\n",
    ")\n",
    "\n",
    "\n",
    "# If a config file exists, override the defaults with those values.\n",
    "try:\n",
    "    if \"PITN_CONFIG\" in os.environ.keys():\n",
    "        config_fname = Path(os.environ[\"PITN_CONFIG\"])\n",
    "    else:\n",
    "        config_fname = pitn.utils.system.get_file_glob_unique(Path(\".\"), r\"config.*\")\n",
    "    f_type = config_fname.suffix.casefold()\n",
    "    if f_type in {\".yaml\", \".yml\"}:\n",
    "        f_params = Box.from_yaml(filename=config_fname)\n",
    "    elif f_type == \".json\":\n",
    "        f_params = Box.from_json(filename=config_fname)\n",
    "    elif f_type == \".toml\":\n",
    "        f_params = Box.from_toml(filename=config_fname)\n",
    "    else:\n",
    "        raise RuntimeError()\n",
    "\n",
    "    p.merge_update(f_params)\n",
    "\n",
    "except:\n",
    "    print(\"WARNING: Config file not loaded\")\n",
    "    pass\n",
    "\n",
    "# Remove the default_box behavior now that params have been fully read in.\n",
    "_p = Box(default_box=False)\n",
    "_p.merge_update(p)\n",
    "p = _p\n",
    "\n",
    "print(\"Voxel size: \", p.target_vox_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1047b3",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6a12cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hcp_full_res_data_dir = Path(\"/data/srv/data/pitn/hcp\")\n",
    "hcp_full_res_fodf_dir = Path(\"/data/srv/outputs/pitn/hcp/full-res/fodf\")\n",
    "hcp_low_res_data_dir = Path(\"/data/srv/outputs/pitn/hcp/downsample/scale-2.00mm/vol\")\n",
    "hcp_low_res_fodf_dir = Path(\"/data/srv/outputs/pitn/hcp/downsample/scale-2.00mm/fodf\")\n",
    "\n",
    "assert hcp_full_res_data_dir.exists()\n",
    "assert hcp_full_res_fodf_dir.exists()\n",
    "assert hcp_low_res_data_dir.exists()\n",
    "assert hcp_low_res_fodf_dir.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79432e4d",
   "metadata": {},
   "source": [
    "### Create Patch-Based Training Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bf5a57",
   "metadata": {},
   "source": [
    "### Validation & Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9105038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 1/1 [00:05<00:00,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Warnings caught:\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [2. 2. 2. 1.] to [  2.           2.           2.         148.08121716]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25 1.  ] to [  1.25        1.25        1.25      147.5679928]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25  nan] to [  1.25        1.25        1.25      147.5679928]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [ 2.  2.  2. nan] to [  2.           2.           2.         148.08121716]\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings(record=True) as warn_list:\n",
    "\n",
    "    # Validation dataset.\n",
    "    test_paths_dataset = pitn.data.datasets.HCPfODFINRDataset(\n",
    "        subj_ids=p.test.subj_ids[:2],  #!DEBUG\n",
    "        dwi_root_dir=hcp_full_res_data_dir,\n",
    "        fodf_root_dir=hcp_full_res_fodf_dir,\n",
    "        lr_dwi_root_dir=hcp_low_res_data_dir,\n",
    "        lr_fodf_root_dir=hcp_low_res_fodf_dir,\n",
    "    )\n",
    "    cached_test_dataset = monai.data.CacheDataset(\n",
    "        test_paths_dataset,\n",
    "        transform=test_paths_dataset.default_pre_sample_tf(0, skip_sample_mask=True),\n",
    "        copy_cache=False,\n",
    "        num_workers=2,\n",
    "    )\n",
    "    test_dataset = pitn.data.datasets.HCPfODFINRWholeVolDataset(\n",
    "        cached_test_dataset,\n",
    "        transform=pitn.data.datasets.HCPfODFINRWholeVolDataset.default_tf(),\n",
    "    )\n",
    "\n",
    "    # # Test dataset.\n",
    "    # # The test dataset won't be cached, as each image should only be loaded once.\n",
    "    # test_paths_dataset = pitn.data.datasets.HCPfODFINRDataset(\n",
    "    #     subj_ids=p.test.subj_ids,\n",
    "    #     dwi_root_dir=hcp_full_res_data_dir,\n",
    "    #     fodf_root_dir=hcp_full_res_fodf_dir,\n",
    "    #     lr_dwi_root_dir=hcp_low_res_data_dir,\n",
    "    #     lr_fodf_root_dir=hcp_low_res_fodf_dir,\n",
    "    #     transform=pitn.data.datasets.HCPfODFINRDataset.default_pre_sample_tf(\n",
    "    #         0, skip_sample_mask=True\n",
    "    #     ),\n",
    "    # )\n",
    "    # test_dataset = pitn.data.datasets.HCPfODFINRWholeVolDataset(\n",
    "    #     test_paths_dataset,\n",
    "    #     transform=pitn.data.datasets.HCPfODFINRWholeVolDataset.default_tf(),\n",
    "    # )\n",
    "\n",
    "print(\"=\" * 10)\n",
    "print(\"Warnings caught:\")\n",
    "ws = \"\\n\".join(\n",
    "    [\n",
    "        warnings.formatwarning(\n",
    "            w.message, w.category, w.filename, w.lineno, w.file, w.line\n",
    "        )\n",
    "        for w in warn_list\n",
    "    ]\n",
    ")\n",
    "ws = \"\\n\".join(filter(lambda s: bool(s.strip()), ws.splitlines()))\n",
    "print(ws, flush=True)\n",
    "print(\"=\" * 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "623bd3d7",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2721d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "# Break ISO format because many programs don't like having colons ':' in a filename.\n",
    "ts = ts.replace(\":\", \"_\")\n",
    "# tmp_res_dir = Path(p.tmp_results_dir) / \"_\".join([ts, \"super_res_odf_test\"])\n",
    "tmp_res_dir = (\n",
    "    Path(p.tmp_results_dir) / \"2023-05-24T12_03_39__super_res_odf_multi-res_viz\"\n",
    ")\n",
    "tmp_res_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73b1d32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = monai.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d756642",
   "metadata": {},
   "source": [
    "### INR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cccb282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding model\n",
    "class INREncoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        interior_channels: int,\n",
    "        out_channels: int,\n",
    "        n_res_units: int,\n",
    "        n_dense_units: int,\n",
    "        activate_fn,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.init_kwargs = dict(\n",
    "            in_channels=in_channels,\n",
    "            interior_channels=interior_channels,\n",
    "            out_channels=out_channels,\n",
    "            n_res_units=n_res_units,\n",
    "            n_dense_units=n_dense_units,\n",
    "            activate_fn=activate_fn,\n",
    "        )\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.interior_channels = interior_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        if isinstance(activate_fn, str):\n",
    "            activate_fn = pitn.utils.torch_lookups.activation[activate_fn]\n",
    "\n",
    "        self._activation_fn_init = activate_fn\n",
    "        self.activate_fn = activate_fn()\n",
    "\n",
    "        # Pad to maintain the same input shape.\n",
    "        self.pre_conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(\n",
    "                self.in_channels,\n",
    "                self.in_channels,\n",
    "                kernel_size=1,\n",
    "                padding=\"same\",\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "            self.activate_fn,\n",
    "            torch.nn.Conv3d(\n",
    "                self.in_channels,\n",
    "                self.interior_channels,\n",
    "                kernel_size=3,\n",
    "                padding=\"same\",\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Construct the densely-connected cascading layers.\n",
    "        # Create n_dense_units number of dense units.\n",
    "        top_level_units = list()\n",
    "        for _ in range(n_dense_units):\n",
    "            # Create n_res_units number of residual units for every dense unit.\n",
    "            res_layers = list()\n",
    "            for _ in range(n_res_units):\n",
    "                res_layers.append(\n",
    "                    pitn.nn.layers.ResBlock3dNoBN(\n",
    "                        self.interior_channels,\n",
    "                        kernel_size=3,\n",
    "                        activate_fn=activate_fn,\n",
    "                        padding=\"same\",\n",
    "                        padding_mode=\"reflect\",\n",
    "                    )\n",
    "                )\n",
    "            top_level_units.append(\n",
    "                pitn.nn.layers.DenseCascadeBlock3d(self.interior_channels, *res_layers)\n",
    "            )\n",
    "\n",
    "        # Wrap everything into a densely-connected cascade.\n",
    "        self.cascade = pitn.nn.layers.DenseCascadeBlock3d(\n",
    "            self.interior_channels, *top_level_units\n",
    "        )\n",
    "\n",
    "        self.post_conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(\n",
    "                self.interior_channels,\n",
    "                self.interior_channels,\n",
    "                kernel_size=5,\n",
    "                padding=\"same\",\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "            self.activate_fn,\n",
    "            torch.nn.Conv3d(\n",
    "                self.interior_channels,\n",
    "                self.out_channels,\n",
    "                kernel_size=3,\n",
    "                padding=\"same\",\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "            self.activate_fn,\n",
    "            torch.nn.ReplicationPad3d((1, 0, 1, 0, 1, 0)),\n",
    "            torch.nn.AvgPool3d(kernel_size=2, stride=1),\n",
    "            torch.nn.Conv3d(\n",
    "                self.out_channels,\n",
    "                self.out_channels,\n",
    "                kernel_size=1,\n",
    "                padding=\"same\",\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "        )\n",
    "        # self.post_conv = torch.nn.Conv3d(\n",
    "        #     self.interior_channels,\n",
    "        #     self.out_channels,\n",
    "        #     kernel_size=3,\n",
    "        #     padding=\"same\",\n",
    "        #     padding_mode=\"reflect\",\n",
    "        # )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        y = self.pre_conv(x)\n",
    "        y = self.activate_fn(y)\n",
    "        y = self.cascade(y)\n",
    "        y = self.activate_fn(y)\n",
    "        y = self.post_conv(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class ReducedDecoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        context_v_features: int,\n",
    "        out_features: int,\n",
    "        m_encode_num_freqs: int,\n",
    "        sigma_encode_scale: float,\n",
    "        in_features=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.init_kwargs = dict(\n",
    "            context_v_features=context_v_features,\n",
    "            out_features=out_features,\n",
    "            m_encode_num_freqs=m_encode_num_freqs,\n",
    "            sigma_encode_scale=sigma_encode_scale,\n",
    "            in_features=in_features,\n",
    "        )\n",
    "\n",
    "        # Determine the number of input features needed for the MLP.\n",
    "        # The order for concatenation is\n",
    "        # 1) ctx feats over the low-res input space, unfolded over a 3x3x3 window\n",
    "        # ~~2) target voxel shape~~\n",
    "        # 3) absolute coords of this forward pass' prediction target\n",
    "        # 4) absolute coords of the high-res target voxel\n",
    "        # ~~5) relative coords between high-res target coords and this forward pass'\n",
    "        #    prediction target, normalized by low-res voxel shape~~\n",
    "        # 6) encoding of relative coords\n",
    "        self.context_v_features = context_v_features\n",
    "        self.ndim = 3\n",
    "        self.m_encode_num_freqs = m_encode_num_freqs\n",
    "        self.sigma_encode_scale = torch.as_tensor(sigma_encode_scale)\n",
    "        self.n_encode_features = self.ndim * 2 * self.m_encode_num_freqs\n",
    "        self.n_coord_features = 2 * self.ndim + self.n_encode_features\n",
    "        self.internal_features = self.context_v_features + self.n_coord_features\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # \"Swish\" function, recommended in MeshFreeFlowNet\n",
    "        activate_cls = torch.nn.SiLU\n",
    "        self.activate_fn = activate_cls(inplace=True)\n",
    "        # Optional resizing linear layer, if the input size should be different than\n",
    "        # the hidden layer size.\n",
    "        if self.in_features is not None:\n",
    "            self.lin_pre = torch.nn.Linear(self.in_features, self.context_v_features)\n",
    "            self.norm_pre = None\n",
    "        else:\n",
    "            self.lin_pre = None\n",
    "            self.norm_pre = None\n",
    "        self.norm_pre = None\n",
    "\n",
    "        # Internal hidden layers are two res MLPs.\n",
    "        self.internal_res_repr = torch.nn.ModuleList(\n",
    "            [\n",
    "                pitn.nn.inr.SkipMLPBlock(\n",
    "                    n_context_features=self.context_v_features,\n",
    "                    n_coord_features=self.n_coord_features,\n",
    "                    n_dense_layers=3,\n",
    "                    activate_fn=activate_cls,\n",
    "                )\n",
    "                for _ in range(2)\n",
    "            ]\n",
    "        )\n",
    "        self.lin_post = torch.nn.Linear(self.context_v_features, self.out_features)\n",
    "\n",
    "    def encode_relative_coord(self, coords):\n",
    "        c = einops.rearrange(coords, \"b d x y z -> (b x y z) d\")\n",
    "        sigma = self.sigma_encode_scale.expand_as(c).to(c)[..., None]\n",
    "        encode_pos = pitn.nn.inr.fourier_position_encoding(\n",
    "            c, sigma_scale=sigma, m_num_freqs=self.m_encode_num_freqs\n",
    "        )\n",
    "\n",
    "        encode_pos = einops.rearrange(\n",
    "            encode_pos,\n",
    "            \"(b x y z) d -> b d x y z\",\n",
    "            x=coords.shape[2],\n",
    "            y=coords.shape[3],\n",
    "            z=coords.shape[4],\n",
    "        )\n",
    "        return encode_pos\n",
    "\n",
    "    def sub_grid_forward(\n",
    "        self,\n",
    "        context_val,\n",
    "        context_coord,\n",
    "        query_coord,\n",
    "        context_vox_size,\n",
    "        # query_vox_size,\n",
    "        return_rel_context_coord=False,\n",
    "    ):\n",
    "        # Take relative coordinate difference between the current context\n",
    "        # coord and the query coord.\n",
    "        rel_context_coord = query_coord - context_coord\n",
    "        # Also normalize to [0, 1) by subtracting the lower bound of differences\n",
    "        # (- voxel size) and dividing by 2xupper bound (2 x voxel size).\n",
    "        rel_norm_context_coord = (rel_context_coord - -context_vox_size) / (\n",
    "            2 * context_vox_size\n",
    "        )\n",
    "        rel_norm_context_coord = torch.round_(rel_norm_context_coord, decimals=5)\n",
    "        assert (rel_norm_context_coord >= 0).all() and (\n",
    "            rel_norm_context_coord <= 1.0\n",
    "        ).all()\n",
    "        encoded_rel_norm_context_coord = self.encode_relative_coord(\n",
    "            rel_norm_context_coord\n",
    "        )\n",
    "\n",
    "        # Perform forward pass of the MLP.\n",
    "        if self.norm_pre is not None:\n",
    "            context_val = self.norm_pre(context_val)\n",
    "        context_feats = einops.rearrange(context_val, \"b c x y z -> (b x y z) c\")\n",
    "\n",
    "        # q_vox_size = query_vox_size.expand_as(rel_norm_context_coord)\n",
    "        coord_feats = (\n",
    "            # q_vox_size,\n",
    "            context_coord,\n",
    "            query_coord,\n",
    "            # rel_norm_context_coord,\n",
    "            encoded_rel_norm_context_coord,\n",
    "        )\n",
    "        coord_feats = torch.cat(coord_feats, dim=1)\n",
    "        spatial_layout = {\n",
    "            \"b\": coord_feats.shape[0],\n",
    "            \"x\": coord_feats.shape[2],\n",
    "            \"y\": coord_feats.shape[3],\n",
    "            \"z\": coord_feats.shape[4],\n",
    "        }\n",
    "\n",
    "        coord_feats = einops.rearrange(coord_feats, \"b c x y z -> (b x y z) c\")\n",
    "        x_coord = coord_feats\n",
    "        sub_grid_pred = context_feats\n",
    "\n",
    "        if self.lin_pre is not None:\n",
    "            sub_grid_pred = self.lin_pre(sub_grid_pred)\n",
    "            sub_grid_pred = self.activate_fn(sub_grid_pred)\n",
    "\n",
    "        for l in self.internal_res_repr:\n",
    "            sub_grid_pred, x_coord = l(sub_grid_pred, x_coord)\n",
    "        # The SkipMLPBlock contains the residual addition, so no need to add here.\n",
    "        sub_grid_pred = self.lin_post(sub_grid_pred)\n",
    "        sub_grid_pred = einops.rearrange(\n",
    "            sub_grid_pred, \"(b x y z) c -> b c x y z\", **spatial_layout\n",
    "        )\n",
    "        if return_rel_context_coord:\n",
    "            ret = (sub_grid_pred, rel_context_coord)\n",
    "        else:\n",
    "            ret = sub_grid_pred\n",
    "        return ret\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        context_v,\n",
    "        context_spatial_extent,\n",
    "        affine_context_vox2mm,\n",
    "        # query_vox_size,\n",
    "        query_coord,\n",
    "    ) -> torch.Tensor:\n",
    "        # if query_vox_size.ndim == 2:\n",
    "        #     query_vox_size = query_vox_size[:, :, None, None, None]\n",
    "        context_vox_size = torch.abs(\n",
    "            context_spatial_extent[..., 1, 1, 1] - context_spatial_extent[..., 0, 0, 0]\n",
    "        )\n",
    "        context_vox_size = context_vox_size[:, :, None, None, None]\n",
    "\n",
    "        batch_size = query_coord.shape[0]\n",
    "\n",
    "        query_coord_in_context_fov = query_coord - torch.amin(\n",
    "            context_spatial_extent, (2, 3, 4), keepdim=True\n",
    "        )\n",
    "        query_bottom_back_left_corner_coord = (\n",
    "            query_coord_in_context_fov - (query_coord_in_context_fov % context_vox_size)\n",
    "        ) + torch.amin(context_spatial_extent, (2, 3, 4), keepdim=True)\n",
    "        context_vox_bottom_back_left_corner = pitn.affine.coord_transform_3d(\n",
    "            query_bottom_back_left_corner_coord.movedim(1, -1),\n",
    "            torch.linalg.inv(affine_context_vox2mm),\n",
    "        )\n",
    "        context_vox_bottom_back_left_corner = (\n",
    "            context_vox_bottom_back_left_corner.movedim(-1, 1)\n",
    "        )\n",
    "        batch_vox_idx = einops.repeat(\n",
    "            torch.arange(\n",
    "                batch_size,\n",
    "                dtype=context_vox_bottom_back_left_corner.dtype,\n",
    "                device=context_vox_bottom_back_left_corner.device,\n",
    "            ),\n",
    "            \"idx_b -> idx_b 1 i j k\",\n",
    "            idx_b=batch_size,\n",
    "            i=query_coord.shape[2],\n",
    "            j=query_coord.shape[3],\n",
    "            k=query_coord.shape[4],\n",
    "        )\n",
    "        #     (context_vox_bottom_back_left_corner.shape[0], 1)\n",
    "        #     + tuple(context_vox_bottom_back_left_corner.shape[2:])\n",
    "        # )\n",
    "        context_vox_bottom_back_left_corner = torch.cat(\n",
    "            [batch_vox_idx, context_vox_bottom_back_left_corner], dim=1\n",
    "        )\n",
    "        context_vox_bottom_back_left_corner = (\n",
    "            context_vox_bottom_back_left_corner.floor().long()\n",
    "        )\n",
    "        # Slice with a range to keep the \"1\" dimension in place.\n",
    "        batch_vox_idx = context_vox_bottom_back_left_corner[:, 0:1]\n",
    "\n",
    "        y_weighted_accumulate = None\n",
    "        # Build the low-res representation one sub-window voxel index at a time.\n",
    "        # The indicators specify if the current voxel index that surrounds the\n",
    "        # query coordinate should be \"off the center voxel\" or not. If not, then\n",
    "        # the center voxel (read: no voxel offset from the center) is selected\n",
    "        # (for that dimension).\n",
    "        for (\n",
    "            corner_offset_i,\n",
    "            corner_offset_j,\n",
    "            corner_offset_k,\n",
    "        ) in itertools.product((0, 1), (0, 1), (0, 1)):\n",
    "            # Rebuild indexing tuple for each element of the sub-window\n",
    "            sub_window_offset_ijk = query_bottom_back_left_corner_coord.new_tensor(\n",
    "                [corner_offset_i, corner_offset_j, corner_offset_k]\n",
    "            ).reshape(1, -1, 1, 1, 1)\n",
    "            corner_offset_mm = sub_window_offset_ijk * context_vox_size\n",
    "\n",
    "            i_idx = context_vox_bottom_back_left_corner[:, 1:2] + corner_offset_i\n",
    "            j_idx = context_vox_bottom_back_left_corner[:, 2:3] + corner_offset_j\n",
    "            k_idx = context_vox_bottom_back_left_corner[:, 3:4] + corner_offset_k\n",
    "            context_val = context_v[\n",
    "                batch_vox_idx.flatten(),\n",
    "                :,\n",
    "                i_idx.flatten(),\n",
    "                j_idx.flatten(),\n",
    "                k_idx.flatten(),\n",
    "            ]\n",
    "            context_val = einops.rearrange(\n",
    "                context_val,\n",
    "                \"(b x y z) c -> b c x y z\",\n",
    "                b=batch_size,\n",
    "                x=query_coord.shape[2],\n",
    "                y=query_coord.shape[3],\n",
    "                z=query_coord.shape[4],\n",
    "            )\n",
    "            context_coord = query_bottom_back_left_corner_coord + corner_offset_mm\n",
    "\n",
    "            sub_grid_pred_ijk = self.sub_grid_forward(\n",
    "                context_val=context_val,\n",
    "                context_coord=context_coord,\n",
    "                query_coord=query_coord,\n",
    "                context_vox_size=context_vox_size,\n",
    "                # query_vox_size=query_vox_size,\n",
    "                return_rel_context_coord=False,\n",
    "            )\n",
    "            # Initialize the accumulated prediction after finding the\n",
    "            # output size; easier than trying to pre-compute it.\n",
    "            if y_weighted_accumulate is None:\n",
    "                y_weighted_accumulate = torch.zeros_like(sub_grid_pred_ijk)\n",
    "\n",
    "            sub_window_offset_ijk_compliment = torch.abs(1 - sub_window_offset_ijk)\n",
    "            sub_window_context_coord_compliment = (\n",
    "                query_bottom_back_left_corner_coord\n",
    "                + (sub_window_offset_ijk_compliment * context_vox_size)\n",
    "            )\n",
    "            w_sub_window_cube = torch.abs(\n",
    "                sub_window_context_coord_compliment - query_coord\n",
    "            )\n",
    "            w_sub_window = einops.reduce(\n",
    "                w_sub_window_cube, \"b side_len i j k -> b 1 i j k\", reduction=\"prod\"\n",
    "            ) / einops.reduce(\n",
    "                context_vox_size, \"b size 1 1 1 -> b 1 1 1 1\", reduction=\"prod\"\n",
    "            )\n",
    "\n",
    "            # Weigh this cell's prediction by the inverse of the distance\n",
    "            # from the cell physical coordinate to the true target\n",
    "            # physical coordinate. Normalize the weight by the inverse\n",
    "            # \"sum of the inverse distances\" found before.\n",
    "\n",
    "            # Accumulate weighted cell predictions to eventually create\n",
    "            # the final prediction.\n",
    "            y_weighted_accumulate += w_sub_window * sub_grid_pred_ijk\n",
    "            # del sub_grid_pred_ijk\n",
    "\n",
    "        y = y_weighted_accumulate\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00fd0ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning:\n",
      "\n",
      "Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "\n",
      "ic| 'Starting net inference.'\n",
      "Traceback (most recent call last):\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 1134, in _pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 311, in _pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\n",
      "  File \"/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py\", line 2062, in do_wait_suspend\n",
      "    keep_suspended = self._do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n",
      "  File \"/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py\", line 2098, in _do_wait_suspend\n",
      "    time.sleep(0.01)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/tas6hh/Projects/pitn/notebooks/continuous_sr/super-res_odf_pred.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/super-res_odf_pred.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m     ctx_v \u001b[39m=\u001b[39m encoder(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/super-res_odf_pred.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m     \u001b[39m# Whole-volume inference is memory-prohibitive, so use a sliding\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/super-res_odf_pred.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m     \u001b[39m# window inference method on the encoded volume.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/super-res_odf_pred.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m     pred_super_fodf \u001b[39m=\u001b[39m monai\u001b[39m.\u001b[39;49minferers\u001b[39m.\u001b[39;49msliding_window_inference(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/super-res_odf_pred.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m         super_coords\u001b[39m.\u001b[39;49mcpu(),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/super-res_odf_pred.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m         \u001b[39m# roi_size=(48, 48, 48),\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/super-res_odf_pred.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m         roi_size\u001b[39m=\u001b[39;49m(\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/super-res_odf_pred.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m         sw_batch_size\u001b[39m=\u001b[39;49msuper_coords\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/super-res_odf_pred.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=88'>89</a>\u001b[0m         predictor\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m q: decoder(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/super-res_odf_pred.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=89'>90</a>\u001b[0m             query_coord\u001b[39m=\u001b[39;49mq\u001b[39m.\u001b[39;49mto(device),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/super-res_odf_pred.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=90'>91</a>\u001b[0m             context_v\u001b[39m=\u001b[39;49mctx_v,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/super-res_odf_pred.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m             context_spatial_extent\u001b[39m=\u001b[39;49mx_coords,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/super-res_odf_pred.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m             affine_context_vox2mm\u001b[39m=\u001b[39;49mx_affine_vox2mm,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/super-res_odf_pred.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=93'>94</a>\u001b[0m         )\u001b[39m.\u001b[39;49mcpu(),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/super-res_odf_pred.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m         overlap\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/super-res_odf_pred.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m         padding_mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mreplicate\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/super-res_odf_pred.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/super-res_odf_pred.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=97'>98</a>\u001b[0m ic(\u001b[39m\"\u001b[39m\u001b[39mFinished network inference.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/super-res_odf_pred.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m mask_coords \u001b[39m=\u001b[39m einops\u001b[39m.\u001b[39mrearrange(super_coords, \u001b[39m\"\u001b[39m\u001b[39mb coord z y x -> b (z y x) coord\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda/envs/pitn/lib/python3.10/site-packages/monai/inferers/utils.py:145\u001b[0m, in \u001b[0;36msliding_window_inference\u001b[0;34m(inputs, roi_size, sw_batch_size, predictor, overlap, mode, sigma_scale, padding_mode, cval, sw_device, device, progress, roi_weight_map, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m scan_interval \u001b[39m=\u001b[39m _get_scan_interval(image_size, roi_size, num_spatial_dims, overlap)\n\u001b[1;32m    144\u001b[0m \u001b[39m# Store all slices in list\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m slices \u001b[39m=\u001b[39m dense_patch_slices(image_size, roi_size, scan_interval)\n\u001b[1;32m    146\u001b[0m num_win \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(slices)  \u001b[39m# number of windows per image\u001b[39;00m\n\u001b[1;32m    147\u001b[0m total_slices \u001b[39m=\u001b[39m num_win \u001b[39m*\u001b[39m batch_size  \u001b[39m# total number of windows\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:198\u001b[0m, in \u001b[0;36mdense_patch_slices\u001b[0;34m(image_size, patch_size, scan_interval)\u001b[0m\n\u001b[1;32m    196\u001b[0m     starts\u001b[39m.\u001b[39mappend(dim_starts)\n\u001b[1;32m    197\u001b[0m out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray([x\u001b[39m.\u001b[39mflatten() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39mmeshgrid(\u001b[39m*\u001b[39mstarts, indexing\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mij\u001b[39m\u001b[39m\"\u001b[39m)])\u001b[39m.\u001b[39mT\n\u001b[0;32m--> 198\u001b[0m \u001b[39mreturn\u001b[39;00m [\u001b[39mtuple\u001b[39m(\u001b[39mslice\u001b[39m(s, s \u001b[39m+\u001b[39m patch_size[d]) \u001b[39mfor\u001b[39;00m d, s \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(x)) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m out]\n",
      "File \u001b[0;32m~/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:198\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    196\u001b[0m     starts\u001b[39m.\u001b[39mappend(dim_starts)\n\u001b[1;32m    197\u001b[0m out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray([x\u001b[39m.\u001b[39mflatten() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39mmeshgrid(\u001b[39m*\u001b[39mstarts, indexing\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mij\u001b[39m\u001b[39m\"\u001b[39m)])\u001b[39m.\u001b[39mT\n\u001b[0;32m--> 198\u001b[0m \u001b[39mreturn\u001b[39;00m [\u001b[39mtuple\u001b[39;49m(\u001b[39mslice\u001b[39;49m(s, s \u001b[39m+\u001b[39;49m patch_size[d]) \u001b[39mfor\u001b[39;49;00m d, s \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(x)) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m out]\n",
      "File \u001b[0;32m~/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:198\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    196\u001b[0m     starts\u001b[39m.\u001b[39mappend(dim_starts)\n\u001b[1;32m    197\u001b[0m out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray([x\u001b[39m.\u001b[39mflatten() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39mmeshgrid(\u001b[39m*\u001b[39mstarts, indexing\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mij\u001b[39m\u001b[39m\"\u001b[39m)])\u001b[39m.\u001b[39mT\n\u001b[0;32m--> 198\u001b[0m \u001b[39mreturn\u001b[39;00m [\u001b[39mtuple\u001b[39m(\u001b[39mslice\u001b[39m(s, s \u001b[39m+\u001b[39m patch_size[d]) \u001b[39mfor\u001b[39;00m d, s \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(x)) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m out]\n",
      "File \u001b[0;32m~/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:198\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    196\u001b[0m     starts\u001b[39m.\u001b[39mappend(dim_starts)\n\u001b[1;32m    197\u001b[0m out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray([x\u001b[39m.\u001b[39mflatten() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39mmeshgrid(\u001b[39m*\u001b[39mstarts, indexing\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mij\u001b[39m\u001b[39m\"\u001b[39m)])\u001b[39m.\u001b[39mT\n\u001b[0;32m--> 198\u001b[0m \u001b[39mreturn\u001b[39;00m [\u001b[39mtuple\u001b[39m(\u001b[39mslice\u001b[39m(s, s \u001b[39m+\u001b[39m patch_size[d]) \u001b[39mfor\u001b[39;00m d, s \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(x)) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m out]\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1443\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:700\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1143\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1134\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:311\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda/envs/pitn/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2062\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2059\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2061\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001b[0;32m-> 2062\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   2064\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2066\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2067\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/pitn/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2098\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2095\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2097\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2098\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m   2100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[1;32m   2102\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test all given subjects.\n",
    "system_state_dict = torch.load(p.model_weight_f)\n",
    "encoder_state_dict = system_state_dict[\"encoder\"]\n",
    "\n",
    "decoder_state_dict = system_state_dict[\"decoder\"]\n",
    "\n",
    "if \"in_channels\" not in p.encoder:\n",
    "    in_channels = int(test_dataset[0][\"lr_dwi\"].shape[0])\n",
    "else:\n",
    "    in_channels = p.encoder.in_channels\n",
    "\n",
    "encoder = INREncoder(**{**p.encoder.to_dict(), **{\"in_channels\": in_channels}})\n",
    "encoder.load_state_dict(encoder_state_dict)\n",
    "encoder.to(device)\n",
    "\n",
    "decoder = ReducedDecoder(**p.decoder.to_dict())\n",
    "decoder.load_state_dict(decoder_state_dict)\n",
    "decoder.to(device)\n",
    "del (\n",
    "    system_state_dict,\n",
    "    encoder_state_dict,\n",
    "    decoder_state_dict,\n",
    ")\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "for batch_dict in test_dataloader:\n",
    "\n",
    "    subj_id = batch_dict[\"subj_id\"]\n",
    "    if len(subj_id) == 1:\n",
    "        subj_id = subj_id[0]\n",
    "    x = batch_dict[\"lr_dwi\"].to(device)\n",
    "    x_coords = batch_dict[\"lr_extent_acpc\"].to(device)\n",
    "    x_affine_vox2mm = batch_dict[\"affine_lrvox2acpc\"].to(device)\n",
    "    x_vox_size = torch.atleast_2d(batch_dict[\"lr_vox_size\"]).to(device)\n",
    "    x_mask = batch_dict[\"lr_mask\"].to(torch.bool).to(device)\n",
    "\n",
    "    lower_lim = torch.stack(\n",
    "        [\n",
    "            x_coords[0, 0][0].unique()[0],\n",
    "            x_coords[0, 1][:, 0].unique()[0],\n",
    "            x_coords[0, 2][:, :, 0].unique()[0],\n",
    "        ]\n",
    "    )\n",
    "    upper_lim = torch.stack(\n",
    "        [\n",
    "            x_coords[0, 0][-1].unique()[0],\n",
    "            x_coords[0, 1][:, -1].unique()[0],\n",
    "            x_coords[0, 2][:, :, -1].unique()[0],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    super_vox_size = torch.ones_like(x_vox_size) * p.target_vox_size\n",
    "    target_fov_shape = torch.floor(torch.abs(upper_lim - lower_lim) / super_vox_size)\n",
    "    target_fov_shape = tuple(target_fov_shape.flatten().int().cpu().numpy().tolist())\n",
    "\n",
    "    vox2acpc = batch_dict[\"affine_lrvox2acpc\"][0].cpu()\n",
    "    scale = (p.target_vox_size / x_vox_size)[0].cpu()\n",
    "    scale = torch.cat([scale, scale.new_ones(1)]).cpu()\n",
    "    scale = torch.diag_embed(scale).to(vox2acpc).cpu()\n",
    "    new_aff = vox2acpc @ scale\n",
    "    new_aff = new_aff.numpy()\n",
    "    super_coords = pitn.affine.affine_coordinate_grid(\n",
    "        torch.from_numpy(new_aff).cpu().to(torch.float32), target_fov_shape\n",
    "    )\n",
    "    ! Changing only for a slice viz.\n",
    "    z = super_coords[..., 2].flatten()\n",
    "    i_dz = torch.argmin(torch.abs(z - 8.625)).flatten()[0].item()\n",
    "    d_z = z[i_dz] - 8.625\n",
    "    d_z = d_z.item()\n",
    "    super_coords[..., 2] -= d_z\n",
    "    new_aff[2, 3] -= d_z\n",
    "    #!#########\n",
    "    # print(\"Resample fodf coeffs\")\n",
    "    # pred_super_fodf = pitn.affine.sample_vol(\n",
    "    #     x.cpu(), super_coords.cpu(), vox2acpc, mode=\"bilinear\", align_corners=True\n",
    "    # )\n",
    "\n",
    "    # super_z = torch.arange(lower_lim[0], upper_lim[0], step=p.target_vox_size).to(\n",
    "    #     x_coords\n",
    "    # )\n",
    "    ! Changing only for a slice viz.\n",
    "    # i_dz = torch.where(\n",
    "    #     torch.abs(super_z - 8.625) == torch.min(torch.abs(super_z - 8.625))\n",
    "    # )[0][0].item()\n",
    "    # d_z = super_z[i_dz] - 8.625\n",
    "    # d_z = d_z.item()\n",
    "    # super_z = super_z - d_z\n",
    "    #!#########\n",
    "\n",
    "    # super_y = torch.arange(lower_lim[1], upper_lim[1], step=p.target_vox_size).to(\n",
    "    #     x_coords\n",
    "    # )\n",
    "    # super_x = torch.arange(lower_lim[2], upper_lim[2], step=p.target_vox_size).to(\n",
    "    #     x_coords\n",
    "    # )\n",
    "\n",
    "    # super_zzz, super_yyy, super_xxx = torch.meshgrid(\n",
    "    #     [super_z, super_y, super_x], indexing=\"ij\"\n",
    "    # )\n",
    "    # super_coords = torch.stack([super_zzz, super_yyy, super_xxx], dim=0)[None]\n",
    "    # super_vol_shape = tuple(super_coords.shape[2:])\n",
    "\n",
    "    # super_vox_size = torch.ones_like(x_vox_size) * p.target_vox_size\n",
    "\n",
    "    # vox2acpc = batch_dict[\"affine_lrvox2acpc\"][0].cpu()\n",
    "    # scale = (p.target_vox_size / x_vox_size)[0].cpu()\n",
    "    # scale = torch.cat([scale, scale.new_ones(1)]).cpu()\n",
    "    # scale = torch.diag_embed(scale).to(vox2acpc).cpu()\n",
    "    # new_aff = vox2acpc @ scale\n",
    "    # new_aff = new_aff.numpy()\n",
    "    ! Changing only for a slice viz.\n",
    "    # new_aff[2, 3] -= d_z\n",
    "    #!#########\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        sc = einops.rearrange(super_coords, \"x y z d -> 1 d x y z\")\n",
    "        ic(\"Starting net inference.\")\n",
    "        ctx_v = encoder(x)\n",
    "\n",
    "        # Whole-volume inference is memory-prohibitive, so use a sliding\n",
    "        # window inference method on the encoded volume.\n",
    "        pred_super_fodf = monai.inferers.sliding_window_inference(\n",
    "            sc.cpu(),\n",
    "            # roi_size=(36, 36, 36),\n",
    "            roi_size=(96, 96, 96),\n",
    "            sw_batch_size=sc.shape[0],\n",
    "            predictor=lambda q: decoder(\n",
    "                query_coord=q.to(device),\n",
    "                context_v=ctx_v,\n",
    "                context_spatial_extent=x_coords,\n",
    "                affine_context_vox2mm=x_affine_vox2mm,\n",
    "            ).cpu(),\n",
    "            overlap=0,\n",
    "            padding_mode=\"replicate\",\n",
    "        )\n",
    "    ic(\"Finished network inference.\")\n",
    "    # mask_coords = einops.rearrange(super_coords, \"b coord z y x -> b (z y x) coord\")\n",
    "    # super_mask = pitn.affine.sample_3d(\n",
    "    #     x_mask.cpu(), mask_coords.cpu(), vox2acpc, mode=\"nearest\", align_corners=True\n",
    "    # )\n",
    "    # super_mask = (\n",
    "    #     einops.rearrange(\n",
    "    #         super_mask,\n",
    "    #         \"b (z y x) c -> b z y x c\",\n",
    "    #         z=super_vol_shape[0],\n",
    "    #         y=super_vol_shape[1],\n",
    "    #         x=super_vol_shape[2],\n",
    "    #     )\n",
    "    #     .squeeze()\n",
    "    #     .cpu()\n",
    "    #     .to(torch.int8)\n",
    "    #     .numpy()\n",
    "    # )\n",
    "\n",
    "    super_mask = pitn.affine.sample_vol(\n",
    "        x_mask.cpu(), super_coords.cpu(), vox2acpc, mode=\"nearest\", align_corners=True\n",
    "    )\n",
    "    pred_super_fodf = pred_super_fodf * super_mask.bool()\n",
    "    superres_pred = pred_super_fodf.detach().cpu().squeeze()\n",
    "    superres_pred = einops.rearrange(\n",
    "        superres_pred,\n",
    "        \"c x y z -> x y z c\",\n",
    "    )\n",
    "    superres_pred = superres_pred.numpy().astype(np.float32).squeeze()\n",
    "\n",
    "    super_mask = super_mask.squeeze().cpu().to(torch.int8).numpy()\n",
    "\n",
    "    # superres_pred = pred_super_fodf.cpu().numpy()\n",
    "    # superres_pred = superres_pred * super_mask\n",
    "    ic(\"Saving super-res fodf coeffs.\")\n",
    "    nib.save(\n",
    "        nib.Nifti1Image(superres_pred, affine=new_aff),\n",
    "        tmp_res_dir / f\"{subj_id}_odf-coeff_inr-super-res_{p.target_vox_size}mm.nii.gz\",\n",
    "    )\n",
    "    ic(\"Saving mask.\")\n",
    "    nib.save(\n",
    "        nib.Nifti1Image(super_mask, affine=new_aff),\n",
    "        tmp_res_dir / f\"{subj_id}_mask-super-res_{p.target_vox_size}mm.nii.gz\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d00c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9613a23",
   "metadata": {},
   "source": [
    "### Tri-Linear Interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7964bd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_dict in test_dataloader:\n",
    "\n",
    "    subj_id = batch_dict[\"subj_id\"]\n",
    "    if len(subj_id) == 1:\n",
    "        subj_id = subj_id[0]\n",
    "    x = batch_dict[\"lr_fodf\"].to(device)\n",
    "    x_coords = batch_dict[\"lr_extent_acpc\"].to(device)\n",
    "    x_vox_size = torch.atleast_2d(batch_dict[\"lr_vox_size\"]).to(device)\n",
    "    x_mask = batch_dict[\"lr_mask\"].to(torch.bool).to(device)\n",
    "\n",
    "    lower_lim = torch.stack(\n",
    "        [\n",
    "            x_coords[0, 0][0].unique()[0],\n",
    "            x_coords[0, 1][:, 0].unique()[0],\n",
    "            x_coords[0, 2][:, :, 0].unique()[0],\n",
    "        ]\n",
    "    )\n",
    "    upper_lim = torch.stack(\n",
    "        [\n",
    "            x_coords[0, 0][-1].unique()[0],\n",
    "            x_coords[0, 1][:, -1].unique()[0],\n",
    "            x_coords[0, 2][:, :, -1].unique()[0],\n",
    "        ]\n",
    "    )\n",
    "    super_vox_size = torch.ones_like(x_vox_size) * p.target_vox_size\n",
    "    target_fov_shape = torch.floor(torch.abs(upper_lim - lower_lim) / super_vox_size)\n",
    "    target_fov_shape = tuple(target_fov_shape.flatten().int().cpu().numpy().tolist())\n",
    "\n",
    "    vox2acpc = batch_dict[\"affine_lrvox2acpc\"][0].cpu()\n",
    "    scale = (p.target_vox_size / x_vox_size)[0].cpu()\n",
    "    scale = torch.cat([scale, scale.new_ones(1)]).cpu()\n",
    "    scale = torch.diag_embed(scale).to(vox2acpc).cpu()\n",
    "    new_aff = vox2acpc @ scale\n",
    "    new_aff = new_aff.numpy()\n",
    "    super_coords = pitn.affine.affine_coordinate_grid(\n",
    "        torch.from_numpy(new_aff).cpu().to(torch.float32),\n",
    "        target_fov_shape\n",
    "        # torch.from_numpy(new_aff).to(x_coords), target_fov_shape\n",
    "    )\n",
    "    ! Changing only for a slice viz.\n",
    "    z = super_coords[..., 2].flatten()\n",
    "    i_dz = torch.argmin(torch.abs(z - 8.625)).flatten()[0].item()\n",
    "    d_z = z[i_dz] - 8.625\n",
    "    d_z = d_z.item()\n",
    "    super_coords[..., 2] -= d_z\n",
    "    new_aff[2, 3] -= d_z\n",
    "    #!#########\n",
    "\n",
    "    print(\"Resample fodf coeffs\")\n",
    "    pred_super_fodf = pitn.affine.sample_vol(\n",
    "        x.cpu(), super_coords.cpu(), vox2acpc, mode=\"bilinear\", align_corners=True\n",
    "    )\n",
    "    super_mask = pitn.affine.sample_vol(\n",
    "        x_mask.cpu(), super_coords.cpu(), vox2acpc, mode=\"nearest\", align_corners=True\n",
    "    )\n",
    "    pred_super_fodf = pred_super_fodf * super_mask.bool()\n",
    "    superres_pred = pred_super_fodf.detach().cpu().squeeze()\n",
    "    superres_pred = einops.rearrange(\n",
    "        superres_pred,\n",
    "        \"c x y z -> x y z c\",\n",
    "    )\n",
    "    superres_pred = superres_pred.numpy().astype(np.float32).squeeze()\n",
    "\n",
    "    super_mask = super_mask.squeeze().cpu().to(torch.int8).numpy()\n",
    "    nib.save(\n",
    "        nib.Nifti1Image(superres_pred, affine=new_aff),\n",
    "        tmp_res_dir\n",
    "        / f\"{subj_id}_odf-coeff_tri-linear-super-res_{p.target_vox_size}mm.nii.gz\",\n",
    "    )\n",
    "    # nib.save(\n",
    "    #     nib.Nifti1Image(super_mask, affine=new_aff),\n",
    "    #     tmp_res_dir / f\"{subj_id}_mask-super-res_{p.target_vox_size}mm.nii.gz\",\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d037c3a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "comment_magics": true,
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "pitn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a27b66b33b39a2ceaf791f3e24555d5a3d820ed1de6ee5d2f5c031617fe18d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
