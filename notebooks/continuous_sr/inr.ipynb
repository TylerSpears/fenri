{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73408619",
   "metadata": {},
   "source": [
    "# Continuous-Space Super-Resolution of fODFs in Diffusion MRI\n",
    "\n",
    "Code by:\n",
    "\n",
    "Tyler Spears - tas6hh@virginia.edu\n",
    "\n",
    "Dr. Tom Fletcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64ef468",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8d7897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Automatically re-import project-specific modules.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# imports\n",
    "import collections\n",
    "import copy\n",
    "import datetime\n",
    "import functools\n",
    "import inspect\n",
    "import io\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import pdb\n",
    "import random\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "import typing\n",
    "import warnings\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from pprint import pprint as ppr\n",
    "\n",
    "import aim\n",
    "import dotenv\n",
    "import einops\n",
    "\n",
    "# visualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import monai\n",
    "\n",
    "# Data management libraries.\n",
    "import nibabel as nib\n",
    "import nibabel.processing\n",
    "\n",
    "# Computation & ML libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import skimage\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchinfo\n",
    "from box import Box\n",
    "from icecream import ic\n",
    "from natsort import natsorted\n",
    "\n",
    "from lightning_fabric.fabric import Fabric\n",
    "\n",
    "import pitn\n",
    "\n",
    "plt.rcParams.update({\"figure.autolayout\": True})\n",
    "plt.rcParams.update({\"figure.facecolor\": [1.0, 1.0, 1.0, 1.0]})\n",
    "plt.rcParams.update({\"image.cmap\": \"gray\"})\n",
    "\n",
    "# Set print options for ndarrays/tensors.\n",
    "np.set_printoptions(suppress=True, threshold=100, linewidth=88)\n",
    "torch.set_printoptions(sci_mode=False, threshold=100, linewidth=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c68d20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "direnv: loading ~/Projects/pitn/.envrc\n",
      "\n",
      "EnvironmentNameNotFound: Could not find conda environment: pitn\n",
      "You can list all discoverable environments with `conda info --envs`.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update notebook's environment variables with direnv.\n",
    "# This requires the python-dotenv package, and direnv be installed on the system\n",
    "# This will not work on Windows.\n",
    "# NOTE: This is kind of hacky, and not necessarily safe. Be careful...\n",
    "# Libraries needed on the python side:\n",
    "# - os\n",
    "# - subprocess\n",
    "# - io\n",
    "# - dotenv\n",
    "\n",
    "# Form command to be run in direnv's context. This command will print out\n",
    "# all environment variables defined in the subprocess/sub-shell.\n",
    "command = \"direnv exec {} /usr/bin/env\".format(os.getcwd())\n",
    "# Run command in a new subprocess.\n",
    "proc = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, cwd=os.getcwd())\n",
    "# Store and format the subprocess' output.\n",
    "proc_out = proc.communicate()[0].strip().decode(\"utf-8\")\n",
    "# Use python-dotenv to load the environment variables by using the output of\n",
    "# 'direnv exec ...' as a 'dummy' .env file.\n",
    "dotenv.load_dotenv(stream=io.StringIO(proc_out), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4126c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Device IDX  0\n",
      "CUDA Current Device  0\n",
      "CUDA Device properties:  _CudaDeviceProperties(name='NVIDIA RTX A5000', major=8, minor=6, total_memory=24247MB, multi_processor_count=64)\n",
      "CuDNN convolution optimization enabled.\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# torch setup\n",
    "# allow for CUDA usage, if available\n",
    "if torch.cuda.is_available():\n",
    "    # Pick only one device for the default, may use multiple GPUs for training later.\n",
    "    if \"CUDA_PYTORCH_DEVICE_IDX\" in os.environ.keys():\n",
    "        dev_idx = int(os.environ[\"CUDA_PYTORCH_DEVICE_IDX\"])\n",
    "    else:\n",
    "        dev_idx = 0\n",
    "    device = torch.device(f\"cuda:{dev_idx}\")\n",
    "    print(\"CUDA Device IDX \", dev_idx)\n",
    "    torch.cuda.set_device(device)\n",
    "    print(\"CUDA Current Device \", torch.cuda.current_device())\n",
    "    print(\"CUDA Device properties: \", torch.cuda.get_device_properties(device))\n",
    "    # The flag below controls whether to allow TF32 on matmul. This flag defaults to False\n",
    "    # in PyTorch 1.12 and later.\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    # See\n",
    "    # <https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices>\n",
    "    # for details.\n",
    "\n",
    "    # Activate cudnn benchmarking to optimize convolution algorithm speed.\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        print(\"CuDNN convolution optimization enabled.\")\n",
    "        # The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# keep device as the cpu\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26871439",
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr cap\n",
    "# Capture output and save to log. Needs to be at the *very first* line of the cell.\n",
    "# Watermark\n",
    "%load_ext watermark\n",
    "%watermark --author \"Tyler Spears\" --updated --iso8601  --python --machine --iversions --githash\n",
    "if torch.cuda.is_available():\n",
    "    # GPU information\n",
    "    try:\n",
    "        gpu_info = pitn.utils.system.get_gpu_specs()\n",
    "        print(gpu_info)\n",
    "    except NameError:\n",
    "        print(\"CUDA Version: \", torch.version.cuda)\n",
    "else:\n",
    "    print(\"CUDA not in use, falling back to CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb719750",
   "metadata": {
    "tags": [
     "active-ipynb",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Tyler Spears\n",
      "\n",
      "Last updated: 2023-02-20T12:44:47.308019-05:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.9\n",
      "IPython version      : 8.4.0\n",
      "\n",
      "Compiler    : GCC 11.3.0\n",
      "OS          : Linux\n",
      "Release     : 5.19.0-32-generic\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 20\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: df3b44f9a82760fa5c11b154eb52f3fc3bf0cebc\n",
      "\n",
      "aim       : 3.14.4\n",
      "pandas    : 1.5.2\n",
      "skimage   : 0.19.3\n",
      "sys       : 3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:20:04) [GCC 11.3.0]\n",
      "torchinfo : 1.7.1\n",
      "numpy     : 1.23.4\n",
      "torch     : 1.13.1\n",
      "einops    : 0.6.0\n",
      "nibabel   : 4.0.1\n",
      "seaborn   : 0.12.1\n",
      "matplotlib: 3.5.2\n",
      "monai     : 1.0.1\n",
      "pitn      : 0.0.post1.dev258+g67cc03c.d20230208\n",
      "\n",
      "==================================================GPU Specs==================================================\n",
      "  id  Name              Driver Version      CUDA Version  Total Memory    uuid\n",
      "----  ----------------  ----------------  --------------  --------------  ----------------------------------------\n",
      "   0  NVIDIA RTX A5000  525.60.13                   11.7  24564.0MB       GPU-ed20d87f-e88e-692f-0b56-548b8a05ddea\n",
      "   1  NVIDIA RTX A5000  525.60.13                   11.7  24564.0MB       GPU-0636ee40-2eab-9533-1be7-dbbadade95c4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cap is defined in an ipython magic command\n",
    "try:\n",
    "    print(cap)\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d3a5c6",
   "metadata": {},
   "source": [
    "## Experiment & Parameters Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cd75175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Config file not loaded\n"
     ]
    }
   ],
   "source": [
    "p = Box(default_box=True)\n",
    "# Experiment defaults, can be overridden in a config file.\n",
    "\n",
    "# General experiment-wide params\n",
    "###############################################\n",
    "p.experiment_name = \"dev_fixed-inr-ensemble\"\n",
    "p.override_experiment_name = False\n",
    "p.results_dir = \"/data/srv/outputs/pitn/results/runs\"\n",
    "p.tmp_results_dir = \"/data/srv/outputs/pitn/results/tmp\"\n",
    "p.train_val_test_split_file = random.choice(\n",
    "    list(Path(\"./data_splits\").glob(\"HCP*train-val-test_split*.csv\"))\n",
    ")\n",
    "p.aim_logger = dict(\n",
    "    repo=\"aim://dali.cpe.virginia.edu:53800\",\n",
    "    experiment=\"PITN_INR\",\n",
    "    meta_params=dict(run_name=p.experiment_name),\n",
    "    tags=(\"PITN\", \"INR\", \"HCP\", \"super-res\", \"dMRI\"),\n",
    ")\n",
    "###############################################\n",
    "p.train = dict(\n",
    "    in_patch_size=(24, 24, 24),\n",
    "    batch_size=4,\n",
    "    samples_per_subj_per_epoch=10,\n",
    "    max_epochs=5,\n",
    "    dwi_recon_epoch_proportion=0.05,\n",
    ")\n",
    "# Optimizer kwargs for training.\n",
    "p.train.optim.encoder.lr = 1e-4\n",
    "p.train.optim.decoder.lr = 5e-4\n",
    "p.train.optim.recon_decoder.lr = 1e-4\n",
    "# Train dataloader kwargs.\n",
    "p.train.dataloader = dict(num_workers=17, persistent_workers=True, prefetch_factor=3)\n",
    "\n",
    "# Network/model parameters.\n",
    "p.encoder = dict(\n",
    "    interior_channels=80,\n",
    "    out_channels=128,\n",
    "    n_res_units=3,\n",
    "    n_dense_units=3,\n",
    "    activate_fn=\"relu\",\n",
    ")\n",
    "p.decoder = dict(\n",
    "    context_v_features=128,\n",
    "    in_features=p.encoder.out_channels,\n",
    "    out_features=45,\n",
    "    m_encode_num_freqs=36,\n",
    "    sigma_encode_scale=3.0,\n",
    ")\n",
    "\n",
    "\n",
    "# If a config file exists, override the defaults with those values.\n",
    "try:\n",
    "    if \"PITN_CONFIG\" in os.environ.keys():\n",
    "        config_fname = Path(os.environ[\"PITN_CONFIG\"])\n",
    "    else:\n",
    "        config_fname = pitn.utils.system.get_file_glob_unique(Path(\".\"), r\"config.*\")\n",
    "    f_type = config_fname.suffix.casefold()\n",
    "    if f_type in {\".yaml\", \".yml\"}:\n",
    "        f_params = Box.from_yaml(filename=config_fname)\n",
    "    elif f_type == \".json\":\n",
    "        f_params = Box.from_json(filename=config_fname)\n",
    "    elif f_type == \".toml\":\n",
    "        f_params = Box.from_toml(filename=config_fname)\n",
    "    else:\n",
    "        raise RuntimeError()\n",
    "\n",
    "    p.merge_update(f_params)\n",
    "\n",
    "except:\n",
    "    print(\"WARNING: Config file not loaded\")\n",
    "    pass\n",
    "\n",
    "# Remove the default_box behavior now that params have been fully read in.\n",
    "_p = Box(default_box=False)\n",
    "_p.merge_update(p)\n",
    "p = _p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79e24dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvt_split = pd.read_csv(p.train_val_test_split_file)\n",
    "p.train.subj_ids = natsorted(tvt_split[tvt_split.split == \"train\"].subj_id.tolist())\n",
    "p.val = dict()\n",
    "p.val.subj_ids = natsorted(tvt_split[tvt_split.split == \"val\"].subj_id.tolist())\n",
    "p.test = dict()\n",
    "p.test.subj_ids = natsorted(tvt_split[tvt_split.split == \"test\"].subj_id.tolist())\n",
    "\n",
    "# Ensure that no test subj ids are in either the training or validation sets.\n",
    "# However, we can have overlap between training and validation.\n",
    "assert len(set(p.train.subj_ids) & set(p.test.subj_ids)) == 0\n",
    "assert len(set(p.val.subj_ids) & set(p.test.subj_ids)) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a847f016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aim_logger': {'experiment': 'PITN_INR',\n",
      "                'meta_params': {'run_name': 'dev_fixed-inr-ensemble'},\n",
      "                'repo': 'aim://dali.cpe.virginia.edu:53800',\n",
      "                'tags': ('PITN', 'INR', 'HCP', 'super-res', 'dMRI')},\n",
      " 'decoder': {'context_v_features': 128,\n",
      "             'in_features': 128,\n",
      "             'm_encode_num_freqs': 36,\n",
      "             'out_features': 45,\n",
      "             'sigma_encode_scale': 3.0},\n",
      " 'encoder': {'activate_fn': 'relu',\n",
      "             'interior_channels': 80,\n",
      "             'n_dense_units': 3,\n",
      "             'n_res_units': 3,\n",
      "             'out_channels': 128},\n",
      " 'experiment_name': 'dev_fixed-inr-ensemble',\n",
      " 'override_experiment_name': False,\n",
      " 'results_dir': '/data/srv/outputs/pitn/results/runs',\n",
      " 'test': {'subj_ids': [100206,\n",
      "                       102109,\n",
      "                       104820,\n",
      "                       106824,\n",
      "                       107422,\n",
      "                       118124,\n",
      "                       118730,\n",
      "                       139637,\n",
      "                       141422,\n",
      "                       144125,\n",
      "                       145127,\n",
      "                       146836,\n",
      "                       150019,\n",
      "                       158540,\n",
      "                       159946,\n",
      "                       160931,\n",
      "                       163331,\n",
      "                       168341,\n",
      "                       172433,\n",
      "                       178849,\n",
      "                       183741,\n",
      "                       186545,\n",
      "                       195849,\n",
      "                       200513,\n",
      "                       208226,\n",
      "                       211922,\n",
      "                       213522,\n",
      "                       236130,\n",
      "                       245333,\n",
      "                       248339,\n",
      "                       250427,\n",
      "                       250932,\n",
      "                       255639,\n",
      "                       256540,\n",
      "                       300719,\n",
      "                       376247,\n",
      "                       379657,\n",
      "                       459453,\n",
      "                       461743,\n",
      "                       469961,\n",
      "                       480141,\n",
      "                       485757,\n",
      "                       518746,\n",
      "                       562446,\n",
      "                       566454,\n",
      "                       580751,\n",
      "                       647858,\n",
      "                       654754,\n",
      "                       656657,\n",
      "                       677968,\n",
      "                       680452,\n",
      "                       715647,\n",
      "                       727654,\n",
      "                       773257,\n",
      "                       788674,\n",
      "                       845458,\n",
      "                       872562,\n",
      "                       911849,\n",
      "                       991267,\n",
      "                       994273]},\n",
      " 'tmp_results_dir': '/data/srv/outputs/pitn/results/tmp',\n",
      " 'train': {'batch_size': 4,\n",
      "           'dataloader': {'num_workers': 17,\n",
      "                          'persistent_workers': True,\n",
      "                          'prefetch_factor': 3},\n",
      "           'dwi_recon_epoch_proportion': 0.05,\n",
      "           'in_patch_size': (24, 24, 24),\n",
      "           'max_epochs': 5,\n",
      "           'optim': {'decoder': {'lr': 0.0005},\n",
      "                     'encoder': {'lr': 0.0001},\n",
      "                     'recon_decoder': {'lr': 0.0001}},\n",
      "           'samples_per_subj_per_epoch': 10,\n",
      "           'subj_ids': [105115,\n",
      "                        114419,\n",
      "                        119833,\n",
      "                        122418,\n",
      "                        127832,\n",
      "                        147737,\n",
      "                        151425,\n",
      "                        156435,\n",
      "                        171532,\n",
      "                        180836,\n",
      "                        217429,\n",
      "                        270332,\n",
      "                        361941,\n",
      "                        481042,\n",
      "                        492754,\n",
      "                        516742,\n",
      "                        735148,\n",
      "                        748662,\n",
      "                        802844,\n",
      "                        809252]},\n",
      " 'train_val_test_split_file': PosixPath('data_splits/HCP_train-val-test_split_03_seed_871113743.csv'),\n",
      " 'val': {'subj_ids': [110613, 121719, 156637, 627852, 993675]}}\n"
     ]
    }
   ],
   "source": [
    "ppr(p.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42fab1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which parameters to store in the aim meta-params.\n",
    "p.aim_logger.meta_params.hparams = dict(\n",
    "    batch_size=p.train.batch_size,\n",
    "    patch_size=p.train.in_patch_size,\n",
    "    samples_per_subj_per_epoch=p.train.samples_per_subj_per_epoch,\n",
    "    max_epochs=p.train.max_epochs,\n",
    ")\n",
    "p.aim_logger.meta_params.data = dict(\n",
    "    train_subj_ids=p.train.subj_ids,\n",
    "    val_subj_ids=p.val.subj_ids,\n",
    "    test_subj_ids=p.test.subj_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1047b3",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6a12cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hcp_full_res_data_dir = Path(\"/data/srv/data/pitn/hcp\")\n",
    "hcp_full_res_fodf_dir = Path(\"/data/srv/outputs/pitn/hcp/full-res/fodf\")\n",
    "hcp_low_res_data_dir = Path(\"/data/srv/outputs/pitn/hcp/downsample/scale-2.00mm/vol\")\n",
    "hcp_low_res_fodf_dir = Path(\"/data/srv/outputs/pitn/hcp/downsample/scale-2.00mm/fodf\")\n",
    "\n",
    "assert hcp_full_res_data_dir.exists()\n",
    "assert hcp_full_res_fodf_dir.exists()\n",
    "assert hcp_low_res_data_dir.exists()\n",
    "assert hcp_low_res_fodf_dir.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79432e4d",
   "metadata": {},
   "source": [
    "### Create Patch-Based Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed703dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG Train subject numbers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 10/10 [00:24<00:00,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Warnings caught:\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [2. 2. 2. 1.] to [  2.           2.           2.         146.97974988]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [2. 2. 2. 1.] to [  2.           2.           2.         143.20456304]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [2. 2. 2. 1.] to [  2.           2.           2.         146.58972295]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [2. 2. 2. 1.] to [  2.           2.           2.         145.67273896]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25 1.  ] to [  1.25         1.25         1.25       141.23473369]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25 1.  ] to [  1.25         1.25         1.25       145.12623815]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25 1.  ] to [  1.25         1.25         1.25       144.85358988]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25 1.  ] to [  1.25         1.25         1.25       145.05623909]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25  nan] to [  1.25         1.25         1.25       141.23473369]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [ 2.  2.  2. nan] to [  2.           2.           2.         143.20456304]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [2. 2. 2. 1.] to [  2.           2.           2.         141.25348447]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25  nan] to [  1.25         1.25         1.25       144.85358988]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25  nan] to [  1.25         1.25         1.25       145.12623815]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [ 2.  2.  2. nan] to [  2.           2.           2.         145.67273896]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [ 2.  2.  2. nan] to [  2.           2.           2.         146.58972295]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [2. 2. 2. 1.] to [  2.           2.           2.         139.27507629]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25  nan] to [  1.25         1.25         1.25       145.05623909]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [ 2.  2.  2. nan] to [  2.           2.           2.         146.97974988]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [2. 2. 2. 1.] to [  2.           2.           2.         150.05014787]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25 1.  ] to [  1.25         1.25         1.25       139.70079635]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25 1.  ] to [  1.25         1.25         1.25       137.53544998]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25 1.  ] to [  1.25         1.25         1.25       145.71826413]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25 1.  ] to [  1.25         1.25         1.25       148.80587186]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25  nan] to [  1.25         1.25         1.25       139.70079635]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [ 2.  2.  2. nan] to [  2.           2.           2.         141.25348447]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [2. 2. 2. 1.] to [  2.           2.           2.         135.88431431]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25  nan] to [  1.25         1.25         1.25       137.53544998]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [ 2.  2.  2. nan] to [  2.           2.           2.         139.27507629]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [2. 2. 2. 1.] to [  2.           2.           2.         136.34715573]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25  nan] to [  1.25         1.25         1.25       145.71826413]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25  nan] to [  1.25         1.25         1.25       148.80587186]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [ 2.  2.  2. nan] to [  2.           2.           2.         150.05014787]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25 1.  ] to [  1.25         1.25         1.25       134.51788914]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25 1.  ] to [  1.25         1.25         1.25       135.11592245]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25  nan] to [  1.25         1.25         1.25       134.51788914]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [ 2.  2.  2. nan] to [  2.           2.           2.         135.88431431]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25  nan] to [  1.25         1.25         1.25       135.11592245]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [ 2.  2.  2. nan] to [  2.           2.           2.         136.34715573]\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DEBUG_TRAIN_DATA_SUBJS = 10\n",
    "with warnings.catch_warnings(record=True) as warn_list:\n",
    "    # pre_sample_ds = pitn.data.datasets.HCPfODFINRDataset(\n",
    "    #     subj_ids=p.train.subj_ids,\n",
    "    #     dwi_root_dir=hcp_full_res_data_dir,\n",
    "    #     fodf_root_dir=hcp_full_res_fodf_dir,\n",
    "    #     lr_dwi_root_dir=hcp_low_res_data_dir,\n",
    "    #     lr_fodf_root_dir=hcp_low_res_fodf_dir,\n",
    "    #     transform=None,\n",
    "    # )\n",
    "\n",
    "    #! DEBUG\n",
    "    print(\"DEBUG Train subject numbers\")\n",
    "    pre_sample_ds = pitn.data.datasets.HCPfODFINRDataset(\n",
    "        subj_ids=p.train.subj_ids[:DEBUG_TRAIN_DATA_SUBJS],\n",
    "        dwi_root_dir=hcp_full_res_data_dir,\n",
    "        fodf_root_dir=hcp_full_res_fodf_dir,\n",
    "        lr_dwi_root_dir=hcp_low_res_data_dir,\n",
    "        lr_fodf_root_dir=hcp_low_res_fodf_dir,\n",
    "        transform=None,\n",
    "    )\n",
    "    #!\n",
    "\n",
    "    pre_sample_train_dataset = monai.data.CacheDataset(\n",
    "        pre_sample_ds,\n",
    "        transform=pre_sample_ds.default_pre_sample_tf(\n",
    "            # Dilate by half the radius of one patch size.\n",
    "            mask_dilate_radius=max(p.train.in_patch_size)\n",
    "            // 4\n",
    "        ),\n",
    "        copy_cache=False,\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "train_dataset = pitn.data.datasets.HCPfODFINRPatchDataset(\n",
    "    pre_sample_train_dataset,\n",
    "    patch_func=pitn.data.datasets.HCPfODFINRPatchDataset.default_patch_func(\n",
    "        spatial_size=p.train.in_patch_size,\n",
    "        num_samples=p.train.samples_per_subj_per_epoch,\n",
    "    ),\n",
    "    samples_per_image=p.train.samples_per_subj_per_epoch,\n",
    "    transform=pitn.data.datasets.HCPfODFINRPatchDataset.default_feature_tf(\n",
    "        p.train.in_patch_size\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"=\" * 10)\n",
    "print(\"Warnings caught:\")\n",
    "ws = \"\\n\".join(\n",
    "    [\n",
    "        warnings.formatwarning(\n",
    "            w.message, w.category, w.filename, w.lineno, w.file, w.line\n",
    "        )\n",
    "        for w in warn_list\n",
    "    ]\n",
    ")\n",
    "ws = \"\\n\".join(filter(lambda s: bool(s.strip()), ws.splitlines()))\n",
    "print(ws, flush=True)\n",
    "print(\"=\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bf5a57",
   "metadata": {},
   "source": [
    "### Validation & Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9105038",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 3/3 [00:09<00:00,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Warnings caught:\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [2. 2. 2. 1.] to [  2.           2.           2.         151.06471089]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [2. 2. 2. 1.] to [  2.           2.           2.         140.22498663]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [2. 2. 2. 1.] to [  2.           2.           2.         151.53397928]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25 1.  ] to [  1.25         1.25         1.25       138.74121594]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25 1.  ] to [  1.25         1.25         1.25       150.92133713]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25 1.  ] to [  1.25         1.25         1.25       150.06061275]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25  nan] to [  1.25         1.25         1.25       138.74121594]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [ 2.  2.  2. nan] to [  2.           2.           2.         140.22498663]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25  nan] to [  1.25         1.25         1.25       150.06061275]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [ 2.  2.  2. nan] to [  2.           2.           2.         151.06471089]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25  nan] to [  1.25         1.25         1.25       150.92133713]\n",
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/monai/data/utils.py:769: UserWarning:\n",
      "Modifying image pixdim from [ 2.  2.  2. nan] to [  2.           2.           2.         151.53397928]\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings(record=True) as warn_list:\n",
    "\n",
    "    #!DEBUG\n",
    "    DEBUG_VAL_SUBJS = 3\n",
    "    # Validation dataset.\n",
    "    val_paths_dataset = pitn.data.datasets.HCPfODFINRDataset(\n",
    "        subj_ids=p.val.subj_ids[:DEBUG_VAL_SUBJS],\n",
    "        dwi_root_dir=hcp_full_res_data_dir,\n",
    "        fodf_root_dir=hcp_full_res_fodf_dir,\n",
    "        lr_dwi_root_dir=hcp_low_res_data_dir,\n",
    "        lr_fodf_root_dir=hcp_low_res_fodf_dir,\n",
    "    )\n",
    "    cached_val_dataset = monai.data.CacheDataset(\n",
    "        val_paths_dataset,\n",
    "        transform=val_paths_dataset.default_pre_sample_tf(0, skip_sample_mask=True),\n",
    "        copy_cache=False,\n",
    "        num_workers=4,\n",
    "    )\n",
    "    val_dataset = pitn.data.datasets.HCPfODFINRWholeVolDataset(\n",
    "        cached_val_dataset,\n",
    "        transform=pitn.data.datasets.HCPfODFINRWholeVolDataset.default_tf(),\n",
    "    )\n",
    "\n",
    "    # Test dataset.\n",
    "    # The test dataset won't be cached, as each image should only be loaded once.\n",
    "    test_paths_dataset = pitn.data.datasets.HCPfODFINRDataset(\n",
    "        subj_ids=p.test.subj_ids,\n",
    "        dwi_root_dir=hcp_full_res_data_dir,\n",
    "        fodf_root_dir=hcp_full_res_fodf_dir,\n",
    "        lr_dwi_root_dir=hcp_low_res_data_dir,\n",
    "        lr_fodf_root_dir=hcp_low_res_fodf_dir,\n",
    "        transform=pitn.data.datasets.HCPfODFINRDataset.default_pre_sample_tf(\n",
    "            0, skip_sample_mask=True\n",
    "        ),\n",
    "    )\n",
    "    test_dataset = pitn.data.datasets.HCPfODFINRWholeVolDataset(\n",
    "        test_paths_dataset,\n",
    "        transform=pitn.data.datasets.HCPfODFINRWholeVolDataset.default_tf(),\n",
    "    )\n",
    "\n",
    "print(\"=\" * 10)\n",
    "print(\"Warnings caught:\")\n",
    "ws = \"\\n\".join(\n",
    "    [\n",
    "        warnings.formatwarning(\n",
    "            w.message, w.category, w.filename, w.lineno, w.file, w.line\n",
    "        )\n",
    "        for w in warn_list\n",
    "    ]\n",
    ")\n",
    "ws = \"\\n\".join(filter(lambda s: bool(s.strip()), ws.splitlines()))\n",
    "print(ws, flush=True)\n",
    "print(\"=\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb7595d",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35c3cee7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Encoding model\n",
    "class INREncoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        interior_channels: int,\n",
    "        out_channels: int,\n",
    "        n_res_units: int,\n",
    "        n_dense_units: int,\n",
    "        activate_fn,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.init_kwargs = dict(\n",
    "            in_channels=in_channels,\n",
    "            interior_channels=interior_channels,\n",
    "            out_channels=out_channels,\n",
    "            n_res_units=n_res_units,\n",
    "            n_dense_units=n_dense_units,\n",
    "            activate_fn=activate_fn,\n",
    "        )\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.interior_channels = interior_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        if isinstance(activate_fn, str):\n",
    "            activate_fn = pitn.utils.torch_lookups.activation[activate_fn]\n",
    "\n",
    "        self._activation_fn_init = activate_fn\n",
    "        self.activate_fn = activate_fn()\n",
    "\n",
    "        # Pad to maintain the same input shape.\n",
    "        self.pre_conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(\n",
    "                self.in_channels,\n",
    "                self.in_channels,\n",
    "                kernel_size=1,\n",
    "                padding=\"same\",\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "            self.activate_fn,\n",
    "            torch.nn.Conv3d(\n",
    "                self.in_channels,\n",
    "                self.interior_channels,\n",
    "                kernel_size=3,\n",
    "                padding=\"same\",\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Construct the densely-connected cascading layers.\n",
    "        # Create n_dense_units number of dense units.\n",
    "        top_level_units = list()\n",
    "        for _ in range(n_dense_units):\n",
    "            # Create n_res_units number of residual units for every dense unit.\n",
    "            res_layers = list()\n",
    "            for _ in range(n_res_units):\n",
    "                res_layers.append(\n",
    "                    pitn.nn.layers.ResBlock3dNoBN(\n",
    "                        self.interior_channels,\n",
    "                        kernel_size=3,\n",
    "                        activate_fn=activate_fn,\n",
    "                        padding=\"same\",\n",
    "                        padding_mode=\"reflect\",\n",
    "                    )\n",
    "                )\n",
    "            top_level_units.append(\n",
    "                pitn.nn.layers.DenseCascadeBlock3d(self.interior_channels, *res_layers)\n",
    "            )\n",
    "\n",
    "        # Wrap everything into a densely-connected cascade.\n",
    "        self.cascade = pitn.nn.layers.DenseCascadeBlock3d(\n",
    "            self.interior_channels, *top_level_units\n",
    "        )\n",
    "\n",
    "        self.post_conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(\n",
    "                self.interior_channels,\n",
    "                self.interior_channels,\n",
    "                kernel_size=5,\n",
    "                padding=\"same\",\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "            self.activate_fn,\n",
    "            torch.nn.Conv3d(\n",
    "                self.interior_channels,\n",
    "                self.out_channels,\n",
    "                kernel_size=3,\n",
    "                padding=\"same\",\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "            self.activate_fn,\n",
    "            torch.nn.ReplicationPad3d((1, 0, 1, 0, 1, 0)),\n",
    "            torch.nn.AvgPool3d(kernel_size=2, stride=1),\n",
    "            torch.nn.Conv3d(\n",
    "                self.out_channels,\n",
    "                self.out_channels,\n",
    "                kernel_size=1,\n",
    "                padding=\"same\",\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "        )\n",
    "        # self.post_conv = torch.nn.Conv3d(\n",
    "        #     self.interior_channels,\n",
    "        #     self.out_channels,\n",
    "        #     kernel_size=3,\n",
    "        #     padding=\"same\",\n",
    "        #     padding_mode=\"reflect\",\n",
    "        # )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        y = self.pre_conv(x)\n",
    "        y = self.activate_fn(y)\n",
    "        y = self.cascade(y)\n",
    "        y = self.activate_fn(y)\n",
    "        y = self.post_conv(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class ReducedDecoder(torch.nn.Module):\n",
    "\n",
    "    TARGET_COORD_EPSILON = 1e-7\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        context_v_features: int,\n",
    "        out_features: int,\n",
    "        m_encode_num_freqs: int,\n",
    "        sigma_encode_scale: float,\n",
    "        in_features=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.init_kwargs = dict(\n",
    "            context_v_features=context_v_features,\n",
    "            out_features=out_features,\n",
    "            m_encode_num_freqs=m_encode_num_freqs,\n",
    "            sigma_encode_scale=sigma_encode_scale,\n",
    "            in_features=in_features,\n",
    "        )\n",
    "\n",
    "        # Determine the number of input features needed for the MLP.\n",
    "        # The order for concatenation is\n",
    "        # 1) ctx feats over the low-res input space, unfolded over a 3x3x3 window\n",
    "        # ~~2) target voxel shape~~\n",
    "        # 3) absolute coords of this forward pass' prediction target\n",
    "        # 4) absolute coords of the high-res target voxel\n",
    "        # ~~5) relative coords between high-res target coords and this forward pass'\n",
    "        #    prediction target, normalized by low-res voxel shape~~\n",
    "        # 6) encoding of relative coords\n",
    "        self.context_v_features = context_v_features\n",
    "        self.ndim = 3\n",
    "        self.m_encode_num_freqs = m_encode_num_freqs\n",
    "        self.sigma_encode_scale = torch.as_tensor(sigma_encode_scale)\n",
    "        self.n_encode_features = self.ndim * 2 * self.m_encode_num_freqs\n",
    "        self.n_coord_features = 2 * self.ndim + self.n_encode_features\n",
    "        self.internal_features = self.context_v_features + self.n_coord_features\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # \"Swish\" function, recommended in MeshFreeFlowNet\n",
    "        activate_cls = torch.nn.SiLU\n",
    "        self.activate_fn = activate_cls(inplace=True)\n",
    "        # Optional resizing linear layer, if the input size should be different than\n",
    "        # the hidden layer size.\n",
    "        if self.in_features is not None:\n",
    "            self.lin_pre = torch.nn.Linear(self.in_features, self.context_v_features)\n",
    "            self.norm_pre = None\n",
    "        else:\n",
    "            self.lin_pre = None\n",
    "            self.norm_pre = None\n",
    "        self.norm_pre = None\n",
    "\n",
    "        # Internal hidden layers are two res MLPs.\n",
    "        self.internal_res_repr = torch.nn.ModuleList(\n",
    "            [\n",
    "                pitn.nn.inr.SkipMLPBlock(\n",
    "                    n_context_features=self.context_v_features,\n",
    "                    n_coord_features=self.n_coord_features,\n",
    "                    n_dense_layers=3,\n",
    "                    activate_fn=activate_cls,\n",
    "                )\n",
    "                for _ in range(2)\n",
    "            ]\n",
    "        )\n",
    "        self.lin_post = torch.nn.Linear(self.context_v_features, self.out_features)\n",
    "\n",
    "    def encode_relative_coord(self, coords):\n",
    "        c = einops.rearrange(coords, \"b d x y z -> (b x y z) d\")\n",
    "        sigma = self.sigma_encode_scale.expand_as(c).to(c)[..., None]\n",
    "        encode_pos = pitn.nn.inr.fourier_position_encoding(\n",
    "            c, sigma_scale=sigma, m_num_freqs=self.m_encode_num_freqs\n",
    "        )\n",
    "\n",
    "        encode_pos = einops.rearrange(\n",
    "            encode_pos,\n",
    "            \"(b x y z) d -> b d x y z\",\n",
    "            x=coords.shape[2],\n",
    "            y=coords.shape[3],\n",
    "            z=coords.shape[4],\n",
    "        )\n",
    "        return encode_pos\n",
    "\n",
    "    def sub_grid_forward(\n",
    "        self,\n",
    "        context_val,\n",
    "        context_coord,\n",
    "        query_coord,\n",
    "        context_vox_size,\n",
    "        query_vox_size,\n",
    "        return_rel_context_coord=False,\n",
    "    ):\n",
    "        # Take relative coordinate difference between the current context\n",
    "        # coord and the query coord.\n",
    "        # rel_context_coord = context_coord - query_coord + self.TARGET_COORD_EPSILON\n",
    "        rel_context_coord = torch.clamp_min(\n",
    "            context_coord - query_coord,\n",
    "            (-context_vox_size / 2) + self.TARGET_COORD_EPSILON,\n",
    "        )\n",
    "        # Also normalize to [0, 1)\n",
    "        # Coordinates are located in the center of the voxel. By the way\n",
    "        # the context vector is being constructed surrounding the query\n",
    "        # coord, the query coord is always within 1.5 x vox_size of the\n",
    "        # context (low-res space) coordinate. So, subtract the\n",
    "        # batch-and-channel-wise minimum, and divide by the known upper\n",
    "        # bound.\n",
    "        rel_norm_context_coord = (\n",
    "            rel_context_coord\n",
    "            - torch.amin(rel_context_coord, dim=(2, 3, 4), keepdim=True)\n",
    "        ) / (1.5 * context_vox_size)\n",
    "        assert (rel_norm_context_coord >= 0).all() and (\n",
    "            rel_norm_context_coord < 1.0\n",
    "        ).all()\n",
    "        encoded_rel_norm_context_coord = self.encode_relative_coord(\n",
    "            rel_norm_context_coord\n",
    "        )\n",
    "\n",
    "        # Perform forward pass of the MLP.\n",
    "        if self.norm_pre is not None:\n",
    "            context_val = self.norm_pre(context_val)\n",
    "        context_feats = einops.rearrange(context_val, \"b c x y z -> (b x y z) c\")\n",
    "\n",
    "        # q_vox_size = query_vox_size.expand_as(rel_norm_context_coord)\n",
    "        coord_feats = (\n",
    "            # q_vox_size,\n",
    "            context_coord,\n",
    "            query_coord,\n",
    "            # rel_norm_context_coord,\n",
    "            encoded_rel_norm_context_coord,\n",
    "        )\n",
    "        coord_feats = torch.cat(coord_feats, dim=1)\n",
    "        spatial_layout = {\n",
    "            \"b\": coord_feats.shape[0],\n",
    "            \"x\": coord_feats.shape[2],\n",
    "            \"y\": coord_feats.shape[3],\n",
    "            \"z\": coord_feats.shape[4],\n",
    "        }\n",
    "\n",
    "        coord_feats = einops.rearrange(coord_feats, \"b c x y z -> (b x y z) c\")\n",
    "        x_coord = coord_feats\n",
    "        sub_grid_pred = context_feats\n",
    "\n",
    "        if self.lin_pre is not None:\n",
    "            sub_grid_pred = self.lin_pre(sub_grid_pred)\n",
    "            sub_grid_pred = self.activate_fn(sub_grid_pred)\n",
    "\n",
    "        for l in self.internal_res_repr:\n",
    "            sub_grid_pred, x_coord = l(sub_grid_pred, x_coord)\n",
    "        sub_grid_pred = self.lin_post(sub_grid_pred)\n",
    "        sub_grid_pred = einops.rearrange(\n",
    "            sub_grid_pred, \"(b x y z) c -> b c x y z\", **spatial_layout\n",
    "        )\n",
    "        if return_rel_context_coord:\n",
    "            ret = (sub_grid_pred, rel_context_coord)\n",
    "        else:\n",
    "            ret = sub_grid_pred\n",
    "        return ret\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        context_v,\n",
    "        context_spatial_extent,\n",
    "        query_vox_size,\n",
    "        query_coord,\n",
    "    ) -> torch.Tensor:\n",
    "        if query_vox_size.ndim == 2:\n",
    "            query_vox_size = query_vox_size[:, :, None, None, None]\n",
    "        context_vox_size = torch.abs(\n",
    "            context_spatial_extent[..., 1, 1, 1] - context_spatial_extent[..., 0, 0, 0]\n",
    "        )\n",
    "        context_vox_size = context_vox_size[:, :, None, None, None]\n",
    "\n",
    "        batch_size = query_coord.shape[0]\n",
    "        # Construct a grid of nearest indices in context space by sampling a grid of\n",
    "        # *indices* given the coordinates in mm.\n",
    "        # The channel dim is just repeated for every\n",
    "        # channel, so that doesn't need to be in the idx grid.\n",
    "        nearest_coord_idx = torch.stack(\n",
    "            torch.meshgrid(\n",
    "                *[\n",
    "                    torch.arange(\n",
    "                        0,\n",
    "                        context_spatial_extent.shape[i],\n",
    "                        dtype=context_spatial_extent.dtype,\n",
    "                        device=context_spatial_extent.device,\n",
    "                    )\n",
    "                    for i in (0, 2, 3, 4)\n",
    "                ],\n",
    "                indexing=\"ij\",\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        # Find the nearest grid point, where the batch+spatial dims are the\n",
    "        # \"channels.\"\n",
    "        nearest_coord_idx = pitn.nn.inr.weighted_ctx_v(\n",
    "            encoded_feat_vol=nearest_coord_idx,\n",
    "            input_space_extent=context_spatial_extent,\n",
    "            target_space_extent=query_coord,\n",
    "            reindex_spatial_extents=True,\n",
    "            sample_mode=\"nearest\",\n",
    "        ).to(torch.long)\n",
    "        # Expand along channel dimension for raw indexing.\n",
    "        nearest_coord_idx = einops.rearrange(\n",
    "            nearest_coord_idx, \"b dim i j k -> dim (b i j k)\"\n",
    "        )\n",
    "        batch_idx = nearest_coord_idx[0]\n",
    "\n",
    "        # Use the world coordinates to determine the necessary voxel coordinate\n",
    "        # offsets such that the offsets enclose the query point.\n",
    "        # World coordinate in the low-res input grid that is closest to the\n",
    "        # query coordinate.\n",
    "        phys_coords_0 = context_spatial_extent[\n",
    "            batch_idx,\n",
    "            :,\n",
    "            nearest_coord_idx[1],\n",
    "            nearest_coord_idx[2],\n",
    "            nearest_coord_idx[3],\n",
    "        ]\n",
    "\n",
    "        phys_coords_0 = einops.rearrange(\n",
    "            phys_coords_0,\n",
    "            \"(b x y z) c -> b c x y z\",\n",
    "            b=batch_size,\n",
    "            c=query_coord.shape[1],\n",
    "            x=query_coord.shape[2],\n",
    "            y=query_coord.shape[3],\n",
    "            z=query_coord.shape[4],\n",
    "        )\n",
    "        # Determine the quadrants that the query point lies in relative to the\n",
    "        # context grid. We only care about the spatial/non-batch coordinates.\n",
    "        surround_query_point_quadrants = (\n",
    "            query_coord - self.TARGET_COORD_EPSILON - phys_coords_0\n",
    "        )\n",
    "        # 3 x batch_and_spatial_size\n",
    "        # The signs of the \"query coordinate - grid coordinate\" should match the\n",
    "        # direction the indexing should go for the nearest voxels to the query.\n",
    "        surround_offsets_vox = einops.rearrange(\n",
    "            surround_query_point_quadrants.sign(), \"b dim i j k -> dim (b i j k)\"\n",
    "        ).to(torch.int8)\n",
    "        del surround_query_point_quadrants\n",
    "\n",
    "        # Now, find sum of distances to normalize the distance-weighted weight vector\n",
    "        # for in-place 'linear interpolation.'\n",
    "        inv_dist_total = torch.zeros_like(phys_coords_0)\n",
    "        inv_dist_total = (inv_dist_total[:, 0])[:, None]\n",
    "        surround_offsets_vox_volume_order = einops.rearrange(\n",
    "            surround_offsets_vox,\n",
    "            \"dim (b i j k) -> b dim i j k\",\n",
    "            b=batch_size,\n",
    "            i=query_coord.shape[2],\n",
    "            j=query_coord.shape[3],\n",
    "            k=query_coord.shape[4],\n",
    "        )\n",
    "        for (\n",
    "            offcenter_indicate_i,\n",
    "            offcenter_indicate_j,\n",
    "            offcenter_indicate_k,\n",
    "        ) in itertools.product((0, 1), (0, 1), (0, 1)):\n",
    "            phys_coords_offset = torch.ones_like(phys_coords_0)\n",
    "            phys_coords_offset[:, 0] *= (\n",
    "                offcenter_indicate_i * surround_offsets_vox_volume_order[:, 0]\n",
    "            ) * context_vox_size[:, 0]\n",
    "            phys_coords_offset[:, 1] *= (\n",
    "                offcenter_indicate_j * surround_offsets_vox_volume_order[:, 1]\n",
    "            ) * context_vox_size[:, 1]\n",
    "            phys_coords_offset[:, 2] *= (\n",
    "                offcenter_indicate_k * surround_offsets_vox_volume_order[:, 2]\n",
    "            ) * context_vox_size[:, 2]\n",
    "            # phys_coords_offset = context_vox_size * phys_coords_offset\n",
    "            phys_coords = phys_coords_0 + phys_coords_offset\n",
    "            inv_dist_total += 1 / torch.linalg.vector_norm(\n",
    "                query_coord - phys_coords, ord=2, dim=1, keepdim=True\n",
    "            )\n",
    "        # Potentially free some memory here.\n",
    "        del phys_coords\n",
    "        del phys_coords_0\n",
    "        del phys_coords_offset\n",
    "        del surround_offsets_vox_volume_order\n",
    "\n",
    "        y_weighted_accumulate = None\n",
    "        # Build the low-res representation one sub-window voxel index at a time.\n",
    "        # The indicators specify if the current voxel index that surrounds the\n",
    "        # query coordinate should be \"off the center voxel\" or not. If not, then\n",
    "        # the center voxel (read: no voxel offset from the center) is selected\n",
    "        # (for that dimension).\n",
    "        for (\n",
    "            offcenter_indicate_i,\n",
    "            offcenter_indicate_j,\n",
    "            offcenter_indicate_k,\n",
    "        ) in itertools.product((0, 1), (0, 1), (0, 1)):\n",
    "            # Rebuild indexing tuple for each element of the sub-window\n",
    "            i_idx = nearest_coord_idx[1] + (\n",
    "                offcenter_indicate_i * surround_offsets_vox[0]\n",
    "            )\n",
    "            j_idx = nearest_coord_idx[2] + (\n",
    "                offcenter_indicate_j * surround_offsets_vox[1]\n",
    "            )\n",
    "            k_idx = nearest_coord_idx[3] + (\n",
    "                offcenter_indicate_k * surround_offsets_vox[2]\n",
    "            )\n",
    "            context_val = context_v[batch_idx, :, i_idx, j_idx, k_idx]\n",
    "            context_val = einops.rearrange(\n",
    "                context_val,\n",
    "                \"(b x y z) c -> b c x y z\",\n",
    "                b=batch_size,\n",
    "                x=query_coord.shape[2],\n",
    "                y=query_coord.shape[3],\n",
    "                z=query_coord.shape[4],\n",
    "            )\n",
    "            context_coord = context_spatial_extent[batch_idx, :, i_idx, j_idx, k_idx]\n",
    "            context_coord = einops.rearrange(\n",
    "                context_coord,\n",
    "                \"(b x y z) c -> b c x y z\",\n",
    "                b=batch_size,\n",
    "                x=query_coord.shape[2],\n",
    "                y=query_coord.shape[3],\n",
    "                z=query_coord.shape[4],\n",
    "            )\n",
    "\n",
    "            sub_grid_pred_ijk = self.sub_grid_forward(\n",
    "                context_val=context_val,\n",
    "                context_coord=context_coord,\n",
    "                query_coord=query_coord,\n",
    "                context_vox_size=context_vox_size,\n",
    "                query_vox_size=query_vox_size,\n",
    "                return_rel_context_coord=False,\n",
    "            )\n",
    "            # Initialize the accumulated prediction after finding the\n",
    "            # output size; easier than trying to pre-compute it.\n",
    "            if y_weighted_accumulate is None:\n",
    "                y_weighted_accumulate = torch.zeros_like(sub_grid_pred_ijk)\n",
    "\n",
    "            # Weigh this cell's prediction by the inverse of the distance\n",
    "            # from the cell physical coordinate to the true target\n",
    "            # physical coordinate. Normalize the weight by the inverse\n",
    "            # \"sum of the inverse distances\" found before.\n",
    "            w = (\n",
    "                1\n",
    "                / torch.linalg.vector_norm(\n",
    "                    query_coord - context_coord, ord=2, dim=1, keepdim=True\n",
    "                )\n",
    "            ) / inv_dist_total\n",
    "\n",
    "            # Accumulate weighted cell predictions to eventually create\n",
    "            # the final prediction.\n",
    "            y_weighted_accumulate += w * sub_grid_pred_ijk\n",
    "            del sub_grid_pred_ijk\n",
    "\n",
    "        y = y_weighted_accumulate\n",
    "\n",
    "        return y\n",
    "\n",
    "    # def forward(\n",
    "    #     self,\n",
    "    #     context_v,\n",
    "    #     context_spatial_extent,\n",
    "    #     query_vox_size,\n",
    "    #     query_coord,\n",
    "    # ) -> torch.Tensor:\n",
    "    #     if query_vox_size.ndim == 2:\n",
    "    #         query_vox_size = query_vox_size[:, :, None, None, None]\n",
    "    #     context_vox_size = torch.abs(\n",
    "    #         context_spatial_extent[..., 1, 1, 1] - context_spatial_extent[..., 0, 0, 0]\n",
    "    #     )\n",
    "    #     context_vox_size = context_vox_size[:, :, None, None, None]\n",
    "\n",
    "    #     batch_size = query_coord.shape[0]\n",
    "    #     # Construct a grid of nearest indices in context space by sampling a grid of\n",
    "    #     # *indices* given the coordinates in mm.\n",
    "    #     # The channel dim is just repeated for every\n",
    "    #     # channel, so that doesn't need to be in the idx grid.\n",
    "    #     nearest_coord_idx = torch.stack(\n",
    "    #         torch.meshgrid(\n",
    "    #             *[\n",
    "    #                 torch.arange(\n",
    "    #                     0,\n",
    "    #                     context_spatial_extent.shape[i],\n",
    "    #                     dtype=context_spatial_extent.dtype,\n",
    "    #                     device=context_spatial_extent.device,\n",
    "    #                 )\n",
    "    #                 for i in (0, 2, 3, 4)\n",
    "    #             ],\n",
    "    #             indexing=\"ij\",\n",
    "    #         ),\n",
    "    #         dim=1,\n",
    "    #     )\n",
    "\n",
    "    #     # Find the nearest grid point, where the batch+spatial dims are the\n",
    "    #     # \"channels.\"\n",
    "    #     nearest_coord_idx = pitn.nn.inr.weighted_ctx_v(\n",
    "    #         encoded_feat_vol=nearest_coord_idx,\n",
    "    #         input_space_extent=context_spatial_extent,\n",
    "    #         target_space_extent=query_coord,\n",
    "    #         reindex_spatial_extents=True,\n",
    "    #         sample_mode=\"nearest\",\n",
    "    #     ).to(torch.long)\n",
    "    #     # Expand along channel dimension for raw indexing.\n",
    "    #     nearest_coord_idx = einops.rearrange(\n",
    "    #         nearest_coord_idx, \"b dim i j k -> dim (b i j k)\"\n",
    "    #     )\n",
    "    #     batch_idx = nearest_coord_idx[0]\n",
    "\n",
    "    #     # Use the world coordinates to determine the necessary voxel coordinate\n",
    "    #     # offsets such that the offsets enclose the query point.\n",
    "    #     # World coordinate in the low-res input grid that is closest to the\n",
    "    #     # query coordinate.\n",
    "    #     phys_coords_0 = context_spatial_extent[\n",
    "    #         batch_idx,\n",
    "    #         :,\n",
    "    #         nearest_coord_idx[1],\n",
    "    #         nearest_coord_idx[2],\n",
    "    #         nearest_coord_idx[3],\n",
    "    #     ]\n",
    "\n",
    "    #     phys_coords_0 = einops.rearrange(\n",
    "    #         phys_coords_0,\n",
    "    #         \"(b x y z) c -> b c x y z\",\n",
    "    #         b=batch_size,\n",
    "    #         c=query_coord.shape[1],\n",
    "    #         x=query_coord.shape[2],\n",
    "    #         y=query_coord.shape[3],\n",
    "    #         z=query_coord.shape[4],\n",
    "    #     )\n",
    "    #     # Determine the quadrants that the query point lies in relative to the\n",
    "    #     # context grid. We only care about the spatial/non-batch coordinates.\n",
    "    #     surround_query_point_quadrants = (\n",
    "    #         query_coord - self.TARGET_COORD_EPSILON - phys_coords_0\n",
    "    #     )\n",
    "    #     # 3 x batch_and_spatial_size\n",
    "    #     # The signs of the \"query coordinate - grid coordinate\" should match the\n",
    "    #     # direction the indexing should go for the nearest voxels to the query.\n",
    "    #     surround_offsets_vox = einops.rearrange(\n",
    "    #         surround_query_point_quadrants.sign(), \"b dim i j k -> dim (b i j k)\"\n",
    "    #     ).to(torch.int8)\n",
    "    #     del surround_query_point_quadrants\n",
    "\n",
    "    #     # Now, find sum of distances to normalize the distance-weighted weight vector\n",
    "    #     # for in-place 'linear interpolation.'\n",
    "    #     inv_dist_total = torch.zeros_like(phys_coords_0)\n",
    "    #     inv_dist_total = (inv_dist_total[:, 0])[:, None]\n",
    "    #     surround_offsets_vox_volume_order = einops.rearrange(\n",
    "    #         surround_offsets_vox,\n",
    "    #         \"dim (b i j k) -> b dim i j k\",\n",
    "    #         b=batch_size,\n",
    "    #         i=query_coord.shape[2],\n",
    "    #         j=query_coord.shape[3],\n",
    "    #         k=query_coord.shape[4],\n",
    "    #     )\n",
    "    #     for (\n",
    "    #         offcenter_indicate_i,\n",
    "    #         offcenter_indicate_j,\n",
    "    #         offcenter_indicate_k,\n",
    "    #     ) in itertools.product((0, 1), (0, 1), (0, 1)):\n",
    "    #         phys_coords_offset = torch.ones_like(phys_coords_0)\n",
    "    #         phys_coords_offset[:, 0] *= (\n",
    "    #             offcenter_indicate_i * surround_offsets_vox_volume_order[:, 0]\n",
    "    #         ) * context_vox_size[:, 0]\n",
    "    #         phys_coords_offset[:, 1] *= (\n",
    "    #             offcenter_indicate_j * surround_offsets_vox_volume_order[:, 1]\n",
    "    #         ) * context_vox_size[:, 1]\n",
    "    #         phys_coords_offset[:, 2] *= (\n",
    "    #             offcenter_indicate_k * surround_offsets_vox_volume_order[:, 2]\n",
    "    #         ) * context_vox_size[:, 2]\n",
    "    #         # phys_coords_offset = context_vox_size * phys_coords_offset\n",
    "    #         phys_coords = phys_coords_0 + phys_coords_offset\n",
    "    #         inv_dist_total += 1 / torch.linalg.vector_norm(\n",
    "    #             query_coord - phys_coords, ord=2, dim=1, keepdim=True\n",
    "    #         )\n",
    "    #     # Potentially free some memory here.\n",
    "    #     del phys_coords\n",
    "    #     del phys_coords_0\n",
    "    #     del phys_coords_offset\n",
    "    #     del surround_offsets_vox_volume_order\n",
    "\n",
    "    #     y_weighted_accumulate = None\n",
    "    #     # Build the low-res representation one sub-window voxel index at a time.\n",
    "    #     # The indicators specify if the current voxel index that surrounds the\n",
    "    #     # query coordinate should be \"off the center voxel\" or not. If not, then\n",
    "    #     # the center voxel (read: no voxel offset from the center) is selected\n",
    "    #     # (for that dimension).\n",
    "    #     for (\n",
    "    #         offcenter_indicate_i,\n",
    "    #         offcenter_indicate_j,\n",
    "    #         offcenter_indicate_k,\n",
    "    #     ) in itertools.product((0, 1), (0, 1), (0, 1)):\n",
    "    #         # Rebuild indexing tuple for each element of the sub-window\n",
    "    #         i_idx = nearest_coord_idx[1] + (\n",
    "    #             offcenter_indicate_i * surround_offsets_vox[0]\n",
    "    #         )\n",
    "    #         j_idx = nearest_coord_idx[2] + (\n",
    "    #             offcenter_indicate_j * surround_offsets_vox[1]\n",
    "    #         )\n",
    "    #         k_idx = nearest_coord_idx[3] + (\n",
    "    #             offcenter_indicate_k * surround_offsets_vox[2]\n",
    "    #         )\n",
    "    #         context_val = context_v[batch_idx, :, i_idx, j_idx, k_idx]\n",
    "    #         context_val = einops.rearrange(\n",
    "    #             context_val,\n",
    "    #             \"(b x y z) c -> b c x y z\",\n",
    "    #             b=batch_size,\n",
    "    #             x=query_coord.shape[2],\n",
    "    #             y=query_coord.shape[3],\n",
    "    #             z=query_coord.shape[4],\n",
    "    #         )\n",
    "    #         context_coord = context_spatial_extent[batch_idx, :, i_idx, j_idx, k_idx]\n",
    "    #         context_coord = einops.rearrange(\n",
    "    #             context_coord,\n",
    "    #             \"(b x y z) c -> b c x y z\",\n",
    "    #             b=batch_size,\n",
    "    #             x=query_coord.shape[2],\n",
    "    #             y=query_coord.shape[3],\n",
    "    #             z=query_coord.shape[4],\n",
    "    #         )\n",
    "\n",
    "    #         sub_grid_pred_ijk = self.sub_grid_forward(\n",
    "    #             context_val=context_val,\n",
    "    #             context_coord=context_coord,\n",
    "    #             query_coord=query_coord,\n",
    "    #             context_vox_size=context_vox_size,\n",
    "    #             query_vox_size=query_vox_size,\n",
    "    #             return_rel_context_coord=False,\n",
    "    #         )\n",
    "    #         # Initialize the accumulated prediction after finding the\n",
    "    #         # output size; easier than trying to pre-compute it.\n",
    "    #         if y_weighted_accumulate is None:\n",
    "    #             y_weighted_accumulate = torch.zeros_like(sub_grid_pred_ijk)\n",
    "\n",
    "    #         # Weigh this cell's prediction by the inverse of the distance\n",
    "    #         # from the cell physical coordinate to the true target\n",
    "    #         # physical coordinate. Normalize the weight by the inverse\n",
    "    #         # \"sum of the inverse distances\" found before.\n",
    "    #         w = (\n",
    "    #             1\n",
    "    #             / torch.linalg.vector_norm(\n",
    "    #                 query_coord - context_coord, ord=2, dim=1, keepdim=True\n",
    "    #             )\n",
    "    #         ) / inv_dist_total\n",
    "\n",
    "    #         # Accumulate weighted cell predictions to eventually create\n",
    "    #         # the final prediction.\n",
    "    #         y_weighted_accumulate += w * sub_grid_pred_ijk\n",
    "    #         del sub_grid_pred_ijk\n",
    "\n",
    "    #     y = y_weighted_accumulate\n",
    "\n",
    "    #     return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343cddbb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a78e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger_run(run_kwargs: dict, logger_meta_params: dict, logger_tags: list):\n",
    "    aim_run = aim.Run(\n",
    "        system_tracking_interval=None,\n",
    "        log_system_params=True,\n",
    "        capture_terminal_logs=True,\n",
    "        **run_kwargs,\n",
    "    )\n",
    "    for k, v in logger_meta_params.items():\n",
    "        aim_run[k] = v\n",
    "    for v in logger_tags:\n",
    "        aim_run.add_tag(v)\n",
    "\n",
    "    return aim_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a059d78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad_norm(model, norm_type=2):\n",
    "    # https://discuss.pytorch.org/t/check-the-norm-of-gradients/27961/5\n",
    "    total_norm = 0\n",
    "    parameters = [\n",
    "        p for p in model.parameters() if p.grad is not None and p.requires_grad\n",
    "    ]\n",
    "    for p in parameters:\n",
    "        param_norm = p.grad.detach().data.norm(2)\n",
    "        total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm**0.5\n",
    "    return total_norm\n",
    "\n",
    "\n",
    "def batchwise_masked_mse(y_pred, y, mask):\n",
    "    masked_y_pred = y_pred.clone()\n",
    "    masked_y = y.clone()\n",
    "    masked_y_pred[mask] = torch.nan\n",
    "    masked_y[mask] = torch.nan\n",
    "    se = F.mse_loss(masked_y_pred, masked_y, reduction=\"none\")\n",
    "    se = se.reshape(se.shape[0], -1)\n",
    "    mse = torch.nanmean(se, dim=1)\n",
    "    return mse\n",
    "\n",
    "\n",
    "def validate_stage(\n",
    "    fabric,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    val_dataloader,\n",
    "    step: int,\n",
    "    epoch: int,\n",
    "    aim_run,\n",
    "    val_viz_subj_id,\n",
    "):\n",
    "    encoder_was_training = encoder.training\n",
    "    decoder_was_training = decoder.training\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        # Set up validation metrics to track for this validation run.\n",
    "        val_metrics = {\"mse\": list()}\n",
    "        for batch_dict in val_dataloader:\n",
    "            subj_id = batch_dict[\"subj_id\"]\n",
    "            if len(subj_id) == 1:\n",
    "                subj_id = subj_id[0]\n",
    "            if val_viz_subj_id is None:\n",
    "                val_viz_subj_id = subj_id\n",
    "            x = batch_dict[\"lr_dwi\"]\n",
    "            x_coords = batch_dict[\"lr_extent_acpc\"]\n",
    "            # lr_fodf = batch_dict[\"lr_fodf\"]\n",
    "            y = batch_dict[\"fodf\"]\n",
    "            y_mask = batch_dict[\"mask\"].to(torch.bool)\n",
    "            y_coords = batch_dict[\"extent_acpc\"]\n",
    "            y_vox_size = torch.atleast_2d(batch_dict[\"vox_size\"])\n",
    "\n",
    "            ctx_v = encoder(x)\n",
    "            # pred_fodf = decoder(\n",
    "            #     context_v=ctx_v,\n",
    "            #     context_spatial_extent=x_coords,\n",
    "            #     query_vox_size=y_vox_size,\n",
    "            #     query_coord=y_coords,\n",
    "            # )\n",
    "            # Whole-volume inference is memory-prohibitive, so use a sliding\n",
    "            # window inference method on the encoded volume.\n",
    "            pred_fodf = monai.inferers.sliding_window_inference(\n",
    "                y_coords,\n",
    "                roi_size=(32, 32, 32),\n",
    "                sw_batch_size=y_coords.shape[0],\n",
    "                predictor=lambda q: decoder(\n",
    "                    query_coord=q,\n",
    "                    context_v=ctx_v,\n",
    "                    context_spatial_extent=x_coords,\n",
    "                    query_vox_size=y_vox_size,\n",
    "                ),\n",
    "                overlap=0,\n",
    "                padding_mode=\"replicate\",\n",
    "            )\n",
    "\n",
    "            y_mask_broad = torch.broadcast_to(y_mask, y.shape)\n",
    "            # Calculate performance metrics\n",
    "            mse_loss = batchwise_masked_mse(y, pred_fodf, mask=y_mask_broad)\n",
    "            val_metrics[\"mse\"].append(mse_loss.detach().cpu().flatten())\n",
    "\n",
    "            # If visualization subj_id is in this batch, create the visual and log it.\n",
    "            if subj_id == val_viz_subj_id:\n",
    "                with mpl.rc_context({\"font.size\": 6.0}):\n",
    "                    fig = plt.figure(dpi=175, figsize=(8, 5))\n",
    "                    fig, _ = pitn.viz.plot_fodf_coeff_slices(\n",
    "                        pred_fodf,\n",
    "                        y,\n",
    "                        pred_fodf - y,\n",
    "                        fig=fig,\n",
    "                        fodf_vol_labels=(\"Predicted\", \"Target\", \"Pred - GT\"),\n",
    "                        imshow_kwargs={\n",
    "                            \"interpolation\": \"antialiased\",\n",
    "                            \"cmap\": \"gray\",\n",
    "                        },\n",
    "                    )\n",
    "                    aim_run.track(\n",
    "                        aim.Image(\n",
    "                            fig,\n",
    "                            caption=f\"Val Subj {subj_id}, \"\n",
    "                            + f\"MSE = {val_metrics['mse'][-1].item()}\",\n",
    "                            optimize=True,\n",
    "                            quality=100,\n",
    "                            format=\"png\",\n",
    "                        ),\n",
    "                        name=\"sh_whole_volume\",\n",
    "                        context={\"subset\": \"val\"},\n",
    "                        epoch=epoch,\n",
    "                        step=step,\n",
    "                    )\n",
    "                    plt.close(fig)\n",
    "\n",
    "                # Plot MSE as distributed over the SH orders.\n",
    "                sh_coeff_labels = {\n",
    "                    \"idx\": list(range(0, 45)),\n",
    "                    \"l\": np.concatenate(\n",
    "                        list(\n",
    "                            map(\n",
    "                                lambda x: np.array([x] * (2 * x + 1)),\n",
    "                                range(0, 9, 2),\n",
    "                            )\n",
    "                        ),\n",
    "                        dtype=int,\n",
    "                    ).flatten(),\n",
    "                }\n",
    "                error_fodf = F.mse_loss(pred_fodf, y, reduction=\"none\")\n",
    "                error_fodf = einops.rearrange(\n",
    "                    error_fodf, \"b sh_idx x y z -> b x y z sh_idx\"\n",
    "                )\n",
    "                error_fodf = error_fodf[\n",
    "                    y_mask[:, 0, ..., None].broadcast_to(error_fodf.shape)\n",
    "                ]\n",
    "                error_fodf = einops.rearrange(\n",
    "                    error_fodf, \"(elem sh_idx) -> elem sh_idx\", sh_idx=45\n",
    "                )\n",
    "                error_fodf = error_fodf.flatten().detach().cpu().numpy()\n",
    "                error_df = pd.DataFrame.from_dict(\n",
    "                    {\n",
    "                        \"MSE\": error_fodf,\n",
    "                        \"SH_idx\": np.tile(\n",
    "                            sh_coeff_labels[\"idx\"], error_fodf.shape[0] // 45\n",
    "                        ),\n",
    "                        \"L Order\": np.tile(\n",
    "                            sh_coeff_labels[\"l\"], error_fodf.shape[0] // 45\n",
    "                        ),\n",
    "                    }\n",
    "                )\n",
    "                with mpl.rc_context({\"font.size\": 6.0}):\n",
    "                    fig = plt.figure(dpi=140, figsize=(6, 2))\n",
    "                    sns.boxplot(\n",
    "                        data=error_df,\n",
    "                        x=\"SH_idx\",\n",
    "                        y=\"MSE\",\n",
    "                        hue=\"L Order\",\n",
    "                        linewidth=0.8,\n",
    "                        showfliers=False,\n",
    "                        width=0.85,\n",
    "                        dodge=False,\n",
    "                    )\n",
    "                    aim_run.track(\n",
    "                        aim.Image(fig, caption=\"MSE over SH orders\", optimize=True),\n",
    "                        name=\"mse_over_sh_orders\",\n",
    "                        epoch=epoch,\n",
    "                        step=step,\n",
    "                    )\n",
    "                    plt.close(fig)\n",
    "            fabric.print(f\"MSE {val_metrics['mse'][-1].item()}\")\n",
    "            fabric.print(\"Finished validation subj \", subj_id)\n",
    "            del pred_fodf\n",
    "\n",
    "    val_metrics[\"mse\"] = torch.cat(val_metrics[\"mse\"])\n",
    "    # Log metrics\n",
    "    aim_run.track(\n",
    "        {\"mse\": val_metrics[\"mse\"].mean().numpy()},\n",
    "        context={\"subset\": \"val\"},\n",
    "        step=step,\n",
    "        epoch=epoch,\n",
    "    )\n",
    "\n",
    "    encoder.train(mode=encoder_was_training)\n",
    "    decoder.train(mode=decoder_was_training)\n",
    "    return aim_run, val_viz_subj_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2721d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "# Break ISO format because many programs don't like having colons ':' in a filename.\n",
    "ts = ts.replace(\":\", \"_\")\n",
    "tmp_res_dir = Path(p.tmp_results_dir) / ts\n",
    "tmp_res_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00fd0ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tas6hh/miniconda/envs/pitn/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning:\n",
      "\n",
      "Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INREncoder(\n",
      "  (activate_fn): ReLU()\n",
      "  (pre_conv): Sequential(\n",
      "    (0): Conv3d(189, 189, kernel_size=(1, 1, 1), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "    (1): ReLU()\n",
      "    (2): Conv3d(189, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "  )\n",
      "  (cascade): DenseCascadeBlock3d(\n",
      "    (base_layers): ModuleList(\n",
      "      (0): DenseCascadeBlock3d(\n",
      "        (base_layers): ModuleList(\n",
      "          (0): ResBlock3dNoBN(\n",
      "            (conv1): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (conv2): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (active_fn): ReLU()\n",
      "          )\n",
      "          (1): ResBlock3dNoBN(\n",
      "            (conv1): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (conv2): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (active_fn): ReLU()\n",
      "          )\n",
      "          (2): ResBlock3dNoBN(\n",
      "            (conv1): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (conv2): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (active_fn): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (combiner_convs): ModuleList(\n",
      "          (0): LazyConv3d(0, 80, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "          (1): LazyConv3d(0, 80, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "          (2): LazyConv3d(0, 80, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "        )\n",
      "      )\n",
      "      (1): DenseCascadeBlock3d(\n",
      "        (base_layers): ModuleList(\n",
      "          (0): ResBlock3dNoBN(\n",
      "            (conv1): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (conv2): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (active_fn): ReLU()\n",
      "          )\n",
      "          (1): ResBlock3dNoBN(\n",
      "            (conv1): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (conv2): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (active_fn): ReLU()\n",
      "          )\n",
      "          (2): ResBlock3dNoBN(\n",
      "            (conv1): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (conv2): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (active_fn): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (combiner_convs): ModuleList(\n",
      "          (0): LazyConv3d(0, 80, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "          (1): LazyConv3d(0, 80, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "          (2): LazyConv3d(0, 80, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "        )\n",
      "      )\n",
      "      (2): DenseCascadeBlock3d(\n",
      "        (base_layers): ModuleList(\n",
      "          (0): ResBlock3dNoBN(\n",
      "            (conv1): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (conv2): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (active_fn): ReLU()\n",
      "          )\n",
      "          (1): ResBlock3dNoBN(\n",
      "            (conv1): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (conv2): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (active_fn): ReLU()\n",
      "          )\n",
      "          (2): ResBlock3dNoBN(\n",
      "            (conv1): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (conv2): Conv3d(80, 80, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (active_fn): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (combiner_convs): ModuleList(\n",
      "          (0): LazyConv3d(0, 80, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "          (1): LazyConv3d(0, 80, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "          (2): LazyConv3d(0, 80, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (combiner_convs): ModuleList(\n",
      "      (0): LazyConv3d(0, 80, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (1): LazyConv3d(0, 80, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (2): LazyConv3d(0, 80, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "    )\n",
      "  )\n",
      "  (post_conv): Sequential(\n",
      "    (0): Conv3d(80, 80, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "    (1): ReLU()\n",
      "    (2): Conv3d(80, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "    (3): ReLU()\n",
      "    (4): ReplicationPad3d((1, 0, 1, 0, 1, 0))\n",
      "    (5): AvgPool3d(kernel_size=2, stride=1, padding=0)\n",
      "    (6): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "  )\n",
      ")\n",
      "ReducedDecoder(\n",
      "  (activate_fn): SiLU(inplace=True)\n",
      "  (lin_pre): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (internal_res_repr): ModuleList(\n",
      "    (0): SkipMLPBlock(\n",
      "      (layers): ModuleList(\n",
      "        (0): ModuleDict(\n",
      "          (linear): Linear(in_features=350, out_features=128, bias=True)\n",
      "          (activate_fn): SiLU()\n",
      "          (norm): None\n",
      "        )\n",
      "        (1): ModuleDict(\n",
      "          (linear): Linear(in_features=350, out_features=128, bias=True)\n",
      "          (activate_fn): SiLU()\n",
      "          (norm): None\n",
      "        )\n",
      "        (2): ModuleDict(\n",
      "          (linear): Linear(in_features=350, out_features=128, bias=True)\n",
      "          (activate_fn): SiLU()\n",
      "          (norm): None\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): SkipMLPBlock(\n",
      "      (layers): ModuleList(\n",
      "        (0): ModuleDict(\n",
      "          (linear): Linear(in_features=350, out_features=128, bias=True)\n",
      "          (activate_fn): SiLU()\n",
      "          (norm): None\n",
      "        )\n",
      "        (1): ModuleDict(\n",
      "          (linear): Linear(in_features=350, out_features=128, bias=True)\n",
      "          (activate_fn): SiLU()\n",
      "          (norm): None\n",
      "        )\n",
      "        (2): ModuleDict(\n",
      "          (linear): Linear(in_features=350, out_features=128, bias=True)\n",
      "          (activate_fn): SiLU()\n",
      "          (norm): None\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lin_post): Linear(in_features=128, out_features=45, bias=True)\n",
      ")\n",
      "INREncoder(\n",
      "  (activate_fn): ReLU()\n",
      "  (pre_conv): Sequential(\n",
      "    (0): Conv3d(128, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "    (1): ReLU()\n",
      "    (2): Conv3d(128, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "  )\n",
      "  (cascade): DenseCascadeBlock3d(\n",
      "    (base_layers): ModuleList(\n",
      "      (0): DenseCascadeBlock3d(\n",
      "        (base_layers): ModuleList(\n",
      "          (0): ResBlock3dNoBN(\n",
      "            (conv1): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (conv2): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (active_fn): ReLU()\n",
      "          )\n",
      "          (1): ResBlock3dNoBN(\n",
      "            (conv1): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (conv2): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (active_fn): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (combiner_convs): ModuleList(\n",
      "          (0): LazyConv3d(0, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "          (1): LazyConv3d(0, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "        )\n",
      "      )\n",
      "      (1): DenseCascadeBlock3d(\n",
      "        (base_layers): ModuleList(\n",
      "          (0): ResBlock3dNoBN(\n",
      "            (conv1): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (conv2): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (active_fn): ReLU()\n",
      "          )\n",
      "          (1): ResBlock3dNoBN(\n",
      "            (conv1): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (conv2): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (active_fn): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (combiner_convs): ModuleList(\n",
      "          (0): LazyConv3d(0, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "          (1): LazyConv3d(0, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (combiner_convs): ModuleList(\n",
      "      (0): LazyConv3d(0, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (1): LazyConv3d(0, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "    )\n",
      "  )\n",
      "  (post_conv): Sequential(\n",
      "    (0): Conv3d(48, 48, kernel_size=(5, 5, 5), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "    (1): ReLU()\n",
      "    (2): Conv3d(48, 6, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "    (3): ReLU()\n",
      "    (4): ReplicationPad3d((1, 0, 1, 0, 1, 0))\n",
      "    (5): AvgPool3d(kernel_size=2, stride=1, padding=0)\n",
      "    (6): Conv3d(6, 6, kernel_size=(1, 1, 1), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "  )\n",
      ")\n",
      "\n",
      "Epoch 0\n",
      " ==========\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/tas6hh/Projects/pitn/notebooks/continuous_sr/inr.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/inr.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=225'>226</a>\u001b[0m     aim_run\u001b[39m.\u001b[39madd_tag(\u001b[39m\"\u001b[39m\u001b[39mSTOPPED\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/inr.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=226'>227</a>\u001b[0m     (tmp_res_dir \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSTOPPED\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mtouch()\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/inr.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=227'>228</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/inr.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=228'>229</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/inr.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=229'>230</a>\u001b[0m     aim_run\u001b[39m.\u001b[39madd_tag(\u001b[39m\"\u001b[39m\u001b[39mFAILED\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/tas6hh/Projects/pitn/notebooks/continuous_sr/inr.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/inr.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/inr.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m     train_recon \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/inr.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_dict \u001b[39min\u001b[39;00m train_dataloader:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/inr.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m     x \u001b[39m=\u001b[39m batch_dict[\u001b[39m\"\u001b[39m\u001b[39mlr_dwi\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/inr.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m     x_coords \u001b[39m=\u001b[39m batch_dict[\u001b[39m\"\u001b[39m\u001b[39mlr_patch_extent_acpc\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda/envs/pitn/lib/python3.10/site-packages/lightning_fabric/wrappers.py:183\u001b[0m, in \u001b[0;36m_FabricDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[39myield from\u001b[39;00m iterator\n\u001b[1;32m    181\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m iterator:\n\u001b[1;32m    184\u001b[0m     \u001b[39myield\u001b[39;00m move_data_to_device(item, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device)\n",
      "File \u001b[0;32m~/miniconda/envs/pitn/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda/envs/pitn/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1316\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1315\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1316\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1319\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/pitn/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1272\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1270\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m   1271\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_thread\u001b[39m.\u001b[39mis_alive():\n\u001b[0;32m-> 1272\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1273\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1274\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda/envs/pitn/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1120\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1108\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1120\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1121\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1122\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1123\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/pitn/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[39mif\u001b[39;00m remaining \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait(remaining)\n\u001b[1;32m    181\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnot_full\u001b[39m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/miniconda/envs/pitn/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    325\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fabric = Fabric(accelerator=\"gpu\", devices=1, precision=32)\n",
    "fabric.launch()\n",
    "device = fabric.device\n",
    "\n",
    "aim_run = setup_logger_run(\n",
    "    run_kwargs={\n",
    "        k: p.aim_logger[k] for k in set(p.aim_logger.keys()) - {\"meta_params\", \"tags\"}\n",
    "    },\n",
    "    logger_meta_params=p.aim_logger.meta_params.to_dict(),\n",
    "    logger_tags=p.aim_logger.tags,\n",
    ")\n",
    "if \"in_channels\" not in p.encoder:\n",
    "    in_channels = int(train_dataset[0][\"lr_dwi\"].shape[0])\n",
    "else:\n",
    "    in_channels = p.encoder.in_channels\n",
    "\n",
    "# Wrap the entire training & validation loop in a try...except statement.\n",
    "try:\n",
    "    encoder = INREncoder(**{**p.encoder.to_dict(), **{\"in_channels\": in_channels}})\n",
    "    # decoder = ContRepDecoder(**decoder_kwargs)\n",
    "    decoder = ReducedDecoder(**p.decoder.to_dict())\n",
    "    recon_decoder = INREncoder(\n",
    "        in_channels=encoder.out_channels,\n",
    "        interior_channels=48,\n",
    "        out_channels=6,\n",
    "        n_res_units=2,\n",
    "        n_dense_units=2,\n",
    "        activate_fn=p.encoder.activate_fn,\n",
    "    )\n",
    "\n",
    "    fabric.print(encoder)\n",
    "    fabric.print(decoder)\n",
    "    fabric.print(recon_decoder)\n",
    "\n",
    "    optim_encoder = torch.optim.AdamW(\n",
    "        encoder.parameters(), **p.train.optim.encoder.to_dict()\n",
    "    )\n",
    "    encoder, optim_encoder = fabric.setup(encoder, optim_encoder)\n",
    "    optim_decoder = torch.optim.AdamW(\n",
    "        decoder.parameters(), **p.train.optim.decoder.to_dict()\n",
    "    )\n",
    "    decoder, optim_decoder = fabric.setup(decoder, optim_decoder)\n",
    "    optim_recon_decoder = torch.optim.AdamW(\n",
    "        recon_decoder.parameters(), **p.train.optim.recon_decoder.to_dict()\n",
    "    )\n",
    "    recon_decoder, optim_recon_decoder = fabric.setup(\n",
    "        recon_decoder, optim_recon_decoder\n",
    "    )\n",
    "    loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
    "    recon_loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "    train_dataloader = monai.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=p.train.batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        **p.train.dataloader.to_dict(),\n",
    "    )\n",
    "    val_dataloader = monai.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    train_dataloader, val_dataloader = fabric.setup_dataloaders(\n",
    "        train_dataloader, val_dataloader\n",
    "    )\n",
    "    val_viz_subj_id = None\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    recon_decoder.train()\n",
    "    # output_dir = Path(output_dir)\n",
    "    losses = dict(\n",
    "        loss=list(),\n",
    "        epoch=list(),\n",
    "        step=list(),\n",
    "        encoder_grad_norm=list(),\n",
    "        decoder_grad_norm=list(),\n",
    "        recon_decoder_grad_norm=list(),\n",
    "    )\n",
    "    step = 0\n",
    "    train_dwi_recon_epoch_proportion = p.train.dwi_recon_epoch_proportion\n",
    "    train_recon = False\n",
    "    epochs = p.train.max_epochs\n",
    "    for epoch in range(epochs):\n",
    "        fabric.print(f\"\\nEpoch {epoch}\\n\", \"=\" * 10)\n",
    "        if epoch <= math.floor(epochs * train_dwi_recon_epoch_proportion):\n",
    "            if not train_recon:\n",
    "                train_recon = True\n",
    "        else:\n",
    "            train_recon = False\n",
    "\n",
    "        for batch_dict in train_dataloader:\n",
    "\n",
    "            x = batch_dict[\"lr_dwi\"]\n",
    "            x_coords = batch_dict[\"lr_patch_extent_acpc\"]\n",
    "            x_vox_size = torch.atleast_2d(batch_dict[\"lr_vox_size\"])\n",
    "            x_mask = batch_dict[\"lr_mask\"].to(torch.bool)\n",
    "\n",
    "            y = batch_dict[\"fodf\"]\n",
    "            y_mask = batch_dict[\"mask\"].to(torch.bool)\n",
    "            y_coords = batch_dict[\"fr_patch_extent_acpc\"]\n",
    "            y_vox_size = torch.atleast_2d(batch_dict[\"vox_size\"])\n",
    "\n",
    "            optim_encoder.zero_grad()\n",
    "            optim_decoder.zero_grad()\n",
    "            optim_recon_decoder.zero_grad()\n",
    "\n",
    "            ctx_v = encoder(x)\n",
    "\n",
    "            if not train_recon:\n",
    "                y_mask_broad = torch.broadcast_to(y_mask, y.shape)\n",
    "                pred_fodf = decoder(\n",
    "                    context_v=ctx_v,\n",
    "                    context_spatial_extent=x_coords,\n",
    "                    query_vox_size=y_vox_size,\n",
    "                    query_coord=y_coords,\n",
    "                )\n",
    "                loss_fodf = loss_fn(pred_fodf[y_mask_broad], y[y_mask_broad])\n",
    "                loss_recon = y.new_zeros(1)\n",
    "                recon_pred = None\n",
    "            else:\n",
    "                recon_pred = recon_decoder(ctx_v)\n",
    "                # Index bvals to be 2 b=0s, 2 b=1000s, and 2 b=3000s.\n",
    "                recon_y = x[:, (0, 1, 2, 21, 22, 23)]\n",
    "                x_mask_broad = torch.broadcast_to(x_mask, recon_y.shape)\n",
    "                loss_recon = recon_loss_fn(\n",
    "                    recon_pred[x_mask_broad], recon_y[x_mask_broad]\n",
    "                )\n",
    "                loss_fodf = recon_y.new_zeros(1)\n",
    "                pred_fodf = None\n",
    "\n",
    "            loss = loss_fodf + loss_recon\n",
    "\n",
    "            fabric.backward(loss)\n",
    "            for model in (encoder, decoder, recon_decoder):\n",
    "                if train_recon and model is decoder:\n",
    "                    continue\n",
    "                elif not train_recon and model is recon_decoder:\n",
    "                    continue\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(),\n",
    "                    5.0,\n",
    "                    error_if_nonfinite=True,\n",
    "                )\n",
    "            optim_encoder.step()\n",
    "            optim_decoder.step()\n",
    "            optim_recon_decoder.step()\n",
    "\n",
    "            encoder_grad_norm = calc_grad_norm(encoder)\n",
    "            recon_decoder_grad_norm = (\n",
    "                calc_grad_norm(recon_decoder) if train_recon else 0\n",
    "            )\n",
    "            decoder_grad_norm = calc_grad_norm(decoder) if not train_recon else 0\n",
    "            to_track = {\n",
    "                \"loss\": loss.detach().cpu().item(),\n",
    "                \"grad_norm_encoder\": encoder_grad_norm,\n",
    "            }\n",
    "            # Depending on whether or not the reconstruction decoder is training,\n",
    "            # select which metrics to track at this time.\n",
    "            if train_recon:\n",
    "                to_track = {\n",
    "                    **to_track,\n",
    "                    **{\n",
    "                        \"loss_recon\": loss_recon.detach().cpu().item(),\n",
    "                        \"grad_norm_recon_decoder\": recon_decoder_grad_norm,\n",
    "                    },\n",
    "                }\n",
    "            else:\n",
    "                to_track = {\n",
    "                    **to_track,\n",
    "                    **{\n",
    "                        \"loss_pred_fodf\": loss_fodf.detach().cpu().item(),\n",
    "                        \"grad_norm_decoder\": decoder_grad_norm,\n",
    "                    },\n",
    "                }\n",
    "            aim_run.track(\n",
    "                to_track,\n",
    "                context={\n",
    "                    \"subset\": \"train\",\n",
    "                },\n",
    "                step=step,\n",
    "                epoch=epoch,\n",
    "            )\n",
    "            fabric.print(\n",
    "                f\"| {loss.detach().cpu().item()}\",\n",
    "                end=\" \",\n",
    "                flush=(step % 10) == 0,\n",
    "            )\n",
    "            losses[\"loss\"].append(loss.detach().cpu().item())\n",
    "            losses[\"epoch\"].append(epoch)\n",
    "            losses[\"step\"].append(step)\n",
    "            losses[\"encoder_grad_norm\"].append(encoder_grad_norm)\n",
    "            losses[\"recon_decoder_grad_norm\"].append(recon_decoder_grad_norm)\n",
    "            losses[\"decoder_grad_norm\"].append(decoder_grad_norm)\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        optim_encoder.zero_grad(set_to_none=True)\n",
    "        optim_decoder.zero_grad(set_to_none=True)\n",
    "        optim_recon_decoder.zero_grad(set_to_none=True)\n",
    "        # Delete some training inputs to relax memory constraints in whole-\n",
    "        # volume inference inside validation step.\n",
    "        del x, x_coords, y, y_coords, pred_fodf, recon_pred\n",
    "\n",
    "        fabric.print(\"\\n==Validation==\", flush=True)\n",
    "        aim_run, val_viz_subj_id = validate_stage(\n",
    "            fabric,\n",
    "            encoder,\n",
    "            decoder,\n",
    "            val_dataloader=val_dataloader,\n",
    "            step=step,\n",
    "            epoch=epoch,\n",
    "            aim_run=aim_run,\n",
    "            val_viz_subj_id=val_viz_subj_id,\n",
    "        )\n",
    "\n",
    "except KeyboardInterrupt as e:\n",
    "    aim_run.add_tag(\"STOPPED\")\n",
    "    (tmp_res_dir / \"STOPPED\").touch()\n",
    "    raise e\n",
    "except Exception as e:\n",
    "    aim_run.add_tag(\"FAILED\")\n",
    "    (tmp_res_dir / \"FAILED\").touch()\n",
    "    raise e\n",
    "finally:\n",
    "    aim_run.close()\n",
    "\n",
    "# Sync all pytorch-lightning processes.\n",
    "fabric.barrier()\n",
    "if fabric.is_global_zero:\n",
    "    torch.save(\n",
    "        {\n",
    "            \"encoder\": encoder.state_dict(),\n",
    "            \"decoder\": decoder.state_dict(),\n",
    "            \"recon_decoder\": recon_decoder.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"step\": step,\n",
    "            \"aim_run_hash\": aim_run.hash,\n",
    "            \"optim_encoder\": optim_encoder.state_dict(),\n",
    "            \"optim_decoder\": optim_decoder.state_dict(),\n",
    "            \"optim_recon_decoder\": optim_recon_decoder.state_dict(),\n",
    "        },\n",
    "        Path(tmp_res_dir) / f\"state_dict_epoch_{epoch}_step_{step}.pt\",\n",
    "    )\n",
    "    fabric.print(\"=\" * 40)\n",
    "    losses = pd.DataFrame.from_dict(losses)\n",
    "    losses.to_csv(Path(tmp_res_dir) / \"train_losses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b2332d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "comment_magics": true,
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "pitn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a27b66b33b39a2ceaf791f3e24555d5a3d820ed1de6ee5d2f5c031617fe18d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
