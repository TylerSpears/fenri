{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous-Space Super-Resolution of fODFs in Diffusion MRI\n",
    "\n",
    "Code by:\n",
    "\n",
    "Tyler Spears - tas6hh@virginia.edu\n",
    "\n",
    "Dr. Tom Fletcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Automatically re-import project-specific modules.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# imports\n",
    "import collections\n",
    "import functools\n",
    "import io\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "import copy\n",
    "import pdb\n",
    "import inspect\n",
    "import random\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import typing\n",
    "import zipfile\n",
    "\n",
    "import dotenv\n",
    "\n",
    "# visualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data management libraries.\n",
    "import nibabel as nib\n",
    "import nibabel.processing\n",
    "from natsort import natsorted\n",
    "from pprint import pprint as ppr\n",
    "from box import Box\n",
    "\n",
    "# Computation & ML libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchio\n",
    "from pytorch_lightning.lite import LightningLite\n",
    "import monai\n",
    "import einops\n",
    "import torchinfo\n",
    "import skimage\n",
    "\n",
    "import pitn\n",
    "\n",
    "plt.rcParams.update({\"figure.autolayout\": True})\n",
    "plt.rcParams.update({\"figure.facecolor\": [1.0, 1.0, 1.0, 1.0]})\n",
    "\n",
    "# Set print options for ndarrays/tensors.\n",
    "np.set_printoptions(suppress=True, threshold=100, linewidth=88)\n",
    "torch.set_printoptions(sci_mode=False, threshold=100, linewidth=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "direnv: loading ~/Projects/pitn/.envrc\n"
     ]
    }
   ],
   "source": [
    "# Update notebook's environment variables with direnv.\n",
    "# This requires the python-dotenv package, and direnv be installed on the system\n",
    "# This will not work on Windows.\n",
    "# NOTE: This is kind of hacky, and not necessarily safe. Be careful...\n",
    "# Libraries needed on the python side:\n",
    "# - os\n",
    "# - subprocess\n",
    "# - io\n",
    "# - dotenv\n",
    "\n",
    "# Form command to be run in direnv's context. This command will print out\n",
    "# all environment variables defined in the subprocess/sub-shell.\n",
    "command = \"direnv exec {} /usr/bin/env\".format(os.getcwd())\n",
    "# Run command in a new subprocess.\n",
    "proc = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, cwd=os.getcwd())\n",
    "# Store and format the subprocess' output.\n",
    "proc_out = proc.communicate()[0].strip().decode(\"utf-8\")\n",
    "# Use python-dotenv to load the environment variables by using the output of\n",
    "# 'direnv exec ...' as a 'dummy' .env file.\n",
    "dotenv.load_dotenv(stream=io.StringIO(proc_out), override=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Device IDX  0\n",
      "CUDA Current Device  0\n",
      "CUDA Device properties:  _CudaDeviceProperties(name='NVIDIA RTX A5000', major=8, minor=6, total_memory=24256MB, multi_processor_count=64)\n",
      "CuDNN convolution optimization enabled.\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# torch setup\n",
    "# allow for CUDA usage, if available\n",
    "if torch.cuda.is_available():\n",
    "    # Pick only one device for the default, may use multiple GPUs for training later.\n",
    "    if \"CUDA_PYTORCH_DEVICE_IDX\" in os.environ.keys():\n",
    "        dev_idx = int(os.environ[\"CUDA_PYTORCH_DEVICE_IDX\"])\n",
    "    else:\n",
    "        dev_idx = 0\n",
    "    device = torch.device(f\"cuda:{dev_idx}\")\n",
    "    print(\"CUDA Device IDX \", dev_idx)\n",
    "    torch.cuda.set_device(device)\n",
    "    print(\"CUDA Current Device \", torch.cuda.current_device())\n",
    "    print(\"CUDA Device properties: \", torch.cuda.get_device_properties(device))\n",
    "    # Activate cudnn benchmarking to optimize convolution algorithm speed.\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        print(\"CuDNN convolution optimization enabled.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# keep device as the cpu\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr cap\n",
    "# Capture output and save to log. Needs to be at the *very first* line of the cell.\n",
    "# Watermark\n",
    "%load_ext watermark\n",
    "%watermark --author \"Tyler Spears\" --updated --iso8601  --python --machine --iversions --githash\n",
    "if torch.cuda.is_available():\n",
    "    # GPU information\n",
    "    try:\n",
    "        gpu_info = pitn.utils.system.get_gpu_specs()\n",
    "        print(gpu_info)\n",
    "    except NameError:\n",
    "        print(\"CUDA Version: \", torch.version.cuda)\n",
    "else:\n",
    "    print(\"CUDA not in use, falling back to CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Tyler Spears\n",
      "\n",
      "Last updated: 2022-10-19T12:35:58.600728-04:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.5\n",
      "IPython version      : 8.4.0\n",
      "\n",
      "Compiler    : GCC 10.3.0\n",
      "OS          : Linux\n",
      "Release     : 5.15.0-48-generic\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 20\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: ca371418a8928715f8ac72bc33fb228b6affa696\n",
      "\n",
      "torchio   : 0.18.84\n",
      "einops    : 0.4.1\n",
      "seaborn   : 0.11.2\n",
      "skimage   : 0.19.3\n",
      "numpy     : 1.23.1\n",
      "sys       : 3.10.5 | packaged by conda-forge | (main, Jun 14 2022, 07:04:59) [GCC 10.3.0]\n",
      "nibabel   : 4.0.1\n",
      "monai     : 1.0.0\n",
      "matplotlib: 3.5.2\n",
      "pandas    : 1.4.3\n",
      "torch     : 1.12.1\n",
      "pitn      : 0.0.post1.dev206+gf002231.d20220911\n",
      "torchinfo : 1.7.1\n",
      "\n",
      "==================================================GPU Specs==================================================\n",
      "  id  Name              Driver Version      CUDA Version  Total Memory    uuid\n",
      "----  ----------------  ----------------  --------------  --------------  ----------------------------------------\n",
      "   0  NVIDIA RTX A5000  515.65.01                   11.6  24564.0MB       GPU-ed20d87f-e88e-692f-0b56-548b8a05ddea\n",
      "   1  NVIDIA RTX A5000  515.65.01                   11.6  24564.0MB       GPU-0636ee40-2eab-9533-1be7-dbbadade95c4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cap is defined in an ipython magic command\n",
    "print(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment & Parameters Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Config file not loaded\n"
     ]
    }
   ],
   "source": [
    "p = Box(default_box=True)\n",
    "# Experiment defaults, can be overridden in a config file.\n",
    "\n",
    "# General experiment-wide params\n",
    "###############################################\n",
    "p.experiment_name = \"sr_debug\"\n",
    "p.override_experiment_name = False\n",
    "p.results_dir = \"/data/srv/outputs/pitn/results/runs\"\n",
    "p.tmp_results_dir = \"/data/srv/outputs/pitn/results/tmp\"\n",
    "p.train_val_test_split_file = random.choice(\n",
    "    list(Path(\"./data_splits\").glob(\"HCP*train-val-test_split*.csv\"))\n",
    ")\n",
    "###############################################\n",
    "p.train.in_patch_size = (32, 32, 32)\n",
    "p.train.batch_size = 1\n",
    "p.train.samples_per_subj_per_epoch = 50\n",
    "p.train.max_epochs = 50\n",
    "p.train.loss = \"mse\"\n",
    "\n",
    "# If a config file exists, override the defaults with those values.\n",
    "try:\n",
    "    if \"PITN_CONFIG\" in os.environ.keys():\n",
    "        config_fname = Path(os.environ[\"PITN_CONFIG\"])\n",
    "    else:\n",
    "        config_fname = pitn.utils.system.get_file_glob_unique(Path(\".\"), r\"config.*\")\n",
    "    f_type = config_fname.suffix.casefold()\n",
    "    if f_type in {\".yaml\", \".yml\"}:\n",
    "        f_params = Box.from_yaml(filename=config_fname)\n",
    "    elif f_type == \".json\":\n",
    "        f_params = Box.from_json(filename=config_fname)\n",
    "    elif f_type == \".toml\":\n",
    "        f_params = Box.from_toml(filename=config_fname)\n",
    "    else:\n",
    "        raise RuntimeError()\n",
    "\n",
    "    p.merge_update(f_params)\n",
    "\n",
    "except:\n",
    "    print(\"WARNING: Config file not loaded\")\n",
    "    pass\n",
    "\n",
    "# Remove the default_box behavior now that params have been fully read in.\n",
    "_p = Box(default_box=False)\n",
    "_p.merge_update(p)\n",
    "p = _p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvt_split = pd.read_csv(p.train_val_test_split_file)\n",
    "p.train.subj_ids = tvt_split[tvt_split.split == \"train\"].subj_id.tolist()\n",
    "p.val = dict()\n",
    "p.val.subj_ids = tvt_split[tvt_split.split == \"val\"].subj_id.tolist()\n",
    "p.test = dict()\n",
    "p.test.subj_ids = tvt_split[tvt_split.split == \"test\"].subj_id.tolist()\n",
    "\n",
    "# Ensure that no test subj ids are in either the training or validation sets.\n",
    "# However, we can have overlap between training and validation.\n",
    "assert len(set(p.train.subj_ids) & set(p.test.subj_ids)) == 0\n",
    "assert len(set(p.val.subj_ids) & set(p.test.subj_ids)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcp_full_res_data_dir = Path(\"/data/srv/data/pitn/hcp\")\n",
    "hcp_full_res_fodf_dir = Path(\"/data/srv/outputs/pitn/hcp/full-res/fodf\")\n",
    "hcp_low_res_data_dir = Path(\"/data/srv/outputs/pitn/hcp/downsample/scale-2.00mm/vol\")\n",
    "\n",
    "assert hcp_full_res_data_dir.exists()\n",
    "assert hcp_full_res_fodf_dir.exists()\n",
    "assert hcp_low_res_data_dir.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_tfs = list()\n",
    "# Load images\n",
    "vol_reader = monai.data.NibabelReader(as_closest_canonical=True, dtype=np.float32)\n",
    "load_tfs.append(\n",
    "    monai.transforms.LoadImaged(\n",
    "        (\"lr_dwi\", \"fodf\", \"lr_mask\", \"mask\", \"fivett\"),\n",
    "        reader=vol_reader,\n",
    "        dtype=np.float32,\n",
    "        meta_key_postfix=\"meta\",\n",
    "        ensure_channel_first=True,\n",
    "        simple_keys=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "grad_file_reader = monai.transforms.Lambdad(\n",
    "    (\"lr_bval\", \"lr_bvec\"), lambda f: np.loadtxt(str(f)), overwrite=True\n",
    ")\n",
    "load_tfs.append(grad_file_reader)\n",
    "\n",
    "# Data conversion\n",
    "load_tfs.append(\n",
    "    monai.transforms.ToTensord(\n",
    "        (\"lr_dwi\", \"fodf\", \"lr_mask\", \"mask\", \"fivett\"), track_meta=True\n",
    "    )\n",
    ")\n",
    "load_tfs.append(monai.transforms.ToTensord((\"lr_bval\", \"lr_bvec\"), track_meta=False))\n",
    "load_tfs.append(\n",
    "    monai.transforms.CastToTyped((\"lr_mask\", \"mask\", \"fivett\"), dtype=torch.uint8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sampling transformations.\n",
    "sample_tfs = list()\n",
    "\n",
    "# Dilate the lr mask by 1/2 patch shape to allow for uniform sampling of the patch\n",
    "# centers.\n",
    "dilate_tf = pitn.transforms.BinaryDilated(\n",
    "    [\"lr_mask\"],\n",
    "    footprint=skimage.morphology.ball(max(p.train.in_patch_size) // 4),\n",
    "    write_to_keys=[\"lr_sampling_mask\"],\n",
    ")\n",
    "rescale_tf = monai.transforms.Lambdad(\n",
    "    \"lr_sampling_mask\", lambda m: m / torch.sum(m, (1, 2, 3), keepdim=True)\n",
    ")\n",
    "sample_tfs.append(dilate_tf)\n",
    "sample_tfs.append(rescale_tf)\n",
    "\n",
    "# Randomly crop ROIs from the LR input and match to the same spatial coordinates from\n",
    "# the full-res fODFs.\n",
    "# Save the low-res affine as its own field.\n",
    "def extract_affine(d: dict, src_vol_key, write_key: str):\n",
    "    aff = d[src_vol_key].affine\n",
    "    d[write_key] = torch.clone(aff).to(torch.float32)\n",
    "    return d\n",
    "\n",
    "\n",
    "sample_tfs.append(\n",
    "    functools.partial(\n",
    "        extract_affine, src_vol_key=\"lr_dwi\", write_key=\"affine_lrvox2acpc\"\n",
    "    )\n",
    ")\n",
    "\n",
    "sample_crop_tf = monai.transforms.RandWeightedCropd(\n",
    "    [\"lr_dwi\", \"lr_mask\"],\n",
    "    \"lr_sampling_mask\",\n",
    "    spatial_size=p.train.in_patch_size,\n",
    "    num_samples=p.train.samples_per_subj_per_epoch,\n",
    "    # num_samples=2,\n",
    ")\n",
    "sample_tfs.append(sample_crop_tf)\n",
    "\n",
    "# print_tf = monai.transforms.Lambdad(\n",
    "#     [\"lr_dwi\", \"lr_mask\"], lambda x: print(type(x), x.shape), overwrite=False\n",
    "# )\n",
    "# sample_tfs.append(print_tf)\n",
    "\n",
    "# ~~Randomly shuffle the order of DWIs and the associated bvals/bvecs.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms for extracting features for the network.\n",
    "feat_tfs = list()\n",
    "\n",
    "\n",
    "def extract_lr_patch_info(lr_dwi, affine_lrvox2acpc, patch_size: tuple):\n",
    "    # Extract low-resolution input information\n",
    "    patch_center_lrvox = torch.clone(\n",
    "        lr_dwi.meta[\"crop_center\"].as_tensor().to(torch.int)\n",
    "    )\n",
    "    affine_lrvox2acpc = torch.as_tensor(affine_lrvox2acpc).to(torch.float32)\n",
    "    vox_extent = list()\n",
    "    for patch_dim_len, patch_center in zip(patch_size, patch_center_lrvox):\n",
    "        half_patch_start = math.ceil(patch_dim_len // 2)\n",
    "        half_patch_end = math.floor(patch_dim_len // 2)\n",
    "        vox_extent.append(\n",
    "            torch.arange(\n",
    "                patch_center - half_patch_start, patch_center + half_patch_end\n",
    "            ).to(patch_center_lrvox)\n",
    "        )\n",
    "    vox_extent = torch.stack(vox_extent, dim=-1)\n",
    "    patch_extent_lrvox = vox_extent\n",
    "    # Calculate acpc-space coordinates of the vox extent.\n",
    "    acpc_extent = (affine_lrvox2acpc[:3, :3] @ vox_extent.T.to(affine_lrvox2acpc)) + (\n",
    "        affine_lrvox2acpc[:3, 3:4]\n",
    "    )\n",
    "    acpc_extent = acpc_extent.T\n",
    "    lr_patch_extent_acpc = acpc_extent\n",
    "\n",
    "    return dict(\n",
    "        patch_center_lrvox=patch_center_lrvox,\n",
    "        lr_patch_extent_acpc=lr_patch_extent_acpc,\n",
    "        patch_extent_lrvox=patch_extent_lrvox,\n",
    "    )\n",
    "\n",
    "\n",
    "extract_lr_patch_meta_tf = monai.transforms.adaptor(\n",
    "    functools.partial(extract_lr_patch_info, patch_size=p.train.in_patch_size),\n",
    "    outputs={\n",
    "        \"patch_center_lrvox\": \"patch_center_lrvox\",\n",
    "        \"patch_extent_lrvox\": \"patch_extent_lrvox\",\n",
    "        \"lr_patch_extent_acpc\": \"lr_patch_extent_acpc\",\n",
    "    },\n",
    ")\n",
    "feat_tfs.append(extract_lr_patch_meta_tf)\n",
    "\n",
    "\n",
    "def extract_full_res_patch_info(\n",
    "    fodf,\n",
    "    lr_patch_extent_acpc,\n",
    "):\n",
    "    # Extract full-resolution information.\n",
    "    affine_vox2acpc = torch.clone(torch.as_tensor(fodf.affine, dtype=torch.float32))\n",
    "    affine_acpc2vox = torch.inverse(affine_vox2acpc)\n",
    "    lr_patch_extent_acpc = lr_patch_extent_acpc.to(affine_acpc2vox)\n",
    "    patch_extent_vox = (affine_acpc2vox[:3, :3] @ lr_patch_extent_acpc.T) + (\n",
    "        affine_acpc2vox[:3, 3:4]\n",
    "    )\n",
    "\n",
    "    patch_extent_vox = patch_extent_vox.T\n",
    "    l_bound = torch.floor(patch_extent_vox.min(dim=0).values).to(torch.int)\n",
    "    u_bound = torch.ceil(patch_extent_vox.max(dim=0).values).to(torch.int)\n",
    "    fr_patch_shape = u_bound - l_bound\n",
    "    if (fr_patch_shape != fr_patch_shape.max()).any():\n",
    "        target_size = fr_patch_shape.max()\n",
    "        vol_shape = torch.tensor(fodf.shape[1:])\n",
    "        for dim, dim_size in enumerate(fr_patch_shape):\n",
    "            if dim_size != target_size:\n",
    "                diff = target_size - dim_size\n",
    "                # Try to increase the upper bound first.\n",
    "                if u_bound[dim] + diff <= vol_shape[dim]:\n",
    "                    u_bound[dim] = u_bound[dim] + diff\n",
    "                elif l_bound[dim] - diff >= 0:\n",
    "                    l_bound[dim] = l_bound[dim] - diff\n",
    "                else:\n",
    "                    raise RuntimeError(\n",
    "                        \"ERROR: Non-isotropic full-res patch shape\", f\"{fr_patch_shape}\"\n",
    "                    )\n",
    "        fr_patch_shape = u_bound - l_bound\n",
    "\n",
    "    # Store the vox upper and lower bounds now for later patch extraction from the full-\n",
    "    # res fodf and mask (*much* faster to index into a tensor with a range() than each\n",
    "    # individual index).\n",
    "    fr_patch_vox_lu_bound = torch.stack([l_bound, u_bound], dim=-1)\n",
    "    extent_vox_l = list()\n",
    "    for l, u in zip(l_bound.cpu().tolist(), u_bound.cpu().tolist()):\n",
    "        extent_vox_l.append(torch.arange(l, u).to(torch.int))\n",
    "    patch_extent_vox = torch.stack(extent_vox_l, dim=-1).to(torch.int)\n",
    "\n",
    "    fr_patch_extent_acpc = (\n",
    "        affine_vox2acpc[:3, :3] @ patch_extent_vox.T.to(affine_vox2acpc)\n",
    "    ) + (affine_vox2acpc[:3, 3:4])\n",
    "    fr_patch_extent_acpc = fr_patch_extent_acpc.T\n",
    "    fr_patch_extent_acpc = fr_patch_extent_acpc\n",
    "\n",
    "    return dict(\n",
    "        affine_vox2acpc=affine_vox2acpc,\n",
    "        patch_extent_vox=patch_extent_vox,\n",
    "        fr_patch_extent_acpc=fr_patch_extent_acpc,\n",
    "    )\n",
    "\n",
    "\n",
    "extract_full_res_patch_meta_tf = monai.transforms.adaptor(\n",
    "    extract_full_res_patch_info,\n",
    "    outputs={\n",
    "        \"affine_vox2acpc\": \"affine_vox2acpc\",\n",
    "        \"fr_patch_vox_lu_bound\": \"fr_patch_vox_lu_bound\",\n",
    "        \"fr_patch_extent_acpc\": \"fr_patch_extent_acpc\",\n",
    "    },\n",
    ")\n",
    "feat_tfs.append(extract_full_res_patch_meta_tf)\n",
    "\n",
    "# Slice into fodf and full-res mask with the LR patch's spatial extent.\n",
    "class CropSamplefODFMask:\n",
    "    def __init__(self, fr_patch_vox_lu_bound_key, vol_key_map: dict):\n",
    "        self._lu_key = fr_patch_vox_lu_bound_key\n",
    "        self._vol_key_map = vol_key_map\n",
    "\n",
    "    def __call__(self, data_dict: dict):\n",
    "        # Find start and end of the ROI\n",
    "        lu_bound = data_dict[self._lu_key]\n",
    "        roi_start = lu_bound[:, 0]\n",
    "        roi_end = lu_bound[:, 1] + 1\n",
    "        # Crop vols with the SpatialCrop transform.\n",
    "        cropper = monai.transforms.SpatialCropd(\n",
    "            list(self._vol_key_map.keys()), roi_start=roi_start, roi_end=roi_end\n",
    "        )\n",
    "        to_crop = {v: data_dict[v] for v in self._vol_key_map.keys()}\n",
    "        cropped = cropper(to_crop)\n",
    "        # Store the cropped vols into the data dict with the (possibly) new keys.\n",
    "        for old_v in cropped.keys():\n",
    "            data_dict[self._vol_key_map[old_v]] = cropped[old_v]\n",
    "\n",
    "        return data_dict\n",
    "\n",
    "\n",
    "crop_fr_tf = CropSamplefODFMask(\n",
    "    \"fr_patch_vox_lu_bound\", {\"fodf\": \"fodf\", \"mask\": \"mask\"}\n",
    ")\n",
    "\n",
    "feat_tfs.append(crop_fr_tf)\n",
    "\n",
    "# Remove unnecessary items from the data dict.\n",
    "# Convert all MetaTensors to regular Tensors.\n",
    "to_tensor_tf = monai.transforms.ToTensord(\n",
    "    [\"lr_dwi\", \"lr_mask\", \"fodf\", \"mask\"], track_meta=False\n",
    ")\n",
    "feat_tfs.append(to_tensor_tf)\n",
    "# Sub-select keys to free memory.\n",
    "select_k_tf = monai.transforms.SelectItemsd(\n",
    "    [\n",
    "        \"subj_id\",\n",
    "        \"lr_dwi\",\n",
    "        \"lr_mask\",\n",
    "        \"lr_bval\",\n",
    "        \"lr_bvec\",\n",
    "        \"fodf\",\n",
    "        \"mask\",\n",
    "        \"lr_patch_extent_acpc\",\n",
    "        \"fr_patch_extent_acpc\",\n",
    "    ]\n",
    ")\n",
    "feat_tfs.append(select_k_tf)\n",
    "\n",
    "# to_cuda_tf = monai.transforms.ToDeviced(\n",
    "#     [\n",
    "#         \"lr_dwi\",\n",
    "#         \"lr_mask\",\n",
    "#         \"lr_bval\",\n",
    "#         \"lr_bvec\",\n",
    "#         \"fodf\",\n",
    "#         \"mask\",\n",
    "#         \"lr_patch_extent_acpc\",\n",
    "#         \"fr_patch_extent_acpc\",\n",
    "#     ],\n",
    "#     device=device,\n",
    "# )\n",
    "# feat_tfs.append(to_cuda_tf)\n",
    "\n",
    "# Generate the Cartesian product of the spatial coordinates of each \"training voxel\".\n",
    "vox_physical_coords_tf = monai.transforms.Lambdad(\n",
    "    [\n",
    "        \"lr_patch_extent_acpc\",\n",
    "        \"fr_patch_extent_acpc\",\n",
    "    ],\n",
    "    lambda c: einops.rearrange(\n",
    "        torch.cartesian_prod(*c.T),\n",
    "        \"(p1 p2 p3) d -> d p1 p2 p3\",\n",
    "        p1=c.shape[0],\n",
    "        p2=c.shape[0],\n",
    "        p3=c.shape[0],\n",
    "    ).to(torch.float32),\n",
    "    overwrite=True,\n",
    ")\n",
    "feat_tfs.append(vox_physical_coords_tf)\n",
    "\n",
    "# ~~Generate features from each DWI and the associated bval and bvec.~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Patch-Based Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 2/2 [00:10<00:00,  5.25s/it]\n"
     ]
    }
   ],
   "source": [
    "pre_patch_transforms = monai.transforms.Compose(load_tfs + sample_tfs[:-1])\n",
    "tf_patch_sampler = sample_tfs[-1]\n",
    "per_patch_transforms = monai.transforms.Compose(feat_tfs)\n",
    "\n",
    "uncached_dataset = pitn.data.datasets.HCPfODFINRDataset(\n",
    "    subj_ids=p.train.subj_ids,\n",
    "    dwi_root_dir=hcp_full_res_data_dir,\n",
    "    fodf_root_dir=hcp_full_res_fodf_dir,\n",
    "    lr_dwi_root_dir=hcp_low_res_data_dir,\n",
    "    transform=None,\n",
    ")\n",
    "\n",
    "with warnings.catch_warnings(record=True) as warn_list:\n",
    "    # dataset = monai.data.CacheDataset(\n",
    "    #     uncached_dataset, transform=pre_patch_transforms, copy_cache=False\n",
    "    # )\n",
    "    #!DEBUG\n",
    "    cache_dataset = monai.data.CacheDataset(\n",
    "        uncached_dataset[:2], transform=pre_patch_transforms, copy_cache=False\n",
    "    )\n",
    "train_dataset = monai.data.PatchDataset(\n",
    "    cache_dataset,\n",
    "    patch_func=tf_patch_sampler,\n",
    "    samples_per_image=p.train.samples_per_subj_per_epoch,\n",
    "    transform=per_patch_transforms,\n",
    ")\n",
    "print(\"=\" * 10)\n",
    "print(\"Warnings caught:\")\n",
    "[\n",
    "    warnings.showwarning(w.message, w.category, w.filename, w.lineno, w.file, w.line)\n",
    "    for w in warn_list\n",
    "]\n",
    "print(\"=\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation & Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding model\n",
    "class EncodeINR(torch.nn.Module):\n",
    "    # def __init__(self, in_channels, out_channels, )\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INR/Decoder model\n",
    "class ImplicitRepr(torch.nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class INRSystem(LightningLite):\n",
    "    def run(\n",
    "        self,\n",
    "        epochs: int,\n",
    "        batch_size: int,\n",
    "        in_channels: int,\n",
    "        pred_channels: int,\n",
    "        train_dataset,\n",
    "        stage=\"train\",\n",
    "        logger=None,\n",
    "    ):\n",
    "        encoder = pitn.nn.inr\n",
    "\n",
    "    def validate(self, model, val_dataset):\n",
    "        pass\n",
    "\n",
    "    def test(self, model, test_dataset):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f41faa2479836806c9664d670a156675ad0f09912fd4b0aed749f41e3cac86f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
