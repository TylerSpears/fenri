{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ea2a2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Continuous-Space Super-Resolution of fODFs in Diffusion MRI\n",
    "\n",
    "Code by:\n",
    "\n",
    "Tyler Spears - tas6hh@virginia.edu\n",
    "\n",
    "Dr. Tom Fletcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2b219f",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050754dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Automatically re-import project-specific modules.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# imports\n",
    "import collections\n",
    "import copy\n",
    "import datetime\n",
    "import functools\n",
    "import inspect\n",
    "import io\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import pdb\n",
    "import random\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "import typing\n",
    "import warnings\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from pprint import pprint as ppr\n",
    "\n",
    "import dotenv\n",
    "import aim\n",
    "import einops\n",
    "\n",
    "# visualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import monai\n",
    "\n",
    "# Data management libraries.\n",
    "import nibabel as nib\n",
    "import nibabel.processing\n",
    "\n",
    "# Computation & ML libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import skimage\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchinfo\n",
    "import torchio\n",
    "from box import Box\n",
    "from natsort import natsorted\n",
    "from pytorch_lightning.lite import LightningLite\n",
    "\n",
    "import pitn\n",
    "\n",
    "plt.rcParams.update({\"figure.autolayout\": True})\n",
    "plt.rcParams.update({\"figure.facecolor\": [1.0, 1.0, 1.0, 1.0]})\n",
    "plt.rcParams.update({\"image.cmap\": \"gray\"})\n",
    "\n",
    "# Set print options for ndarrays/tensors.\n",
    "np.set_printoptions(suppress=True, threshold=100, linewidth=88)\n",
    "torch.set_printoptions(sci_mode=False, threshold=100, linewidth=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bba4e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update notebook's environment variables with direnv.\n",
    "# This requires the python-dotenv package, and direnv be installed on the system\n",
    "# This will not work on Windows.\n",
    "# NOTE: This is kind of hacky, and not necessarily safe. Be careful...\n",
    "# Libraries needed on the python side:\n",
    "# - os\n",
    "# - subprocess\n",
    "# - io\n",
    "# - dotenv\n",
    "\n",
    "# Form command to be run in direnv's context. This command will print out\n",
    "# all environment variables defined in the subprocess/sub-shell.\n",
    "command = \"direnv exec {} /usr/bin/env\".format(os.getcwd())\n",
    "# Run command in a new subprocess.\n",
    "proc = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, cwd=os.getcwd())\n",
    "# Store and format the subprocess' output.\n",
    "proc_out = proc.communicate()[0].strip().decode(\"utf-8\")\n",
    "# Use python-dotenv to load the environment variables by using the output of\n",
    "# 'direnv exec ...' as a 'dummy' .env file.\n",
    "dotenv.load_dotenv(stream=io.StringIO(proc_out), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e08a24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch setup\n",
    "# allow for CUDA usage, if available\n",
    "if torch.cuda.is_available():\n",
    "    # Pick only one device for the default, may use multiple GPUs for training later.\n",
    "    if \"CUDA_PYTORCH_DEVICE_IDX\" in os.environ.keys():\n",
    "        dev_idx = int(os.environ[\"CUDA_PYTORCH_DEVICE_IDX\"])\n",
    "    else:\n",
    "        dev_idx = 0\n",
    "    device = torch.device(f\"cuda:{dev_idx}\")\n",
    "    print(\"CUDA Device IDX \", dev_idx)\n",
    "    torch.cuda.set_device(device)\n",
    "    print(\"CUDA Current Device \", torch.cuda.current_device())\n",
    "    print(\"CUDA Device properties: \", torch.cuda.get_device_properties(device))\n",
    "    # Activate cudnn benchmarking to optimize convolution algorithm speed.\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        print(\"CuDNN convolution optimization enabled.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# keep device as the cpu\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887f61ad",
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr cap\n",
    "# Capture output and save to log. Needs to be at the *very first* line of the cell.\n",
    "# Watermark\n",
    "%load_ext watermark\n",
    "%watermark --author \"Tyler Spears\" --updated --iso8601  --python --machine --iversions --githash\n",
    "if torch.cuda.is_available():\n",
    "    # GPU information\n",
    "    try:\n",
    "        gpu_info = pitn.utils.system.get_gpu_specs()\n",
    "        print(gpu_info)\n",
    "    except NameError:\n",
    "        print(\"CUDA Version: \", torch.version.cuda)\n",
    "else:\n",
    "    print(\"CUDA not in use, falling back to CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f8ab63",
   "metadata": {
    "tags": [
     "keep_output",
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "# cap is defined in an ipython magic command\n",
    "try:\n",
    "    print(cap)\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf082031",
   "metadata": {},
   "source": [
    "## Experiment & Parameters Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242e12aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = Box(default_box=True)\n",
    "# Experiment defaults, can be overridden in a config file.\n",
    "\n",
    "# General experiment-wide params\n",
    "###############################################\n",
    "p.experiment_name = \"sr_debug\"\n",
    "p.override_experiment_name = False\n",
    "p.results_dir = \"/data/srv/outputs/pitn/results/runs\"\n",
    "p.tmp_results_dir = \"/data/srv/outputs/pitn/results/tmp\"\n",
    "p.train_val_test_split_file = random.choice(\n",
    "    list(Path(\"./data_splits\").glob(\"HCP*train-val-test_split*.csv\"))\n",
    ")\n",
    "p.aim_logger = dict(\n",
    "    repo=\"aim://dali.cpe.virginia.edu:53800\",\n",
    "    experiment=\"PITN_INR\",\n",
    "    meta_params=dict(run_name=p.experiment_name),\n",
    "    tags=(\"PITN\", \"INR\", \"HCP\", \"super-res\", \"dMRI\"),\n",
    ")\n",
    "###############################################\n",
    "p.train = dict(\n",
    "    in_patch_size=(32, 32, 32),\n",
    "    batch_size=3,\n",
    "    samples_per_subj_per_epoch=30,\n",
    "    max_epochs=80,\n",
    "    # loss=\"mse\",\n",
    ")\n",
    "\n",
    "# Network/model parameters.\n",
    "p.encoder = dict(\n",
    "    interior_channels=80,\n",
    "    # (number of SH orders (l) + 1) * X that is as close to 100 as possible.\n",
    "    out_channels=32,\n",
    "    # out_channels=189,\n",
    "    n_res_units=3,\n",
    "    n_dense_units=3,\n",
    "    activate_fn=\"relu\",\n",
    ")\n",
    "p.decoder = dict(\n",
    "    context_v_features=28,\n",
    "    in_features=p.encoder.out_channels,\n",
    "    out_features=45,\n",
    "    m_encode_num_freqs=24,\n",
    "    sigma_encode_scale=3.0,\n",
    ")\n",
    "\n",
    "\n",
    "# If a config file exists, override the defaults with those values.\n",
    "try:\n",
    "    if \"PITN_CONFIG\" in os.environ.keys():\n",
    "        config_fname = Path(os.environ[\"PITN_CONFIG\"])\n",
    "    else:\n",
    "        config_fname = pitn.utils.system.get_file_glob_unique(Path(\".\"), r\"config.*\")\n",
    "    f_type = config_fname.suffix.casefold()\n",
    "    if f_type in {\".yaml\", \".yml\"}:\n",
    "        f_params = Box.from_yaml(filename=config_fname)\n",
    "    elif f_type == \".json\":\n",
    "        f_params = Box.from_json(filename=config_fname)\n",
    "    elif f_type == \".toml\":\n",
    "        f_params = Box.from_toml(filename=config_fname)\n",
    "    else:\n",
    "        raise RuntimeError()\n",
    "\n",
    "    p.merge_update(f_params)\n",
    "\n",
    "except:\n",
    "    print(\"WARNING: Config file not loaded\")\n",
    "    pass\n",
    "\n",
    "# Remove the default_box behavior now that params have been fully read in.\n",
    "_p = Box(default_box=False)\n",
    "_p.merge_update(p)\n",
    "p = _p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7044cdbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tvt_split = pd.read_csv(p.train_val_test_split_file)\n",
    "p.train.subj_ids = natsorted(tvt_split[tvt_split.split == \"train\"].subj_id.tolist())\n",
    "p.val = dict()\n",
    "p.val.subj_ids = natsorted(tvt_split[tvt_split.split == \"val\"].subj_id.tolist())\n",
    "p.test = dict()\n",
    "p.test.subj_ids = natsorted(tvt_split[tvt_split.split == \"test\"].subj_id.tolist())\n",
    "\n",
    "# Ensure that no test subj ids are in either the training or validation sets.\n",
    "# However, we can have overlap between training and validation.\n",
    "assert len(set(p.train.subj_ids) & set(p.test.subj_ids)) == 0\n",
    "assert len(set(p.val.subj_ids) & set(p.test.subj_ids)) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad71651d-b87b-4b84-b492-cb8d29f28660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ppr(p.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae12f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which parameters to store in the aim meta-params.\n",
    "p.aim_logger.meta_params.hparams = dict(\n",
    "    batch_size=p.train.batch_size,\n",
    "    patch_size=p.train.in_patch_size,\n",
    "    samples_per_subj_per_epoch=p.train.samples_per_subj_per_epoch,\n",
    "    max_epochs=p.train.max_epochs,\n",
    ")\n",
    "p.aim_logger.meta_params.data = dict(\n",
    "    train_subj_ids=p.train.subj_ids,\n",
    "    val_subj_ids=p.val.subj_ids,\n",
    "    test_subj_ids=p.test.subj_ids,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0d366b",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4a7461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hcp_full_res_data_dir = Path(\"/data/srv/data/pitn/hcp\")\n",
    "hcp_full_res_fodf_dir = Path(\"/data/srv/outputs/pitn/hcp/full-res/fodf\")\n",
    "hcp_low_res_data_dir = Path(\"/data/srv/outputs/pitn/hcp/downsample/scale-2.00mm/vol\")\n",
    "hcp_low_res_fodf_dir = Path(\"/data/srv/outputs/pitn/hcp/downsample/scale-2.00mm/fodf\")\n",
    "\n",
    "assert hcp_full_res_data_dir.exists()\n",
    "assert hcp_full_res_fodf_dir.exists()\n",
    "assert hcp_low_res_data_dir.exists()\n",
    "assert hcp_low_res_fodf_dir.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4f9741",
   "metadata": {},
   "source": [
    "### Create Patch-Based Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04af172b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEBUG_TRAIN_DATA_SUBJS = 15\n",
    "with warnings.catch_warnings(record=True) as warn_list:\n",
    "    # pre_sample_ds = pitn.data.datasets.HCPfODFINRDataset(\n",
    "    #     subj_ids=p.train.subj_ids,\n",
    "    #     dwi_root_dir=hcp_full_res_data_dir,\n",
    "    #     fodf_root_dir=hcp_full_res_fodf_dir,\n",
    "    #     lr_dwi_root_dir=hcp_low_res_data_dir,\n",
    "    #     lr_fodf_root_dir=hcp_low_res_fodf_dir,\n",
    "    #     transform=None,\n",
    "    # )\n",
    "\n",
    "    #!DEBUG\n",
    "    pre_sample_ds = pitn.data.datasets.HCPfODFINRDataset(\n",
    "        subj_ids=p.train.subj_ids[:DEBUG_TRAIN_DATA_SUBJS],\n",
    "        dwi_root_dir=hcp_full_res_data_dir,\n",
    "        fodf_root_dir=hcp_full_res_fodf_dir,\n",
    "        lr_dwi_root_dir=hcp_low_res_data_dir,\n",
    "        lr_fodf_root_dir=hcp_low_res_fodf_dir,\n",
    "        transform=None,\n",
    "    )\n",
    "    #!\n",
    "\n",
    "    pre_sample_train_dataset = monai.data.CacheDataset(\n",
    "        pre_sample_ds,\n",
    "        transform=pre_sample_ds.default_pre_sample_tf(\n",
    "            # Dilate by half the radius of one patch size.\n",
    "            mask_dilate_radius=max(p.train.in_patch_size)\n",
    "            // 4\n",
    "        ),\n",
    "        copy_cache=False,\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "train_dataset = pitn.data.datasets.HCPINRfODFPatchDataset(\n",
    "    pre_sample_train_dataset,\n",
    "    patch_func=pitn.data.datasets.HCPINRfODFPatchDataset.default_patch_func(\n",
    "        spatial_size=p.train.in_patch_size,\n",
    "        num_samples=p.train.samples_per_subj_per_epoch,\n",
    "    ),\n",
    "    samples_per_image=p.train.samples_per_subj_per_epoch,\n",
    "    transform=pitn.data.datasets.HCPINRfODFPatchDataset.default_feature_tf(\n",
    "        p.train.in_patch_size\n",
    "    ),\n",
    ")\n",
    "# train_dataset = monai.data.PatchDataset(\n",
    "#     cache_dataset,\n",
    "#     patch_func=tf_patch_sampler,\n",
    "#     samples_per_image=p.train.samples_per_subj_per_epoch,\n",
    "#     transform=per_patch_transforms,\n",
    "# )\n",
    "print(\"=\" * 10)\n",
    "print(\"Warnings caught:\")\n",
    "ws = \"\\n\".join(\n",
    "    [\n",
    "        warnings.formatwarning(\n",
    "            w.message, w.category, w.filename, w.lineno, w.file, w.line\n",
    "        )\n",
    "        for w in warn_list\n",
    "    ]\n",
    ")\n",
    "ws = \"\\n\".join(filter(lambda s: bool(s.strip()), ws.splitlines()))\n",
    "print(ws, flush=True)\n",
    "print(\"=\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea571a3",
   "metadata": {},
   "source": [
    "### Validation & Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d2f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "869bdf8e",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe8c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding model\n",
    "class INREncoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        interior_channels: int,\n",
    "        out_channels: int,\n",
    "        n_res_units: int,\n",
    "        n_dense_units: int,\n",
    "        activate_fn,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.interior_channels = interior_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        if isinstance(activate_fn, str):\n",
    "            activate_fn = pitn.utils.torch_lookups.activation[activate_fn]\n",
    "\n",
    "        # Pad to maintain the same input shape.\n",
    "        self.pre_conv = torch.nn.Conv3d(\n",
    "            self.in_channels,\n",
    "            self.interior_channels,\n",
    "            kernel_size=3,\n",
    "            padding=\"same\",\n",
    "            padding_mode=\"reflect\",\n",
    "        )\n",
    "\n",
    "        # Construct the densely-connected cascading layers.\n",
    "        # Create n_dense_units number of dense units.\n",
    "        top_level_units = list()\n",
    "        for _ in range(n_dense_units):\n",
    "            # Create n_res_units number of residual units for every dense unit.\n",
    "            res_layers = list()\n",
    "            for _ in range(n_res_units):\n",
    "                res_layers.append(\n",
    "                    pitn.nn.layers.ResBlock3dNoBN(\n",
    "                        self.interior_channels,\n",
    "                        kernel_size=3,\n",
    "                        activate_fn=activate_fn,\n",
    "                        padding=\"same\",\n",
    "                        padding_mode=\"reflect\",\n",
    "                    )\n",
    "                )\n",
    "            top_level_units.append(\n",
    "                pitn.nn.layers.DenseCascadeBlock3d(self.interior_channels, *res_layers)\n",
    "            )\n",
    "        self._activation_fn_init = activate_fn\n",
    "        self.activate_fn = activate_fn()\n",
    "\n",
    "        # Wrap everything into a densely-connected cascade.\n",
    "        self.cascade = pitn.nn.layers.DenseCascadeBlock3d(\n",
    "            self.interior_channels, *top_level_units\n",
    "        )\n",
    "\n",
    "        self.post_conv = torch.nn.Conv3d(\n",
    "            self.interior_channels,\n",
    "            self.out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=\"same\",\n",
    "            padding_mode=\"reflect\",\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        y = self.pre_conv(x)\n",
    "        y = self.activate_fn(y)\n",
    "        y = self.cascade(y)\n",
    "        y = self.activate_fn(y)\n",
    "        y = self.post_conv(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8030e179-79d0-40b5-8b6a-a8bf92aa9243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyINRDecoder(torch.nn.Module):\n",
    "    # Encoding model\n",
    "    def __init__(self, in_channels: int, out_channels: int, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.conv = torch.nn.Conv3d(self.in_channels, self.out_channels, 1, bias=False)\n",
    "        for param in self.conv.parameters():\n",
    "            param.requires_grad = False\n",
    "            torch.nn.init.dirac_(param)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c516c22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# INR/Decoder model\n",
    "class ContRepDecoder(torch.nn.Module):\n",
    "\n",
    "    TARGET_COORD_EPSILON = 1e-7\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        context_v_features: int,\n",
    "        out_features: int,\n",
    "        m_encode_num_freqs: int,\n",
    "        sigma_encode_scale: float,\n",
    "        in_features=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Determine the number of input features needed for the MLP.\n",
    "        # The order for concatenation is\n",
    "        # 1) ctx feats over the low-res input space\n",
    "        # 2) target voxel shape\n",
    "        # 3) absolute coords of this forward pass' prediction target\n",
    "        # 4) absolute coords of the high-res target voxel\n",
    "        # 5) relative coords between high-res target coords and this forward pass'\n",
    "        #    prediction target, normalized by low-res voxel shape\n",
    "        # 6) encoding of relative coords\n",
    "        self.context_v_features = context_v_features\n",
    "        self.ndim = 3\n",
    "        self.m_encode_num_freqs = m_encode_num_freqs\n",
    "        self.sigma_encode_scale = torch.as_tensor(sigma_encode_scale)\n",
    "        self.n_encode_features = self.ndim * 2 * self.m_encode_num_freqs\n",
    "        self.n_coord_features = 4 * self.ndim + self.n_encode_features\n",
    "        self.internal_features = self.context_v_features + self.n_coord_features\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # \"Swish\" function, recommended in MeshFreeFlowNet\n",
    "        activate_cls = torch.nn.SiLU\n",
    "        self.activate_fn = activate_cls(inplace=True)\n",
    "        # Optional resizing linear layer, if the input size should be different than\n",
    "        # the hidden layer size.\n",
    "        if self.in_features is not None:\n",
    "            self.lin_pre = torch.nn.Linear(self.in_features, self.context_v_features)\n",
    "            self.norm_pre = None\n",
    "            # self.norm_pre = torch.nn.LazyBatchNorm1d(affine=True, track_running_stats=True)\n",
    "            # self.norm_pre = torch.nn.LazyInstanceNorm3d(affine=False, track_running_stats=False)\n",
    "        else:\n",
    "            self.lin_pre = None\n",
    "            self.norm_pre = None\n",
    "        self.norm_pre = None\n",
    "\n",
    "        # Internal hidden layers are two res MLPs.\n",
    "        self.internal_res_repr = torch.nn.ModuleList(\n",
    "            [\n",
    "                pitn.nn.inr.SkipMLPBlock(\n",
    "                    n_context_features=self.context_v_features,\n",
    "                    n_coord_features=self.n_coord_features,\n",
    "                    n_dense_layers=3,\n",
    "                    activate_fn=activate_cls,\n",
    "                )\n",
    "                for _ in range(2)\n",
    "            ]\n",
    "        )\n",
    "        self.lin_post = torch.nn.Linear(self.context_v_features, self.out_features)\n",
    "\n",
    "    def encode_relative_coord(self, coords):\n",
    "        c = einops.rearrange(coords, \"b d x y z -> (b x y z) d\")\n",
    "        sigma = self.sigma_encode_scale.expand_as(c).to(c)[..., None]\n",
    "        encode_pos = pitn.nn.inr.fourier_position_encoding(\n",
    "            c, sigma_scale=sigma, m_num_freqs=self.m_encode_num_freqs\n",
    "        )\n",
    "\n",
    "        encode_pos = einops.rearrange(\n",
    "            encode_pos,\n",
    "            \"(b x y z) d -> b d x y z\",\n",
    "            x=coords.shape[2],\n",
    "            y=coords.shape[3],\n",
    "            z=coords.shape[4],\n",
    "        )\n",
    "        return encode_pos\n",
    "\n",
    "    def sub_grid_forward(\n",
    "        self,\n",
    "        context_val,\n",
    "        context_coord,\n",
    "        query_coord,\n",
    "        context_vox_size,\n",
    "        query_vox_size,\n",
    "        return_rel_context_coord=False,\n",
    "    ):\n",
    "        # Take relative coordinate difference between the current context\n",
    "        # coord and the query coord.\n",
    "        rel_context_coord = torch.clamp_min(\n",
    "            context_coord - query_coord,\n",
    "            (-context_vox_size / 2) + self.TARGET_COORD_EPSILON,\n",
    "        )\n",
    "        # Also normalize to [0, 1)\n",
    "        # Coordinates are located in the center of the voxel. By the way\n",
    "        # the context vector is being constructed surrounding the query\n",
    "        # coord, the query coord is always within 1.5 x vox_size of the\n",
    "        # context (low-res space) coordinate. So, subtract the\n",
    "        # batch-and-channel-wise minimum, and divide by the known upper\n",
    "        # bound.\n",
    "        rel_norm_context_coord = (\n",
    "            rel_context_coord\n",
    "            - torch.amin(rel_context_coord, dim=(2, 3, 4), keepdim=True)\n",
    "        ) / (1.5 * context_vox_size)\n",
    "        assert (rel_norm_context_coord >= 0).all() and (\n",
    "            rel_norm_context_coord < 1.0\n",
    "        ).all()\n",
    "        encoded_rel_norm_context_coord = self.encode_relative_coord(\n",
    "            rel_norm_context_coord\n",
    "        )\n",
    "        q_vox_size = query_vox_size.expand_as(rel_norm_context_coord)\n",
    "\n",
    "        # Perform forward pass of the MLP.\n",
    "        if self.norm_pre is not None:\n",
    "            context_val = self.norm_pre(context_val)\n",
    "        context_feats = einops.rearrange(context_val, \"b c x y z -> (b x y z) c\")\n",
    "\n",
    "        coord_feats = (\n",
    "            q_vox_size,\n",
    "            context_coord,\n",
    "            query_coord,\n",
    "            rel_norm_context_coord,\n",
    "            encoded_rel_norm_context_coord,\n",
    "        )\n",
    "        coord_feats = torch.cat(coord_feats, dim=1)\n",
    "        spatial_layout = {\n",
    "            \"b\": coord_feats.shape[0],\n",
    "            \"x\": coord_feats.shape[2],\n",
    "            \"y\": coord_feats.shape[3],\n",
    "            \"z\": coord_feats.shape[4],\n",
    "        }\n",
    "\n",
    "        coord_feats = einops.rearrange(coord_feats, \"b c x y z -> (b x y z) c\")\n",
    "        x_coord = coord_feats\n",
    "        sub_grid_pred = context_feats\n",
    "\n",
    "        if self.lin_pre is not None:\n",
    "            sub_grid_pred = self.lin_pre(sub_grid_pred)\n",
    "            sub_grid_pred = self.activate_fn(sub_grid_pred)\n",
    "\n",
    "        for l in self.internal_res_repr:\n",
    "            sub_grid_pred, x_coord = l(sub_grid_pred, x_coord)\n",
    "        sub_grid_pred = self.lin_post(sub_grid_pred)\n",
    "        sub_grid_pred = einops.rearrange(\n",
    "            sub_grid_pred, \"(b x y z) c -> b c x y z\", **spatial_layout\n",
    "        )\n",
    "        if return_rel_context_coord:\n",
    "            ret = (sub_grid_pred, rel_context_coord)\n",
    "        else:\n",
    "            ret = sub_grid_pred\n",
    "        return ret\n",
    "\n",
    "    def equal_space_forward(self, context_v, context_spatial_extent, context_vox_size):\n",
    "        return self.sub_grid_forward(\n",
    "            context_val=context_v,\n",
    "            context_coord=context_spatial_extent,\n",
    "            query_coord=context_spatial_extent,\n",
    "            context_vox_size=context_vox_size,\n",
    "            query_vox_size=context_vox_size,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        context_v,\n",
    "        context_spatial_extent,\n",
    "        query_vox_size,\n",
    "        query_coord,\n",
    "    ) -> torch.Tensor:\n",
    "        if query_vox_size.ndim == 2:\n",
    "            query_vox_size = query_vox_size[:, :, None, None, None]\n",
    "        context_vox_size = torch.abs(\n",
    "            context_spatial_extent[..., 1, 1, 1] - context_spatial_extent[..., 0, 0, 0]\n",
    "        )\n",
    "        context_vox_size = context_vox_size[:, :, None, None, None]\n",
    "\n",
    "        # If the context space and the query coordinates are equal, then we are actually\n",
    "        # just mapping within the same physical space to the same coordinates. So,\n",
    "        # linear interpolation would just zero-out all surrounding predicted voxels,\n",
    "        # and would be a massive waste of computation.\n",
    "        if (\n",
    "            (context_spatial_extent.shape == query_coord.shape)\n",
    "            and torch.isclose(context_spatial_extent, query_coord).all()\n",
    "            and torch.isclose(query_vox_size, context_vox_size).all()\n",
    "        ):\n",
    "            y = self.equal_space_forward(\n",
    "                context_v=context_v,\n",
    "                context_spatial_extent=context_spatial_extent,\n",
    "                context_vox_size=context_vox_size,\n",
    "            )\n",
    "        # More commonly, the input space will not equal the output space, and the\n",
    "        # prediction will need to be interpolated.\n",
    "        else:\n",
    "            # Construct a grid of nearest indices in context space by sampling a grid of\n",
    "            # *indices* given the coordinates in mm.\n",
    "            # The channel dim is just repeated for every\n",
    "            # channel, so that doesn't need to be in the idx grid.\n",
    "            idx_grid = torch.stack(\n",
    "                torch.meshgrid(\n",
    "                    *[\n",
    "                        torch.arange(0, context_spatial_extent.shape[i])\n",
    "                        for i in (0, 2, 3, 4)\n",
    "                    ],\n",
    "                    indexing=\"ij\",\n",
    "                ),\n",
    "                dim=1,\n",
    "            ).to(context_spatial_extent)\n",
    "            # Find the nearest grid point, where the batch+spatial dims are the \"channels.\"\n",
    "            nearest_coord_idx = pitn.nn.inr.weighted_ctx_v(\n",
    "                idx_grid,\n",
    "                # context_spatial_extent,\n",
    "                input_space_extent=context_spatial_extent,\n",
    "                target_space_extent=query_coord,\n",
    "                reindex_spatial_extents=True,\n",
    "                sample_mode=\"nearest\",\n",
    "            ).to(torch.long)\n",
    "            # Expand along channel dimension for raw indexing.\n",
    "            # nearest_coord_idx = einops.repeat(\n",
    "            #     nearest_coord_idx,\n",
    "            #     \"b dim x y z -> dim b repeat_c x y z\",\n",
    "            #     repeat_c=self.context_v_features,\n",
    "            # )\n",
    "            nearest_coord_idx = einops.rearrange(\n",
    "                nearest_coord_idx, \"b dim x y z -> dim (b x y z)\"\n",
    "            )\n",
    "            # nearest_coord_idx = tuple(torch.swapdims(nearest_coord_idx, 0, 1)).view(4, batch_size, -1)\n",
    "            batch_idx = nearest_coord_idx[0]\n",
    "            rel_norm_sub_window_grid_coord: torch.Tensor\n",
    "            sub_window_query_sample_grid = list()\n",
    "            # Build the low-res representation one sub-window voxel index at a time.\n",
    "            for i in (0, 1):\n",
    "                # Rebuild indexing tuple for each element of the sub-window\n",
    "                x_idx = nearest_coord_idx[1] + i\n",
    "                for j in (0, 1):\n",
    "                    y_idx = nearest_coord_idx[2] + j\n",
    "                    for k in (0, 1):\n",
    "                        z_idx = nearest_coord_idx[3] + k\n",
    "                        context_val = context_v[batch_idx, :, x_idx, y_idx, z_idx]\n",
    "                        context_val = einops.rearrange(\n",
    "                            context_val,\n",
    "                            \"(b x y z) c -> b c x y z\",\n",
    "                            x=query_coord.shape[2],\n",
    "                            y=query_coord.shape[3],\n",
    "                            z=query_coord.shape[4],\n",
    "                        )\n",
    "                        context_coord = context_spatial_extent[\n",
    "                            batch_idx, :, x_idx, y_idx, z_idx\n",
    "                        ]\n",
    "                        context_coord = einops.rearrange(\n",
    "                            context_coord,\n",
    "                            \"(b x y z) c -> b c x y z\",\n",
    "                            x=query_coord.shape[2],\n",
    "                            y=query_coord.shape[3],\n",
    "                            z=query_coord.shape[4],\n",
    "                        )\n",
    "\n",
    "                        ret_ctx_coord = True if (i == j == k == 0) else False\n",
    "                        sub_grid_pred_ijk = self.sub_grid_forward(\n",
    "                            context_val=context_val,\n",
    "                            context_coord=context_coord,\n",
    "                            query_coord=query_coord,\n",
    "                            context_vox_size=context_vox_size,\n",
    "                            query_vox_size=query_vox_size,\n",
    "                            return_rel_context_coord=ret_ctx_coord,\n",
    "                        )\n",
    "                        if ret_ctx_coord:\n",
    "                            sub_grid_pred_ijk = sub_grid_pred_ijk[0]\n",
    "                            rel_norm_context_coord = sub_grid_pred_ijk[1]\n",
    "                        else:\n",
    "                            rel_norm_context_coord = None\n",
    "\n",
    "                        sub_window_query_sample_grid.append(sub_grid_pred_ijk)\n",
    "\n",
    "                        if i == j == k == 0:\n",
    "                            # Find the relative coordinate of the query within the\n",
    "                            # sub-window.\n",
    "                            rel_norm_sub_window_grid_coord = torch.clamp(\n",
    "                                (rel_norm_context_coord - 0.5) * 2,\n",
    "                                -1 + self.TARGET_COORD_EPSILON,\n",
    "                                1 - self.TARGET_COORD_EPSILON,\n",
    "                            )\n",
    "            sub_window_query_sample_grid = torch.stack(\n",
    "                sub_window_query_sample_grid, dim=0\n",
    "            )\n",
    "            spatial_layout = {\n",
    "                \"b\": sub_window_query_sample_grid.shape[1],\n",
    "                \"x\": sub_window_query_sample_grid.shape[3],\n",
    "                \"y\": sub_window_query_sample_grid.shape[4],\n",
    "                \"z\": sub_window_query_sample_grid.shape[5],\n",
    "            }\n",
    "            sub_window = einops.rearrange(\n",
    "                sub_window_query_sample_grid,\n",
    "                \"(x_sub y_sub z_sub) b c x y z -> (b x y z) c x_sub y_sub z_sub\",\n",
    "                x_sub=2,\n",
    "                y_sub=2,\n",
    "                z_sub=2,\n",
    "            )\n",
    "            sub_window_grid = einops.rearrange(\n",
    "                rel_norm_sub_window_grid_coord, \"b dim x y z -> (b x y z) 1 1 1 dim \"\n",
    "            )\n",
    "\n",
    "            y = F.grid_sample(\n",
    "                sub_window,\n",
    "                sub_window_grid,\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=True,\n",
    "                padding_mode=\"reflection\",\n",
    "            )\n",
    "            y = einops.rearrange(y, \"(b x y z) c 1 1 1 -> b c x y z\", **spatial_layout)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3e7af3-8f50-4fa6-804d-b0b37d64baa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# INR/Decoder model\n",
    "class PoorConvContRepDecoder(torch.nn.Module):\n",
    "\n",
    "    TARGET_COORD_EPSILON = 1e-7\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        context_v_features: int,\n",
    "        out_features: int,\n",
    "        m_encode_num_freqs: int,\n",
    "        sigma_encode_scale: float,\n",
    "        in_features=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Determine the number of input features needed for the MLP.\n",
    "        # The order for concatenation is\n",
    "        # 1) ctx feats over the low-res input space, unfolded over a 3x3x3 window\n",
    "        # 2) target voxel shape\n",
    "        # 3) absolute coords of this forward pass' prediction target\n",
    "        # 4) absolute coords of the high-res target voxel\n",
    "        # 5) relative coords between high-res target coords and this forward pass'\n",
    "        #    prediction target, normalized by low-res voxel shape\n",
    "        # 6) encoding of relative coords\n",
    "        self.context_v_features = context_v_features * 3 * 3 * 3\n",
    "        self.ndim = 3\n",
    "        self.m_encode_num_freqs = m_encode_num_freqs\n",
    "        self.sigma_encode_scale = torch.as_tensor(sigma_encode_scale)\n",
    "        self.n_encode_features = self.ndim * 2 * self.m_encode_num_freqs\n",
    "        self.n_coord_features = 4 * self.ndim + self.n_encode_features\n",
    "        self.internal_features = self.context_v_features + self.n_coord_features\n",
    "\n",
    "        self.in_features = in_features * 3 * 3 * 3\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # \"Swish\" function, recommended in MeshFreeFlowNet\n",
    "        activate_cls = torch.nn.SiLU\n",
    "        self.activate_fn = activate_cls(inplace=True)\n",
    "        # Optional resizing linear layer, if the input size should be different than\n",
    "        # the hidden layer size.\n",
    "        if self.in_features is not None:\n",
    "            self.lin_pre = torch.nn.Linear(self.in_features, self.context_v_features)\n",
    "            self.norm_pre = None\n",
    "            # self.norm_pre = torch.nn.LazyBatchNorm1d(affine=True, track_running_stats=True)\n",
    "            # self.norm_pre = torch.nn.LazyInstanceNorm3d(affine=False, track_running_stats=False)\n",
    "        else:\n",
    "            self.lin_pre = None\n",
    "            self.norm_pre = None\n",
    "        self.norm_pre = None\n",
    "\n",
    "        # Internal hidden layers are two res MLPs.\n",
    "        self.internal_res_repr = torch.nn.ModuleList(\n",
    "            [\n",
    "                pitn.nn.inr.SkipMLPBlock(\n",
    "                    n_context_features=self.context_v_features,\n",
    "                    n_coord_features=self.n_coord_features,\n",
    "                    n_dense_layers=3,\n",
    "                    activate_fn=activate_cls,\n",
    "                )\n",
    "                for _ in range(2)\n",
    "            ]\n",
    "        )\n",
    "        self.lin_post = torch.nn.Linear(self.context_v_features, self.out_features)\n",
    "\n",
    "    def encode_relative_coord(self, coords):\n",
    "        c = einops.rearrange(coords, \"b d x y z -> (b x y z) d\")\n",
    "        sigma = self.sigma_encode_scale.expand_as(c).to(c)[..., None]\n",
    "        encode_pos = pitn.nn.inr.fourier_position_encoding(\n",
    "            c, sigma_scale=sigma, m_num_freqs=self.m_encode_num_freqs\n",
    "        )\n",
    "\n",
    "        encode_pos = einops.rearrange(\n",
    "            encode_pos,\n",
    "            \"(b x y z) d -> b d x y z\",\n",
    "            x=coords.shape[2],\n",
    "            y=coords.shape[3],\n",
    "            z=coords.shape[4],\n",
    "        )\n",
    "        return encode_pos\n",
    "\n",
    "    def sub_grid_forward(\n",
    "        self,\n",
    "        context_val,\n",
    "        context_coord,\n",
    "        query_coord,\n",
    "        context_vox_size,\n",
    "        query_vox_size,\n",
    "        return_rel_context_coord=False,\n",
    "    ):\n",
    "        # Take relative coordinate difference between the current context\n",
    "        # coord and the query coord.\n",
    "        rel_context_coord = torch.clamp_min(\n",
    "            context_coord - query_coord,\n",
    "            (-context_vox_size / 2) + self.TARGET_COORD_EPSILON,\n",
    "        )\n",
    "        # Also normalize to [0, 1)\n",
    "        # Coordinates are located in the center of the voxel. By the way\n",
    "        # the context vector is being constructed surrounding the query\n",
    "        # coord, the query coord is always within 1.5 x vox_size of the\n",
    "        # context (low-res space) coordinate. So, subtract the\n",
    "        # batch-and-channel-wise minimum, and divide by the known upper\n",
    "        # bound.\n",
    "        rel_norm_context_coord = (\n",
    "            rel_context_coord\n",
    "            - torch.amin(rel_context_coord, dim=(2, 3, 4), keepdim=True)\n",
    "        ) / (1.5 * context_vox_size)\n",
    "        assert (rel_norm_context_coord >= 0).all() and (\n",
    "            rel_norm_context_coord < 1.0\n",
    "        ).all()\n",
    "        encoded_rel_norm_context_coord = self.encode_relative_coord(\n",
    "            rel_norm_context_coord\n",
    "        )\n",
    "        q_vox_size = query_vox_size.expand_as(rel_norm_context_coord)\n",
    "\n",
    "        # Perform forward pass of the MLP.\n",
    "        if self.norm_pre is not None:\n",
    "            context_val = self.norm_pre(context_val)\n",
    "        context_feats = einops.rearrange(context_val, \"b c x y z -> (b x y z) c\")\n",
    "\n",
    "        coord_feats = (\n",
    "            q_vox_size,\n",
    "            context_coord,\n",
    "            query_coord,\n",
    "            rel_norm_context_coord,\n",
    "            encoded_rel_norm_context_coord,\n",
    "        )\n",
    "        coord_feats = torch.cat(coord_feats, dim=1)\n",
    "        spatial_layout = {\n",
    "            \"b\": coord_feats.shape[0],\n",
    "            \"x\": coord_feats.shape[2],\n",
    "            \"y\": coord_feats.shape[3],\n",
    "            \"z\": coord_feats.shape[4],\n",
    "        }\n",
    "\n",
    "        coord_feats = einops.rearrange(coord_feats, \"b c x y z -> (b x y z) c\")\n",
    "        x_coord = coord_feats\n",
    "        sub_grid_pred = context_feats\n",
    "\n",
    "        if self.lin_pre is not None:\n",
    "            sub_grid_pred = self.lin_pre(sub_grid_pred)\n",
    "            sub_grid_pred = self.activate_fn(sub_grid_pred)\n",
    "\n",
    "        for l in self.internal_res_repr:\n",
    "            sub_grid_pred, x_coord = l(sub_grid_pred, x_coord)\n",
    "        sub_grid_pred = self.lin_post(sub_grid_pred)\n",
    "        sub_grid_pred = einops.rearrange(\n",
    "            sub_grid_pred, \"(b x y z) c -> b c x y z\", **spatial_layout\n",
    "        )\n",
    "        if return_rel_context_coord:\n",
    "            ret = (sub_grid_pred, rel_context_coord)\n",
    "        else:\n",
    "            ret = sub_grid_pred\n",
    "        return ret\n",
    "\n",
    "    def equal_space_forward(self, context_v, context_spatial_extent, context_vox_size):\n",
    "        return self.sub_grid_forward(\n",
    "            context_val=context_v,\n",
    "            context_coord=context_spatial_extent,\n",
    "            query_coord=context_spatial_extent,\n",
    "            context_vox_size=context_vox_size,\n",
    "            query_vox_size=context_vox_size,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        context_v,\n",
    "        context_spatial_extent,\n",
    "        query_vox_size,\n",
    "        query_coord,\n",
    "    ) -> torch.Tensor:\n",
    "        if query_vox_size.ndim == 2:\n",
    "            query_vox_size = query_vox_size[:, :, None, None, None]\n",
    "        context_vox_size = torch.abs(\n",
    "            context_spatial_extent[..., 1, 1, 1] - context_spatial_extent[..., 0, 0, 0]\n",
    "        )\n",
    "        context_vox_size = context_vox_size[:, :, None, None, None]\n",
    "\n",
    "        # Unfold the context vector to include all 3x3x3 neighbors via concatenation of\n",
    "        # feature vectors into a new, larger feature space.\n",
    "        context_v = pitn.nn.functional.unfold_3d(\n",
    "            context_v,\n",
    "            kernel=3,\n",
    "            stride=1,\n",
    "            reshape_output=False,\n",
    "            pad=(1,) * 6,\n",
    "            mode=\"reflect\",\n",
    "        )\n",
    "        context_v = einops.rearrange(\n",
    "            context_v, \"b c xmk ymk zmk k_x k_y k_z -> b (k_x k_y k_z c) xmk ymk zmk\"\n",
    "        )\n",
    "\n",
    "        # If the context space and the query coordinates are equal, then we are actually\n",
    "        # just mapping within the same physical space to the same coordinates. So,\n",
    "        # linear interpolation would just zero-out all surrounding predicted voxels,\n",
    "        # and would be a massive waste of computation.\n",
    "        if (\n",
    "            (context_spatial_extent.shape == query_coord.shape)\n",
    "            and torch.isclose(context_spatial_extent, query_coord).all()\n",
    "            and torch.isclose(query_vox_size, context_vox_size).all()\n",
    "        ):\n",
    "            y = self.equal_space_forward(\n",
    "                context_v=context_v,\n",
    "                context_spatial_extent=context_spatial_extent,\n",
    "                context_vox_size=context_vox_size,\n",
    "            )\n",
    "        # More commonly, the input space will not equal the output space, and the\n",
    "        # prediction will need to be interpolated.\n",
    "        else:\n",
    "            # Construct a grid of nearest indices in context space by sampling a grid of\n",
    "            # *indices* given the coordinates in mm.\n",
    "            # The channel dim is just repeated for every\n",
    "            # channel, so that doesn't need to be in the idx grid.\n",
    "            idx_grid = torch.stack(\n",
    "                torch.meshgrid(\n",
    "                    *[\n",
    "                        torch.arange(0, context_spatial_extent.shape[i])\n",
    "                        for i in (0, 2, 3, 4)\n",
    "                    ],\n",
    "                    indexing=\"ij\",\n",
    "                ),\n",
    "                dim=1,\n",
    "            ).to(context_spatial_extent)\n",
    "            # Find the nearest grid point, where the batch+spatial dims are the \"channels.\"\n",
    "            nearest_coord_idx = pitn.nn.inr.weighted_ctx_v(\n",
    "                idx_grid,\n",
    "                # context_spatial_extent,\n",
    "                input_space_extent=context_spatial_extent,\n",
    "                target_space_extent=query_coord,\n",
    "                reindex_spatial_extents=True,\n",
    "                sample_mode=\"nearest\",\n",
    "            ).to(torch.long)\n",
    "            # Expand along channel dimension for raw indexing.\n",
    "            # nearest_coord_idx = einops.repeat(\n",
    "            #     nearest_coord_idx,\n",
    "            #     \"b dim x y z -> dim b repeat_c x y z\",\n",
    "            #     repeat_c=self.context_v_features,\n",
    "            # )\n",
    "            nearest_coord_idx = einops.rearrange(\n",
    "                nearest_coord_idx, \"b dim x y z -> dim (b x y z)\"\n",
    "            )\n",
    "            # nearest_coord_idx = tuple(torch.swapdims(nearest_coord_idx, 0, 1)).view(4, batch_size, -1)\n",
    "            batch_idx = nearest_coord_idx[0]\n",
    "            rel_norm_sub_window_grid_coord: torch.Tensor\n",
    "            sub_window_query_sample_grid = list()\n",
    "            # Build the low-res representation one sub-window voxel index at a time.\n",
    "            for i in (0, 1):\n",
    "                # Rebuild indexing tuple for each element of the sub-window\n",
    "                x_idx = nearest_coord_idx[1] + i\n",
    "                for j in (0, 1):\n",
    "                    y_idx = nearest_coord_idx[2] + j\n",
    "                    for k in (0, 1):\n",
    "                        z_idx = nearest_coord_idx[3] + k\n",
    "                        context_val = context_v[batch_idx, :, x_idx, y_idx, z_idx]\n",
    "                        context_val = einops.rearrange(\n",
    "                            context_val,\n",
    "                            \"(b x y z) c -> b c x y z\",\n",
    "                            x=query_coord.shape[2],\n",
    "                            y=query_coord.shape[3],\n",
    "                            z=query_coord.shape[4],\n",
    "                        )\n",
    "                        context_coord = context_spatial_extent[\n",
    "                            batch_idx, :, x_idx, y_idx, z_idx\n",
    "                        ]\n",
    "                        context_coord = einops.rearrange(\n",
    "                            context_coord,\n",
    "                            \"(b x y z) c -> b c x y z\",\n",
    "                            x=query_coord.shape[2],\n",
    "                            y=query_coord.shape[3],\n",
    "                            z=query_coord.shape[4],\n",
    "                        )\n",
    "\n",
    "                        ret_ctx_coord = True if (i == j == k == 0) else False\n",
    "                        sub_grid_pred_ijk = self.sub_grid_forward(\n",
    "                            context_val=context_val,\n",
    "                            context_coord=context_coord,\n",
    "                            query_coord=query_coord,\n",
    "                            context_vox_size=context_vox_size,\n",
    "                            query_vox_size=query_vox_size,\n",
    "                            return_rel_context_coord=ret_ctx_coord,\n",
    "                        )\n",
    "                        if ret_ctx_coord:\n",
    "                            sub_grid_pred_ijk = sub_grid_pred_ijk[0]\n",
    "                            rel_norm_context_coord = sub_grid_pred_ijk[1]\n",
    "                        else:\n",
    "                            rel_norm_context_coord = None\n",
    "\n",
    "                        sub_window_query_sample_grid.append(sub_grid_pred_ijk)\n",
    "\n",
    "                        if i == j == k == 0:\n",
    "                            # Find the relative coordinate of the query within the\n",
    "                            # sub-window.\n",
    "                            rel_norm_sub_window_grid_coord = torch.clamp(\n",
    "                                (rel_norm_context_coord - 0.5) * 2,\n",
    "                                -1 + self.TARGET_COORD_EPSILON,\n",
    "                                1 - self.TARGET_COORD_EPSILON,\n",
    "                            )\n",
    "            sub_window_query_sample_grid = torch.stack(\n",
    "                sub_window_query_sample_grid, dim=0\n",
    "            )\n",
    "            spatial_layout = {\n",
    "                \"b\": sub_window_query_sample_grid.shape[1],\n",
    "                \"x\": sub_window_query_sample_grid.shape[3],\n",
    "                \"y\": sub_window_query_sample_grid.shape[4],\n",
    "                \"z\": sub_window_query_sample_grid.shape[5],\n",
    "            }\n",
    "            sub_window = einops.rearrange(\n",
    "                sub_window_query_sample_grid,\n",
    "                \"(x_sub y_sub z_sub) b c x y z -> (b x y z) c x_sub y_sub z_sub\",\n",
    "                x_sub=2,\n",
    "                y_sub=2,\n",
    "                z_sub=2,\n",
    "            )\n",
    "            sub_window_grid = einops.rearrange(\n",
    "                rel_norm_sub_window_grid_coord, \"b dim x y z -> (b x y z) 1 1 1 dim \"\n",
    "            )\n",
    "\n",
    "            y = F.grid_sample(\n",
    "                sub_window,\n",
    "                sub_window_grid,\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=True,\n",
    "                padding_mode=\"reflection\",\n",
    "            )\n",
    "            y = einops.rearrange(y, \"(b x y z) c 1 1 1 -> b c x y z\", **spatial_layout)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673f5f3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ts = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "# Break ISO format because many programs don't like having colons ':' in a filename.\n",
    "ts = ts.replace(\":\", \"_\")\n",
    "tmp_res_dir = Path(p.tmp_results_dir) / ts\n",
    "tmp_res_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82057705",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class INRSystem(LightningLite):\n",
    "    def setup_logger_run(\n",
    "        self, run_kwargs: dict, logger_meta_params: dict, logger_tags: list\n",
    "    ):\n",
    "        aim_run = aim.Run(\n",
    "            system_tracking_interval=None,\n",
    "            log_system_params=True,\n",
    "            capture_terminal_logs=True,\n",
    "            **run_kwargs,\n",
    "        )\n",
    "        for k, v in logger_meta_params.items():\n",
    "            aim_run[k] = v\n",
    "        for v in logger_tags:\n",
    "            aim_run.add_tag(v)\n",
    "\n",
    "        return aim_run\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        epochs: int,\n",
    "        batch_size: int,\n",
    "        in_channels: int,\n",
    "        pred_channels: int,\n",
    "        encoder_kwargs: dict,\n",
    "        decoder_kwargs: dict,\n",
    "        train_dataset,\n",
    "        optim_kwargs: dict = dict(),\n",
    "        dataloader_kwargs: dict = dict(),\n",
    "        stage=\"train\",\n",
    "        logger_kwargs: dict = dict(),\n",
    "        logger_meta_params: dict = dict(),\n",
    "        logger_tags: list = list(),\n",
    "    ):\n",
    "\n",
    "        self.aim_run = self.setup_logger_run(\n",
    "            logger_kwargs, logger_meta_params, logger_tags\n",
    "        )\n",
    "        try:\n",
    "            encoder = INREncoder(**{**encoder_kwargs, **{\"in_channels\": in_channels}})\n",
    "            # decoder = ContRepDecoder(**decoder_kwargs)\n",
    "            decoder = PoorConvContRepDecoder(**decoder_kwargs)\n",
    "            recon_decoder = INREncoder(\n",
    "                in_channels=encoder.out_channels,\n",
    "                interior_channels=64,\n",
    "                # out_channels=encoder.in_channels,\n",
    "                out_channels=1,\n",
    "                n_res_units=2,\n",
    "                n_dense_units=2,\n",
    "                activate_fn=encoder_kwargs[\"activate_fn\"],\n",
    "            )\n",
    "\n",
    "            #!DEBUG\n",
    "            # encoder = DummyINRDecoder(**{**encoder_kwargs, **{\"in_channels\": in_channels}})\n",
    "            #!\n",
    "            self.print(encoder)\n",
    "            self.print(decoder)\n",
    "            self.print(recon_decoder)\n",
    "\n",
    "            optim_encoder = torch.optim.AdamW(encoder.parameters(), lr=1e-3)\n",
    "            encoder, optim_encoder = self.setup(encoder, optim_encoder)\n",
    "            optim_decoder = torch.optim.AdamW(decoder.parameters(), lr=5e-4)\n",
    "            decoder, optim_decoder = self.setup(decoder, optim_decoder)\n",
    "            optim_recon_decoder = torch.optim.AdamW(recon_decoder.parameters(), lr=1e-4)\n",
    "            recon_decoder, optim_recon_decoder = self.setup(\n",
    "                recon_decoder, optim_recon_decoder\n",
    "            )\n",
    "            loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
    "            # recon_loss_fn = pitn.metrics.NormRMSEMetric(reduction=\"mean\")\n",
    "            recon_loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "            train_dataloader = monai.data.DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                pin_memory=True,\n",
    "                **dataloader_kwargs,\n",
    "            )\n",
    "            train_dataloader = self.setup_dataloaders(train_dataloader)\n",
    "\n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "            recon_decoder.train()\n",
    "            out_dir = tmp_res_dir\n",
    "\n",
    "            losses = dict(\n",
    "                loss=list(),\n",
    "                epoch=list(),\n",
    "                step=list(),\n",
    "                encoder_grad_norm=list(),\n",
    "                decoder_grad_norm=list(),\n",
    "                recon_decoder_grad_norm=list(),\n",
    "            )\n",
    "            step = 0\n",
    "            train_lr = False\n",
    "            for epoch in range(epochs):\n",
    "                self.print(f\"\\nEpoch {epoch}\\n\", \"=\" * 10)\n",
    "                if epoch <= (epochs // 10):\n",
    "                    if not train_lr:\n",
    "                        train_dataloader.dataset.set_select_tf_keys(\n",
    "                            # add_keys=[\"lr_fodf\"],\n",
    "                            remove_keys=[\"fodf\", \"mask\", \"fr_patch_extent_acpc\"],\n",
    "                        )\n",
    "                    train_lr = True\n",
    "                # elif epoch == (epochs // 10):\n",
    "                elif False:\n",
    "                    if train_lr:\n",
    "                        train_dataloader.dataset.set_select_tf_keys(\n",
    "                            add_keys=[\"fodf\", \"mask\", \"fr_patch_extent_acpc\"],\n",
    "                            # remove_keys=[\"lr_fodf\"],\n",
    "                        )\n",
    "                    train_lr = False\n",
    "\n",
    "                for batch_dict in train_dataloader:\n",
    "\n",
    "                    x = batch_dict[\"lr_dwi\"]\n",
    "                    x_coords = batch_dict[\"lr_patch_extent_acpc\"]\n",
    "                    x_vox_size = torch.atleast_2d(batch_dict[\"lr_vox_size\"])\n",
    "                    x_mask = batch_dict[\"lr_mask\"].to(torch.bool)\n",
    "\n",
    "                    if not train_lr:\n",
    "                        y = batch_dict[\"fodf\"]\n",
    "                        y_mask = batch_dict[\"mask\"].to(torch.bool)\n",
    "                        y_coords = batch_dict[\"fr_patch_extent_acpc\"]\n",
    "                        y_vox_size = torch.atleast_2d(batch_dict[\"vox_size\"])\n",
    "                    else:\n",
    "                        y = batch_dict[\"lr_fodf\"]\n",
    "                        y_mask = batch_dict[\"lr_mask\"].to(torch.bool)\n",
    "                        y_coords = x_coords\n",
    "                        y_vox_size = x_vox_size\n",
    "\n",
    "                    optim_encoder.zero_grad()\n",
    "                    optim_decoder.zero_grad()\n",
    "                    optim_recon_decoder.zero_grad()\n",
    "\n",
    "                    ctx_v = encoder(x)\n",
    "\n",
    "                    if epoch < (epochs // 4):\n",
    "                        lambda_pred_fodf = 0.0\n",
    "                        lambda_recon = 1.0\n",
    "                    else:\n",
    "                        lambda_pred_fodf = 1.0\n",
    "                        lambda_recon = 0.0\n",
    "\n",
    "                    if lambda_pred_fodf != 0:\n",
    "                        pred_fodf = decoder(\n",
    "                            context_v=ctx_v,\n",
    "                            context_spatial_extent=x_coords,\n",
    "                            query_vox_size=y_vox_size,\n",
    "                            query_coord=y_coords,\n",
    "                        )\n",
    "                        y_mask_broad = torch.broadcast_to(y_mask, y.shape)\n",
    "                        loss_fodf = loss_fn(pred_fodf[y_mask_broad], y[y_mask_broad])\n",
    "                    else:\n",
    "                        with torch.no_grad():\n",
    "                            pred_fodf = decoder(\n",
    "                                context_v=ctx_v,\n",
    "                                context_spatial_extent=x_coords,\n",
    "                                query_vox_size=y_vox_size,\n",
    "                                query_coord=y_coords,\n",
    "                            )\n",
    "                        loss_fodf = torch.as_tensor([0]).to(y)\n",
    "\n",
    "                    if lambda_recon != 0:\n",
    "                        recon_pred = recon_decoder(ctx_v)\n",
    "                        recon_y = x[:, 0][:, None]\n",
    "                        x_mask_broad = torch.broadcast_to(x_mask, recon_y.shape)\n",
    "                        loss_recon = recon_loss_fn(\n",
    "                            recon_pred[x_mask_broad], recon_y[x_mask_broad]\n",
    "                        )\n",
    "                        # loss_recon = recon_loss_fn(\n",
    "                        #     recon_pred * x_mask_broad, recon_y * x_mask_broad\n",
    "                        # )\n",
    "                    else:\n",
    "                        with torch.no_grad():\n",
    "                            recon_pred = recon_decoder(ctx_v)\n",
    "                        loss_recon = torch.as_tensor([0]).to(y)\n",
    "\n",
    "                    loss = (lambda_pred_fodf * loss_fodf) + (lambda_recon * loss_recon)\n",
    "\n",
    "                    self.backward(loss)\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        itertools.chain(\n",
    "                            encoder.parameters(),\n",
    "                            decoder.parameters(),\n",
    "                            recon_decoder.parameters(),\n",
    "                        ),\n",
    "                        5.0,\n",
    "                        error_if_nonfinite=True,\n",
    "                    )\n",
    "                    optim_encoder.step()\n",
    "                    optim_decoder.step()\n",
    "                    optim_recon_decoder.step()\n",
    "\n",
    "                    encoder_grad_norm = self._calc_grad_norm(encoder)\n",
    "                    recon_decoder_grad_norm = self._calc_grad_norm(recon_decoder)\n",
    "                    decoder_grad_norm = self._calc_grad_norm(decoder)\n",
    "                    self.aim_run.track(\n",
    "                        {\n",
    "                            \"loss\": loss.detach().cpu().item(),\n",
    "                            \"loss_pred_fodf\": loss_fodf.detach().cpu().item(),\n",
    "                            \"loss_recon\": loss_recon.detach().cpu().item(),\n",
    "                            \"grad_norm_encoder\": encoder_grad_norm,\n",
    "                            \"grad_norm_decoder\": decoder_grad_norm,\n",
    "                            \"grad_norm_recon_decoder\": recon_decoder_grad_norm,\n",
    "                        },\n",
    "                        context={\n",
    "                            \"subset\": \"train\",\n",
    "                        },\n",
    "                        step=step,\n",
    "                        epoch=epoch,\n",
    "                    )\n",
    "                    self.print(\n",
    "                        f\"| {loss.detach().cpu().item()}\",\n",
    "                        f\"= {loss_fodf.detach().cpu().item()}\",\n",
    "                        f\"+ {lambda_recon}*{loss_recon.detach().cpu().item()}\",\n",
    "                        end=\" \",\n",
    "                        flush=True,\n",
    "                    )\n",
    "                    losses[\"loss\"].append(loss.detach().cpu().item())\n",
    "                    losses[\"epoch\"].append(epoch)\n",
    "                    losses[\"step\"].append(step)\n",
    "                    losses[\"encoder_grad_norm\"].append(self._calc_grad_norm(encoder))\n",
    "                    losses[\"recon_decoder_grad_norm\"].append(\n",
    "                        self._calc_grad_norm(recon_decoder)\n",
    "                    )\n",
    "                    losses[\"decoder_grad_norm\"].append(self._calc_grad_norm(decoder))\n",
    "\n",
    "                    step += 1\n",
    "\n",
    "                with mpl.rc_context({\"font.size\": 4.0}):\n",
    "                    # Save some example predictions after each epoch\n",
    "                    fig = plt.figure(dpi=180, figsize=(3, 7))\n",
    "                    pitn.viz.plot_vol_slices(\n",
    "                        x[0, 0].detach(),\n",
    "                        pred_fodf[0, 0].detach() * y_mask[0, 0].detach(),\n",
    "                        recon_pred[0, 0].detach() * x_mask[0, 0].detach(),\n",
    "                        y[0, 0].detach(),\n",
    "                        slice_idx=(0.4, 0.5, 0.5),\n",
    "                        title=f\"Epoch {epoch} Step {step}\",\n",
    "                        vol_labels=[\"Input\", \"Pred\", \"Recon\\nPred\", \"Target\"],\n",
    "                        colorbars=\"each\",\n",
    "                        fig=fig,\n",
    "                        cmap=\"gray\",\n",
    "                    )\n",
    "                    # plt.savefig(Path(out_dir) / f\"epoch_{epoch}_sample.png\")\n",
    "                    self.aim_run.track(\n",
    "                        aim.Image(fig, optimize=True, quality=100, format=\"png\"),\n",
    "                        name=\"train_sample\",\n",
    "                        epoch=epoch,\n",
    "                        step=step,\n",
    "                        context={\"subset\": \"train\"},\n",
    "                    )\n",
    "        except Exception as e:\n",
    "            self.aim_run.add_tag(\"failed\")\n",
    "            self.aim_run.close()\n",
    "            raise e\n",
    "\n",
    "        self.aim_run.close()\n",
    "\n",
    "        self.print(\"=\" * 10)\n",
    "        losses = pd.DataFrame.from_dict(losses)\n",
    "        losses.to_csv(Path(out_dir) / \"train_losses.csv\")\n",
    "\n",
    "        # Sync all pytorch-lightning processes.\n",
    "        self.barrier()\n",
    "\n",
    "    @staticmethod\n",
    "    def _calc_grad_norm(model, norm_type=2):\n",
    "        # https://discuss.pytorch.org/t/check-the-norm-of-gradients/27961/5\n",
    "        total_norm = 0\n",
    "        parameters = [\n",
    "            p for p in model.parameters() if p.grad is not None and p.requires_grad\n",
    "        ]\n",
    "        for p in parameters:\n",
    "            param_norm = p.grad.detach().data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm**0.5\n",
    "        return total_norm\n",
    "\n",
    "    # def validate(self, model, val_dataset):\n",
    "    #     pass\n",
    "\n",
    "    # def test(self, model, test_dataset):\n",
    "    #     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a906f7",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f861ead2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the system, may be re-used between training and testing.\n",
    "model_system = INRSystem(accelerator=\"gpu\", devices=1, precision=32)\n",
    "\n",
    "if \"in_channels\" not in p.encoder:\n",
    "    in_channels = int(train_dataset[0][\"lr_dwi\"].shape[0])\n",
    "else:\n",
    "    in_channels = p.encoder.in_channels\n",
    "\n",
    "model_system.run(\n",
    "    epochs=p.train.max_epochs,\n",
    "    batch_size=p.train.batch_size,\n",
    "    in_channels=in_channels,\n",
    "    pred_channels=p.decoder.out_features,\n",
    "    encoder_kwargs=p.encoder.to_dict(),\n",
    "    decoder_kwargs=p.decoder.to_dict(),\n",
    "    train_dataset=train_dataset,\n",
    "    # optim_kwargs={\"lr\": 1e-3},\n",
    "    dataloader_kwargs={\n",
    "        \"num_workers\": 17,\n",
    "        \"persistent_workers\": True,\n",
    "        \"prefetch_factor\": 3,\n",
    "    },\n",
    "    logger_kwargs={\n",
    "        k: p.aim_logger[k] for k in set(p.aim_logger.keys()) - {\"meta_params\", \"tags\"}\n",
    "    },\n",
    "    logger_meta_params=p.aim_logger.meta_params.to_dict(),\n",
    "    logger_tags=p.aim_logger.tags,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705e455f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "losses = pd.read_csv(tmp_res_dir / \"train_losses.csv\")\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(losses.step[50:], losses.loss[50:], label=\"loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(losses.step[50:], losses.encoder_grad_norm[50:], label=\"encoder grad norm\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(losses.step[50:], losses.decoder_grad_norm[50:], label=\"decoder grad norm\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(\n",
    "    losses.step[50:],\n",
    "    losses.recon_decoder_grad_norm[50:],\n",
    "    label=\"recon decoder grad norm\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd642c35",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=100)\n",
    "plt.plot(losses.step[750:], losses.loss[750:], label=\"loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(losses.step[750:], losses.encoder_grad_norm[750:], label=\"encoder grad norm\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(losses.step[750:], losses.decoder_grad_norm[750:], label=\"decoder grad norm\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(\n",
    "    losses.step[750:],\n",
    "    losses.recon_decoder_grad_norm[750:],\n",
    "    label=\"recon decoder grad norm\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fd85a0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1d5c6b6",
   "metadata": {},
   "source": [
    "## Testing & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55056e76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dbcc36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python [conda env:miniconda-pitn2]",
   "language": "python",
   "name": "conda-env-miniconda-pitn2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc-autonumbering": true,
  "toc-showtags": true,
  "vscode": {
   "interpreter": {
    "hash": "f41faa2479836806c9664d670a156675ad0f09912fd4b0aed749f41e3cac86f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
