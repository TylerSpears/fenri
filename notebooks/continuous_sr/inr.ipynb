{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ea2a2f",
   "metadata": {},
   "source": [
    "# Continuous-Space Super-Resolution of fODFs in Diffusion MRI\n",
    "\n",
    "Code by:\n",
    "\n",
    "Tyler Spears - tas6hh@virginia.edu\n",
    "\n",
    "Dr. Tom Fletcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2b219f",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "050754dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T13:09:37.197709Z",
     "iopub.status.busy": "2022-11-04T13:09:37.197097Z",
     "iopub.status.idle": "2022-11-04T13:09:39.732453Z",
     "shell.execute_reply": "2022-11-04T13:09:39.731176Z",
     "shell.execute_reply.started": "2022-11-04T13:09:37.197659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-04 13:09:39,434 - Created a temporary directory at /tmp/tmp5x6fmns0\n",
      "2022-11-04 13:09:39,436 - Writing /tmp/tmp5x6fmns0/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "# Automatically re-import project-specific modules.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# imports\n",
    "import collections\n",
    "import copy\n",
    "import datetime\n",
    "import functools\n",
    "import inspect\n",
    "import io\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import pdb\n",
    "import random\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "import typing\n",
    "import warnings\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from pprint import pprint as ppr\n",
    "\n",
    "import dotenv\n",
    "import einops\n",
    "\n",
    "# visualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import monai\n",
    "\n",
    "# Data management libraries.\n",
    "import nibabel as nib\n",
    "import nibabel.processing\n",
    "\n",
    "# Computation & ML libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import skimage\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchinfo\n",
    "import torchio\n",
    "from box import Box\n",
    "from natsort import natsorted\n",
    "from pytorch_lightning.lite import LightningLite\n",
    "\n",
    "import pitn\n",
    "\n",
    "plt.rcParams.update({\"figure.autolayout\": True})\n",
    "plt.rcParams.update({\"figure.facecolor\": [1.0, 1.0, 1.0, 1.0]})\n",
    "plt.rcParams.update({\"image.cmap\": \"gray\"})\n",
    "\n",
    "# Set print options for ndarrays/tensors.\n",
    "np.set_printoptions(suppress=True, threshold=100, linewidth=88)\n",
    "torch.set_printoptions(sci_mode=False, threshold=100, linewidth=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1bba4e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T13:09:39.735732Z",
     "iopub.status.busy": "2022-11-04T13:09:39.734691Z",
     "iopub.status.idle": "2022-11-04T13:09:41.231447Z",
     "shell.execute_reply": "2022-11-04T13:09:41.230050Z",
     "shell.execute_reply.started": "2022-11-04T13:09:39.735680Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "direnv: loading ~/work/pitn/.envrc\n",
      "direnv: creating conda environment\n",
      "\n",
      "CondaValueError: prefix already exists: /home/tas6hh/miniconda/envs/pitn2\n",
      "\n",
      "\n",
      "EnvironmentLocationNotFound: Not a conda environment: /\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update notebook's environment variables with direnv.\n",
    "# This requires the python-dotenv package, and direnv be installed on the system\n",
    "# This will not work on Windows.\n",
    "# NOTE: This is kind of hacky, and not necessarily safe. Be careful...\n",
    "# Libraries needed on the python side:\n",
    "# - os\n",
    "# - subprocess\n",
    "# - io\n",
    "# - dotenv\n",
    "\n",
    "# Form command to be run in direnv's context. This command will print out\n",
    "# all environment variables defined in the subprocess/sub-shell.\n",
    "command = \"direnv exec {} /usr/bin/env\".format(os.getcwd())\n",
    "# Run command in a new subprocess.\n",
    "proc = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, cwd=os.getcwd())\n",
    "# Store and format the subprocess' output.\n",
    "proc_out = proc.communicate()[0].strip().decode(\"utf-8\")\n",
    "# Use python-dotenv to load the environment variables by using the output of\n",
    "# 'direnv exec ...' as a 'dummy' .env file.\n",
    "dotenv.load_dotenv(stream=io.StringIO(proc_out), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4e08a24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T13:09:41.233810Z",
     "iopub.status.busy": "2022-11-04T13:09:41.233187Z",
     "iopub.status.idle": "2022-11-04T13:09:41.306177Z",
     "shell.execute_reply": "2022-11-04T13:09:41.304821Z",
     "shell.execute_reply.started": "2022-11-04T13:09:41.233763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Device IDX  0\n",
      "CUDA Current Device  0\n",
      "CUDA Device properties:  _CudaDeviceProperties(name='NVIDIA RTX A5000', major=8, minor=6, total_memory=24256MB, multi_processor_count=64)\n",
      "CuDNN convolution optimization enabled.\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# torch setup\n",
    "# allow for CUDA usage, if available\n",
    "if torch.cuda.is_available():\n",
    "    # Pick only one device for the default, may use multiple GPUs for training later.\n",
    "    if \"CUDA_PYTORCH_DEVICE_IDX\" in os.environ.keys():\n",
    "        dev_idx = int(os.environ[\"CUDA_PYTORCH_DEVICE_IDX\"])\n",
    "    else:\n",
    "        dev_idx = 0\n",
    "    device = torch.device(f\"cuda:{dev_idx}\")\n",
    "    print(\"CUDA Device IDX \", dev_idx)\n",
    "    torch.cuda.set_device(device)\n",
    "    print(\"CUDA Current Device \", torch.cuda.current_device())\n",
    "    print(\"CUDA Device properties: \", torch.cuda.get_device_properties(device))\n",
    "    # Activate cudnn benchmarking to optimize convolution algorithm speed.\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        print(\"CuDNN convolution optimization enabled.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# keep device as the cpu\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "887f61ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T13:09:41.308055Z",
     "iopub.status.busy": "2022-11-04T13:09:41.307657Z",
     "iopub.status.idle": "2022-11-04T13:09:41.457603Z",
     "shell.execute_reply": "2022-11-04T13:09:41.456153Z",
     "shell.execute_reply.started": "2022-11-04T13:09:41.308019Z"
    },
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr cap\n",
    "# Capture output and save to log. Needs to be at the *very first* line of the cell.\n",
    "# Watermark\n",
    "%load_ext watermark\n",
    "%watermark --author \"Tyler Spears\" --updated --iso8601  --python --machine --iversions --githash\n",
    "if torch.cuda.is_available():\n",
    "    # GPU information\n",
    "    try:\n",
    "        gpu_info = pitn.utils.system.get_gpu_specs()\n",
    "        print(gpu_info)\n",
    "    except NameError:\n",
    "        print(\"CUDA Version: \", torch.version.cuda)\n",
    "else:\n",
    "    print(\"CUDA not in use, falling back to CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5f8ab63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T13:09:41.461149Z",
     "iopub.status.busy": "2022-11-04T13:09:41.460734Z",
     "iopub.status.idle": "2022-11-04T13:09:41.500343Z",
     "shell.execute_reply": "2022-11-04T13:09:41.499489Z",
     "shell.execute_reply.started": "2022-11-04T13:09:41.461113Z"
    },
    "tags": [
     "keep_output",
     "active-ipynb"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Tyler Spears\n",
      "\n",
      "Last updated: 2022-11-04T13:09:41.375725+00:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.5\n",
      "IPython version      : 8.4.0\n",
      "\n",
      "Compiler    : GCC 10.3.0\n",
      "OS          : Linux\n",
      "Release     : 5.15.0-48-generic\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 20\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: 4f6e346c4e21b4eb262bfc92ec8ceee3a4ae889e\n",
      "\n",
      "nibabel   : 4.0.1\n",
      "torchinfo : 1.7.1\n",
      "matplotlib: 3.5.2\n",
      "skimage   : 0.19.3\n",
      "einops    : 0.4.1\n",
      "pandas    : 1.4.3\n",
      "torchio   : 0.18.84\n",
      "json      : 2.0.9\n",
      "monai     : 1.0.0\n",
      "seaborn   : 0.11.2\n",
      "sys       : 3.10.5 | packaged by conda-forge | (main, Jun 14 2022, 07:04:59) [GCC 10.3.0]\n",
      "numpy     : 1.23.1\n",
      "torch     : 1.12.1\n",
      "pitn      : 0.0.post1.dev206+gf002231.d20220911\n",
      "\n",
      "==================================================GPU Specs==================================================\n",
      "  id  Name              Driver Version      CUDA Version  Total Memory    uuid\n",
      "----  ----------------  ----------------  --------------  --------------  ----------------------------------------\n",
      "   0  NVIDIA RTX A5000  515.65.01                   11.6  24564.0MB       GPU-ed20d87f-e88e-692f-0b56-548b8a05ddea\n",
      "   1  NVIDIA RTX A5000  515.65.01                   11.6  24564.0MB       GPU-0636ee40-2eab-9533-1be7-dbbadade95c4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cap is defined in an ipython magic command\n",
    "try:\n",
    "    print(cap)\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf082031",
   "metadata": {},
   "source": [
    "## Experiment & Parameters Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "242e12aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T13:09:41.502543Z",
     "iopub.status.busy": "2022-11-04T13:09:41.501936Z",
     "iopub.status.idle": "2022-11-04T13:09:41.552334Z",
     "shell.execute_reply": "2022-11-04T13:09:41.551808Z",
     "shell.execute_reply.started": "2022-11-04T13:09:41.502498Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Config file not loaded\n"
     ]
    }
   ],
   "source": [
    "p = Box(default_box=True)\n",
    "# Experiment defaults, can be overridden in a config file.\n",
    "\n",
    "# General experiment-wide params\n",
    "###############################################\n",
    "p.experiment_name = \"sr_debug\"\n",
    "p.override_experiment_name = False\n",
    "p.results_dir = \"/data/srv/outputs/pitn/results/runs\"\n",
    "p.tmp_results_dir = \"/data/srv/outputs/pitn/results/tmp\"\n",
    "p.train_val_test_split_file = random.choice(\n",
    "    list(Path(\"./data_splits\").glob(\"HCP*train-val-test_split*.csv\"))\n",
    ")\n",
    "p.aim_uri = \"aim://dali.cpe.virginia.edu:53800\"\n",
    "###############################################\n",
    "p.train = dict(\n",
    "    in_patch_size=(32, 32, 32),\n",
    "    batch_size=3,\n",
    "    samples_per_subj_per_epoch=15,\n",
    "    max_epochs=50,\n",
    "    loss=\"mse\",\n",
    ")\n",
    "\n",
    "# Network/model parameters.\n",
    "p.encoder = dict(\n",
    "    interior_channels=100,\n",
    "    # (number of SH orders (l) + 1) * X that is as close to 100 as possible.\n",
    "    out_channels=16 * 6,\n",
    "    # out_channels=189,\n",
    "    n_res_units=1,\n",
    "    n_dense_units=1,\n",
    "    activate_fn=\"elu\",\n",
    ")\n",
    "p.decoder = dict(\n",
    "    context_v_features=128,\n",
    "    in_features=p.encoder.out_channels,\n",
    "    out_features=45,\n",
    "    m_encode_num_freqs=24,\n",
    "    sigma_encode_scale=3.0,\n",
    ")\n",
    "# If a config file exists, override the defaults with those values.\n",
    "try:\n",
    "    if \"PITN_CONFIG\" in os.environ.keys():\n",
    "        config_fname = Path(os.environ[\"PITN_CONFIG\"])\n",
    "    else:\n",
    "        config_fname = pitn.utils.system.get_file_glob_unique(Path(\".\"), r\"config.*\")\n",
    "    f_type = config_fname.suffix.casefold()\n",
    "    if f_type in {\".yaml\", \".yml\"}:\n",
    "        f_params = Box.from_yaml(filename=config_fname)\n",
    "    elif f_type == \".json\":\n",
    "        f_params = Box.from_json(filename=config_fname)\n",
    "    elif f_type == \".toml\":\n",
    "        f_params = Box.from_toml(filename=config_fname)\n",
    "    else:\n",
    "        raise RuntimeError()\n",
    "\n",
    "    p.merge_update(f_params)\n",
    "\n",
    "except:\n",
    "    print(\"WARNING: Config file not loaded\")\n",
    "    pass\n",
    "\n",
    "# Remove the default_box behavior now that params have been fully read in.\n",
    "_p = Box(default_box=False)\n",
    "_p.merge_update(p)\n",
    "p = _p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7044cdbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T13:09:41.553081Z",
     "iopub.status.busy": "2022-11-04T13:09:41.552913Z",
     "iopub.status.idle": "2022-11-04T13:09:41.584848Z",
     "shell.execute_reply": "2022-11-04T13:09:41.584118Z",
     "shell.execute_reply.started": "2022-11-04T13:09:41.553067Z"
    }
   },
   "outputs": [],
   "source": [
    "tvt_split = pd.read_csv(p.train_val_test_split_file)\n",
    "p.train.subj_ids = tvt_split[tvt_split.split == \"train\"].subj_id.tolist()\n",
    "p.val = dict()\n",
    "p.val.subj_ids = tvt_split[tvt_split.split == \"val\"].subj_id.tolist()\n",
    "p.test = dict()\n",
    "p.test.subj_ids = tvt_split[tvt_split.split == \"test\"].subj_id.tolist()\n",
    "\n",
    "# Ensure that no test subj ids are in either the training or validation sets.\n",
    "# However, we can have overlap between training and validation.\n",
    "assert len(set(p.train.subj_ids) & set(p.test.subj_ids)) == 0\n",
    "assert len(set(p.val.subj_ids) & set(p.test.subj_ids)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0d366b",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d4a7461",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T13:09:41.585820Z",
     "iopub.status.busy": "2022-11-04T13:09:41.585660Z",
     "iopub.status.idle": "2022-11-04T13:09:41.611172Z",
     "shell.execute_reply": "2022-11-04T13:09:41.610473Z",
     "shell.execute_reply.started": "2022-11-04T13:09:41.585806Z"
    }
   },
   "outputs": [],
   "source": [
    "hcp_full_res_data_dir = Path(\"/data/srv/data/pitn/hcp\")\n",
    "hcp_full_res_fodf_dir = Path(\"/data/srv/outputs/pitn/hcp/full-res/fodf\")\n",
    "hcp_low_res_data_dir = Path(\"/data/srv/outputs/pitn/hcp/downsample/scale-2.00mm/vol\")\n",
    "hcp_low_res_fodf_dir = Path(\"/data/srv/outputs/pitn/hcp/downsample/scale-2.00mm/fodf\")\n",
    "\n",
    "assert hcp_full_res_data_dir.exists()\n",
    "assert hcp_full_res_fodf_dir.exists()\n",
    "assert hcp_low_res_data_dir.exists()\n",
    "assert hcp_low_res_fodf_dir.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4f9741",
   "metadata": {},
   "source": [
    "### Create Patch-Based Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04af172b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T13:09:41.612133Z",
     "iopub.status.busy": "2022-11-04T13:09:41.611869Z",
     "iopub.status.idle": "2022-11-04T13:09:52.746902Z",
     "shell.execute_reply": "2022-11-04T13:09:52.746158Z",
     "shell.execute_reply.started": "2022-11-04T13:09:41.612119Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 4/4 [00:10<00:00,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Warnings caught:\n",
      "/home/tas6hh/miniconda/envs/pitn2/lib/python3.10/site-packages/monai/data/utils.py:771: UserWarning:\n",
      "Modifying image pixdim from [2. 2. 2. 1.] to [  2.           2.           2.         147.71779471]\n",
      "/home/tas6hh/miniconda/envs/pitn2/lib/python3.10/site-packages/monai/data/utils.py:771: UserWarning:\n",
      "Modifying image pixdim from [2. 2. 2. 1.] to [  2.           2.           2.         139.33250473]\n",
      "/home/tas6hh/miniconda/envs/pitn2/lib/python3.10/site-packages/monai/data/utils.py:771: UserWarning:\n",
      "Modifying image pixdim from [2. 2. 2. 1.] to [  2.           2.           2.         143.64381948]\n",
      "/home/tas6hh/miniconda/envs/pitn2/lib/python3.10/site-packages/monai/data/utils.py:771: UserWarning:\n",
      "Modifying image pixdim from [2. 2. 2. 1.] to [  2.           2.           2.         147.29068835]\n",
      "/home/tas6hh/miniconda/envs/pitn2/lib/python3.10/site-packages/monai/data/utils.py:771: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25 1.  ] to [  1.25         1.25         1.25       136.39006562]\n",
      "/home/tas6hh/miniconda/envs/pitn2/lib/python3.10/site-packages/monai/data/utils.py:771: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25 1.  ] to [  1.25         1.25         1.25       142.98076794]\n",
      "/home/tas6hh/miniconda/envs/pitn2/lib/python3.10/site-packages/monai/data/utils.py:771: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25 1.  ] to [  1.25         1.25         1.25       144.79856698]\n",
      "/home/tas6hh/miniconda/envs/pitn2/lib/python3.10/site-packages/monai/data/utils.py:771: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25 1.  ] to [  1.25        1.25        1.25      145.0314621]\n",
      "/home/tas6hh/miniconda/envs/pitn2/lib/python3.10/site-packages/monai/data/utils.py:771: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25  nan] to [  1.25         1.25         1.25       136.39006562]\n",
      "/home/tas6hh/miniconda/envs/pitn2/lib/python3.10/site-packages/monai/data/utils.py:771: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25  nan] to [  1.25         1.25         1.25       142.98076794]\n",
      "/home/tas6hh/miniconda/envs/pitn2/lib/python3.10/site-packages/monai/data/utils.py:771: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25  nan] to [  1.25         1.25         1.25       144.79856698]\n",
      "/home/tas6hh/miniconda/envs/pitn2/lib/python3.10/site-packages/monai/data/utils.py:771: UserWarning:\n",
      "Modifying image pixdim from [1.25 1.25 1.25  nan] to [  1.25        1.25        1.25      145.0314621]\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DEBUG_TRAIN_DATA_SUBJS = 4\n",
    "with warnings.catch_warnings(record=True) as warn_list:\n",
    "    # pre_sample_ds = pitn.data.datasets.HCPfODFINRDataset(\n",
    "    #     subj_ids=p.train.subj_ids,\n",
    "    #     dwi_root_dir=hcp_full_res_data_dir,\n",
    "    #     fodf_root_dir=hcp_full_res_fodf_dir,\n",
    "    #     lr_dwi_root_dir=hcp_low_res_data_dir,\n",
    "    #     lr_fodf_root_dir=hcp_low_res_fodf_dir,\n",
    "    #     transform=None,\n",
    "    # )\n",
    "\n",
    "    #!DEBUG\n",
    "    pre_sample_ds = pitn.data.datasets.HCPfODFINRDataset(\n",
    "        subj_ids=p.train.subj_ids[:DEBUG_TRAIN_DATA_SUBJS],\n",
    "        dwi_root_dir=hcp_full_res_data_dir,\n",
    "        fodf_root_dir=hcp_full_res_fodf_dir,\n",
    "        lr_dwi_root_dir=hcp_low_res_data_dir,\n",
    "        lr_fodf_root_dir=hcp_low_res_fodf_dir,\n",
    "        transform=None,\n",
    "    )\n",
    "    #!\n",
    "\n",
    "    pre_sample_train_dataset = monai.data.CacheDataset(\n",
    "        pre_sample_ds,\n",
    "        transform=pre_sample_ds.default_pre_sample_tf(\n",
    "            # Dilate by half the radius of one patch size.\n",
    "            mask_dilate_radius=max(p.train.in_patch_size)\n",
    "            // 4\n",
    "        ),\n",
    "        copy_cache=False,\n",
    "        num_workers=10,\n",
    "    )\n",
    "\n",
    "train_dataset = pitn.data.datasets.HCPINRfODFPatchDataset(\n",
    "    pre_sample_train_dataset,\n",
    "    patch_func=pitn.data.datasets.HCPINRfODFPatchDataset.default_patch_func(\n",
    "        spatial_size=p.train.in_patch_size,\n",
    "        num_samples=p.train.samples_per_subj_per_epoch,\n",
    "    ),\n",
    "    samples_per_image=p.train.samples_per_subj_per_epoch,\n",
    "    transform=pitn.data.datasets.HCPINRfODFPatchDataset.default_feature_tf(\n",
    "        p.train.in_patch_size\n",
    "    ),\n",
    ")\n",
    "# train_dataset = monai.data.PatchDataset(\n",
    "#     cache_dataset,\n",
    "#     patch_func=tf_patch_sampler,\n",
    "#     samples_per_image=p.train.samples_per_subj_per_epoch,\n",
    "#     transform=per_patch_transforms,\n",
    "# )\n",
    "print(\"=\" * 10)\n",
    "print(\"Warnings caught:\")\n",
    "ws = \"\\n\".join(\n",
    "    [\n",
    "        warnings.formatwarning(\n",
    "            w.message, w.category, w.filename, w.lineno, w.file, w.line\n",
    "        )\n",
    "        for w in warn_list\n",
    "    ]\n",
    ")\n",
    "ws = \"\\n\".join(filter(lambda s: bool(s.strip()), ws.splitlines()))\n",
    "print(ws, flush=True)\n",
    "print(\"=\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea571a3",
   "metadata": {},
   "source": [
    "### Validation & Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d2f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "869bdf8e",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56fe8c44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T13:09:52.748788Z",
     "iopub.status.busy": "2022-11-04T13:09:52.748207Z",
     "iopub.status.idle": "2022-11-04T13:09:52.788499Z",
     "shell.execute_reply": "2022-11-04T13:09:52.788091Z",
     "shell.execute_reply.started": "2022-11-04T13:09:52.748753Z"
    }
   },
   "outputs": [],
   "source": [
    "# Encoding model\n",
    "class INREncoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        interior_channels: int,\n",
    "        out_channels: int,\n",
    "        n_res_units: int,\n",
    "        n_dense_units: int,\n",
    "        activate_fn,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.interior_channels = interior_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        if isinstance(activate_fn, str):\n",
    "            activate_fn = pitn.utils.torch_lookups.activation[activate_fn]\n",
    "\n",
    "        # Pad to maintain the same input shape.\n",
    "        self.pre_conv = torch.nn.Conv3d(\n",
    "            self.in_channels,\n",
    "            self.interior_channels,\n",
    "            kernel_size=3,\n",
    "            padding=\"same\",\n",
    "            padding_mode=\"reflect\",\n",
    "        )\n",
    "\n",
    "        # Construct the densely-connected cascading layers.\n",
    "        # Create n_dense_units number of dense units.\n",
    "        top_level_units = list()\n",
    "        for _ in range(n_dense_units):\n",
    "            # Create n_res_units number of residual units for every dense unit.\n",
    "            res_layers = list()\n",
    "            for _ in range(n_res_units):\n",
    "                res_layers.append(\n",
    "                    pitn.nn.layers.ResBlock3dNoBN(\n",
    "                        self.interior_channels,\n",
    "                        kernel_size=3,\n",
    "                        activate_fn=activate_fn,\n",
    "                        padding=\"same\",\n",
    "                        padding_mode=\"reflect\",\n",
    "                    )\n",
    "                )\n",
    "            top_level_units.append(\n",
    "                pitn.nn.layers.DenseCascadeBlock3d(self.interior_channels, *res_layers)\n",
    "            )\n",
    "        self._activation_fn_init = activate_fn\n",
    "        self.activate_fn = activate_fn()\n",
    "\n",
    "        # Wrap everything into a densely-connected cascade.\n",
    "        self.cascade = pitn.nn.layers.DenseCascadeBlock3d(\n",
    "            self.interior_channels, *top_level_units\n",
    "        )\n",
    "\n",
    "        self.post_conv = torch.nn.Conv3d(\n",
    "            self.interior_channels,\n",
    "            self.out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=\"same\",\n",
    "            padding_mode=\"reflect\",\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        y = self.pre_conv(x)\n",
    "        y = self.activate_fn(y)\n",
    "        y = self.cascade(y)\n",
    "        y = self.activate_fn(y)\n",
    "        y = self.post_conv(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8030e179-79d0-40b5-8b6a-a8bf92aa9243",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T13:09:52.789343Z",
     "iopub.status.busy": "2022-11-04T13:09:52.789082Z",
     "iopub.status.idle": "2022-11-04T13:09:52.819207Z",
     "shell.execute_reply": "2022-11-04T13:09:52.818798Z",
     "shell.execute_reply.started": "2022-11-04T13:09:52.789329Z"
    }
   },
   "outputs": [],
   "source": [
    "class DummyINRDecoder(torch.nn.Module):\n",
    "    # Encoding model\n",
    "    def __init__(self, in_channels: int, out_channels: int, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.conv = torch.nn.Conv3d(self.in_channels, self.out_channels, 1, bias=False)\n",
    "        for param in self.conv.parameters():\n",
    "            param.requires_grad = False\n",
    "            torch.nn.init.dirac_(param)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c516c22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T13:09:52.820124Z",
     "iopub.status.busy": "2022-11-04T13:09:52.819925Z",
     "iopub.status.idle": "2022-11-04T13:09:52.864753Z",
     "shell.execute_reply": "2022-11-04T13:09:52.864341Z",
     "shell.execute_reply.started": "2022-11-04T13:09:52.820110Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# INR/Decoder model\n",
    "class ContRepDecoder(torch.nn.Module):\n",
    "\n",
    "    TARGET_COORD_EPSILON = 1e-7\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        context_v_features: int,\n",
    "        out_features: int,\n",
    "        m_encode_num_freqs: int,\n",
    "        sigma_encode_scale: float,\n",
    "        in_features=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Determine the number of input features needed for the MLP.\n",
    "        # The order for concatenation is\n",
    "        # 1) ctx feats over the low-res input space\n",
    "        # 2) target voxel shape\n",
    "        # 3) absolute coords of this forward pass' prediction target\n",
    "        # 4) absolute coords of the high-res target voxel\n",
    "        # 5) relative coords between high-res target coords and this forward pass'\n",
    "        #    prediction target, normalized by low-res voxel shape\n",
    "        # 6) encoding of relative coords\n",
    "        self.context_v_features = context_v_features\n",
    "        self.ndim = 3\n",
    "        self.m_encode_num_freqs = m_encode_num_freqs\n",
    "        self.sigma_encode_scale = torch.as_tensor(sigma_encode_scale)\n",
    "        self.n_encode_features = self.ndim * 2 * self.m_encode_num_freqs\n",
    "        self.n_coord_features = 4 * self.ndim + self.n_encode_features\n",
    "        self.internal_features = self.context_v_features + self.n_coord_features\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # \"Swish\" function, recommended in MeshFreeFlowNet\n",
    "        activate_cls = torch.nn.SiLU\n",
    "        self.activate_fn = activate_cls(inplace=True)\n",
    "        # Optional resizing linear layer, if the input size should be different than\n",
    "        # the hidden layer size.\n",
    "        if self.in_features is not None:\n",
    "            self.lin_pre = torch.nn.Linear(self.in_features, self.context_v_features)\n",
    "            self.norm_pre = None\n",
    "            # self.norm_pre = torch.nn.LazyBatchNorm1d(affine=True, track_running_stats=True)\n",
    "            # self.norm_pre = torch.nn.LazyInstanceNorm3d(affine=False, track_running_stats=False)\n",
    "        else:\n",
    "            self.lin_pre = None\n",
    "            self.norm_pre = None\n",
    "        self.norm_pre = None\n",
    "\n",
    "        # Internal hidden layers are two res MLPs.\n",
    "        self.internal_res_repr = torch.nn.ModuleList(\n",
    "            [\n",
    "                pitn.nn.inr.SkipMLPBlock(\n",
    "                    n_context_features=self.context_v_features,\n",
    "                    n_coord_features=self.n_coord_features,\n",
    "                    n_dense_layers=3,\n",
    "                    activate_fn=activate_cls,\n",
    "                )\n",
    "                for _ in range(2)\n",
    "            ]\n",
    "        )\n",
    "        self.lin_post = torch.nn.Linear(self.context_v_features, self.out_features)\n",
    "\n",
    "    def encode_relative_coord(self, coords):\n",
    "        c = einops.rearrange(coords, \"b d x y z -> (b x y z) d\")\n",
    "        sigma = self.sigma_encode_scale.expand_as(c).to(c)[..., None]\n",
    "        encode_pos = pitn.nn.inr.fourier_position_encoding(\n",
    "            c, sigma_scale=sigma, m_num_freqs=self.m_encode_num_freqs\n",
    "        )\n",
    "\n",
    "        encode_pos = einops.rearrange(\n",
    "            encode_pos,\n",
    "            \"(b x y z) d -> b d x y z\",\n",
    "            x=coords.shape[2],\n",
    "            y=coords.shape[3],\n",
    "            z=coords.shape[4],\n",
    "        )\n",
    "        return encode_pos\n",
    "\n",
    "    def sub_grid_forward(\n",
    "        self,\n",
    "        context_val,\n",
    "        context_coord,\n",
    "        query_coord,\n",
    "        context_vox_size,\n",
    "        query_vox_size,\n",
    "        return_rel_context_coord=False,\n",
    "    ):\n",
    "        # Take relative coordinate difference between the current context\n",
    "        # coord and the query coord.\n",
    "        rel_context_coord = torch.clamp_min(\n",
    "            context_coord - query_coord,\n",
    "            (-context_vox_size / 2) + self.TARGET_COORD_EPSILON,\n",
    "        )\n",
    "        # Also normalize to [0, 1)\n",
    "        # Coordinates are located in the center of the voxel. By the way\n",
    "        # the context vector is being constructed surrounding the query\n",
    "        # coord, the query coord is always within 1.5 x vox_size of the\n",
    "        # context (low-res space) coordinate. So, subtract the\n",
    "        # batch-and-channel-wise minimum, and divide by the known upper\n",
    "        # bound.\n",
    "        rel_norm_context_coord = (\n",
    "            rel_context_coord\n",
    "            - torch.amin(rel_context_coord, dim=(2, 3, 4), keepdim=True)\n",
    "        ) / (1.5 * context_vox_size)\n",
    "        assert (rel_norm_context_coord >= 0).all() and (\n",
    "            rel_norm_context_coord < 1.0\n",
    "        ).all()\n",
    "        encoded_rel_norm_context_coord = self.encode_relative_coord(\n",
    "            rel_norm_context_coord\n",
    "        )\n",
    "        q_vox_size = query_vox_size.expand_as(rel_norm_context_coord)\n",
    "\n",
    "        # Perform forward pass of the MLP.\n",
    "        if self.norm_pre is not None:\n",
    "            context_val = self.norm_pre(context_val)\n",
    "        context_feats = einops.rearrange(context_val, \"b c x y z -> (b x y z) c\")\n",
    "\n",
    "        coord_feats = (\n",
    "            q_vox_size,\n",
    "            context_coord,\n",
    "            query_coord,\n",
    "            rel_norm_context_coord,\n",
    "            encoded_rel_norm_context_coord,\n",
    "        )\n",
    "        coord_feats = torch.cat(coord_feats, dim=1)\n",
    "        spatial_layout = {\n",
    "            \"b\": coord_feats.shape[0],\n",
    "            \"x\": coord_feats.shape[2],\n",
    "            \"y\": coord_feats.shape[3],\n",
    "            \"z\": coord_feats.shape[4],\n",
    "        }\n",
    "\n",
    "        coord_feats = einops.rearrange(coord_feats, \"b c x y z -> (b x y z) c\")\n",
    "        x_coord = coord_feats\n",
    "        sub_grid_pred = context_feats\n",
    "\n",
    "        if self.lin_pre is not None:\n",
    "            sub_grid_pred = self.lin_pre(sub_grid_pred)\n",
    "            sub_grid_pred = self.activate_fn(sub_grid_pred)\n",
    "\n",
    "        for l in self.internal_res_repr:\n",
    "            sub_grid_pred, x_coord = l(sub_grid_pred, x_coord)\n",
    "        sub_grid_pred = self.lin_post(sub_grid_pred)\n",
    "        sub_grid_pred = einops.rearrange(\n",
    "            sub_grid_pred, \"(b x y z) c -> b c x y z\", **spatial_layout\n",
    "        )\n",
    "        if return_rel_context_coord:\n",
    "            ret = (sub_grid_pred, rel_context_coord)\n",
    "        else:\n",
    "            ret = sub_grid_pred\n",
    "        return ret\n",
    "\n",
    "    def equal_space_forward(self, context_v, context_spatial_extent, context_vox_size):\n",
    "        return self.sub_grid_forward(\n",
    "            context_val=context_v,\n",
    "            context_coord=context_spatial_extent,\n",
    "            query_coord=context_spatial_extent,\n",
    "            context_vox_size=context_vox_size,\n",
    "            query_vox_size=context_vox_size,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        context_v,\n",
    "        context_spatial_extent,\n",
    "        query_vox_size,\n",
    "        query_coord,\n",
    "    ) -> torch.Tensor:\n",
    "        if query_vox_size.ndim == 2:\n",
    "            query_vox_size = query_vox_size[:, :, None, None, None]\n",
    "        context_vox_size = torch.abs(\n",
    "            context_spatial_extent[..., 1, 1, 1] - context_spatial_extent[..., 0, 0, 0]\n",
    "        )\n",
    "        context_vox_size = context_vox_size[:, :, None, None, None]\n",
    "\n",
    "        # If the context space and the query coordinates are equal, then we are actually\n",
    "        # just mapping within the same physical space to the same coordinates. So,\n",
    "        # linear interpolation would just zero-out all surrounding predicted voxels,\n",
    "        # and would be a massive waste of computation.\n",
    "        if (\n",
    "            (context_spatial_extent.shape == query_coord.shape)\n",
    "            and torch.isclose(context_spatial_extent, query_coord).all()\n",
    "            and torch.isclose(query_vox_size, context_vox_size).all()\n",
    "        ):\n",
    "            y = self.equal_space_forward(\n",
    "                context_v=context_v,\n",
    "                context_spatial_extent=context_spatial_extent,\n",
    "                context_vox_size=context_vox_size,\n",
    "            )\n",
    "        # More commonly, the input space will not equal the output space, and the\n",
    "        # prediction will need to be interpolated.\n",
    "        else:\n",
    "            # Construct a grid of nearest indices in context space by sampling a grid of\n",
    "            # *indices* given the coordinates in mm.\n",
    "            # The channel dim is just repeated for every\n",
    "            # channel, so that doesn't need to be in the idx grid.\n",
    "            idx_grid = torch.stack(\n",
    "                torch.meshgrid(\n",
    "                    *[\n",
    "                        torch.arange(0, context_spatial_extent.shape[i])\n",
    "                        for i in (0, 2, 3, 4)\n",
    "                    ],\n",
    "                    indexing=\"ij\",\n",
    "                ),\n",
    "                dim=1,\n",
    "            ).to(context_spatial_extent)\n",
    "            # Find the nearest grid point, where the batch+spatial dims are the \"channels.\"\n",
    "            nearest_coord_idx = pitn.nn.inr.weighted_ctx_v(\n",
    "                idx_grid,\n",
    "                # context_spatial_extent,\n",
    "                input_space_extent=context_spatial_extent,\n",
    "                target_space_extent=query_coord,\n",
    "                reindex_spatial_extents=True,\n",
    "                sample_mode=\"nearest\",\n",
    "            ).to(torch.long)\n",
    "            # Expand along channel dimension for raw indexing.\n",
    "            # nearest_coord_idx = einops.repeat(\n",
    "            #     nearest_coord_idx,\n",
    "            #     \"b dim x y z -> dim b repeat_c x y z\",\n",
    "            #     repeat_c=self.context_v_features,\n",
    "            # )\n",
    "            nearest_coord_idx = einops.rearrange(\n",
    "                nearest_coord_idx, \"b dim x y z -> dim (b x y z)\"\n",
    "            )\n",
    "            # nearest_coord_idx = tuple(torch.swapdims(nearest_coord_idx, 0, 1)).view(4, batch_size, -1)\n",
    "            batch_idx = nearest_coord_idx[0]\n",
    "            rel_norm_sub_window_grid_coord: torch.Tensor\n",
    "            sub_window_query_sample_grid = list()\n",
    "            # Build the low-res representation one sub-window voxel index at a time.\n",
    "            for i in (0, 1):\n",
    "                # Rebuild indexing tuple for each element of the sub-window\n",
    "                x_idx = nearest_coord_idx[1] + i\n",
    "                for j in (0, 1):\n",
    "                    y_idx = nearest_coord_idx[2] + j\n",
    "                    for k in (0, 1):\n",
    "                        z_idx = nearest_coord_idx[3] + k\n",
    "                        context_val = context_v[batch_idx, :, x_idx, y_idx, z_idx]\n",
    "                        context_val = einops.rearrange(\n",
    "                            context_val,\n",
    "                            \"(b x y z) c -> b c x y z\",\n",
    "                            x=query_coord.shape[2],\n",
    "                            y=query_coord.shape[3],\n",
    "                            z=query_coord.shape[4],\n",
    "                        )\n",
    "                        context_coord = context_spatial_extent[\n",
    "                            batch_idx, :, x_idx, y_idx, z_idx\n",
    "                        ]\n",
    "                        context_coord = einops.rearrange(\n",
    "                            context_coord,\n",
    "                            \"(b x y z) c -> b c x y z\",\n",
    "                            x=query_coord.shape[2],\n",
    "                            y=query_coord.shape[3],\n",
    "                            z=query_coord.shape[4],\n",
    "                        )\n",
    "                        # # Take relative coordinate difference between the current context\n",
    "                        # # coord and the query coord.\n",
    "                        # rel_context_coord = torch.clamp_min(\n",
    "                        #     context_coord - query_coord,\n",
    "                        #     (-context_vox_size / 2) + self.TARGET_COORD_EPSILON,\n",
    "                        # )\n",
    "                        # # Also normalize to [0, 1)\n",
    "                        # # Coordinates are located in the center of the voxel. By the way\n",
    "                        # # the context vector is being constructed surrounding the query\n",
    "                        # # coord, the query coord is always within 1.5 x vox_size of the\n",
    "                        # # context (low-res space) coordinate. So, subtract the\n",
    "                        # # batch-and-channel-wise minimum, and divide by the known upper\n",
    "                        # # bound.\n",
    "                        # rel_norm_context_coord = (\n",
    "                        #     rel_context_coord\n",
    "                        #     - torch.amin(rel_context_coord, dim=(2, 3, 4), keepdim=True)\n",
    "                        # ) / (1.5 * context_vox_size)\n",
    "                        # assert (rel_norm_context_coord >= 0).all() and (\n",
    "                        #     rel_norm_context_coord < 1.0\n",
    "                        # ).all()\n",
    "                        # encoded_rel_norm_context_coord = self.encode_relative_coord(\n",
    "                        #     rel_norm_context_coord\n",
    "                        # )\n",
    "                        # q_vox_size = query_vox_size[..., None, None, None].expand_as(\n",
    "                        #     rel_norm_context_coord\n",
    "                        # )\n",
    "\n",
    "                        # # Perform forward pass of the MLP.\n",
    "                        # context_feats = einops.rearrange(\n",
    "                        #     context_val, \"b c x y z -> (b x y z) c\"\n",
    "                        # )\n",
    "                        # coord_feats = (\n",
    "                        #     q_vox_size,\n",
    "                        #     context_coord,\n",
    "                        #     query_coord,\n",
    "                        #     rel_norm_context_coord,\n",
    "                        #     encoded_rel_norm_context_coord,\n",
    "                        # )\n",
    "                        # coord_feats = torch.cat(coord_feats, dim=1)\n",
    "                        # spatial_layout = {\n",
    "                        #     \"b\": coord_feats.shape[0],\n",
    "                        #     \"x\": coord_feats.shape[2],\n",
    "                        #     \"y\": coord_feats.shape[3],\n",
    "                        #     \"z\": coord_feats.shape[4],\n",
    "                        # }\n",
    "\n",
    "                        # coord_feats = einops.rearrange(\n",
    "                        #     coord_feats, \"b c x y z -> (b x y z) c\"\n",
    "                        # )\n",
    "                        # x_coord = coord_feats\n",
    "                        # sub_grid_pred_ijk = context_feats\n",
    "                        # for l in self.internal_res_repr:\n",
    "                        #     sub_grid_pred_ijk, x_coord = l(sub_grid_pred_ijk, x_coord)\n",
    "                        # sub_grid_pred_ijk = self.lin_post(sub_grid_pred_ijk)\n",
    "                        # sub_grid_pred_ijk = einops.rearrange(\n",
    "                        #     sub_grid_pred_ijk, \"(b x y z) c -> b c x y z\", **spatial_layout\n",
    "                        # )\n",
    "                        ret_ctx_coord = True if (i == j == k == 0) else False\n",
    "                        sub_grid_pred_ijk = self.sub_grid_forward(\n",
    "                            context_val=context_val,\n",
    "                            context_coord=context_coord,\n",
    "                            query_coord=query_coord,\n",
    "                            context_vox_size=context_vox_size,\n",
    "                            query_vox_size=query_vox_size,\n",
    "                            return_rel_context_coord=ret_ctx_coord,\n",
    "                        )\n",
    "                        if ret_ctx_coord:\n",
    "                            sub_grid_pred_ijk = sub_grid_pred_ijk[0]\n",
    "                            rel_norm_context_coord = sub_grid_pred_ijk[1]\n",
    "                        else:\n",
    "                            rel_norm_context_coord = None\n",
    "\n",
    "                        sub_window_query_sample_grid.append(sub_grid_pred_ijk)\n",
    "\n",
    "                        if i == j == k == 0:\n",
    "                            # Find the relative coordinate of the query within the\n",
    "                            # sub-window.\n",
    "                            rel_norm_sub_window_grid_coord = torch.clamp(\n",
    "                                (rel_norm_context_coord - 0.5) * 2,\n",
    "                                -1 + self.TARGET_COORD_EPSILON,\n",
    "                                1 - self.TARGET_COORD_EPSILON,\n",
    "                            )\n",
    "            sub_window_query_sample_grid = torch.stack(\n",
    "                sub_window_query_sample_grid, dim=0\n",
    "            )\n",
    "            spatial_layout = {\n",
    "                \"b\": sub_window_query_sample_grid.shape[1],\n",
    "                \"x\": sub_window_query_sample_grid.shape[3],\n",
    "                \"y\": sub_window_query_sample_grid.shape[4],\n",
    "                \"z\": sub_window_query_sample_grid.shape[5],\n",
    "            }\n",
    "            sub_window = einops.rearrange(\n",
    "                sub_window_query_sample_grid,\n",
    "                \"(x_sub y_sub z_sub) b c x y z -> (b x y z) c x_sub y_sub z_sub\",\n",
    "                x_sub=2,\n",
    "                y_sub=2,\n",
    "                z_sub=2,\n",
    "            )\n",
    "            sub_window_grid = einops.rearrange(\n",
    "                rel_norm_sub_window_grid_coord, \"b dim x y z -> (b x y z) 1 1 1 dim \"\n",
    "            )\n",
    "\n",
    "            y = F.grid_sample(\n",
    "                sub_window,\n",
    "                sub_window_grid,\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=True,\n",
    "                padding_mode=\"reflection\",\n",
    "            )\n",
    "            y = einops.rearrange(y, \"(b x y z) c 1 1 1 -> b c x y z\", **spatial_layout)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "673f5f3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T13:09:52.865695Z",
     "iopub.status.busy": "2022-11-04T13:09:52.865324Z",
     "iopub.status.idle": "2022-11-04T13:09:52.892285Z",
     "shell.execute_reply": "2022-11-04T13:09:52.891879Z",
     "shell.execute_reply.started": "2022-11-04T13:09:52.865678Z"
    }
   },
   "outputs": [],
   "source": [
    "ts = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "# Break ISO format because many programs don't like having colons ':' in a filename.\n",
    "ts = ts.replace(\":\", \"_\")\n",
    "tmp_res_dir = Path(\".\") / \"tmp_res\" / ts\n",
    "tmp_res_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82057705",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T13:09:52.894548Z",
     "iopub.status.busy": "2022-11-04T13:09:52.894396Z",
     "iopub.status.idle": "2022-11-04T13:09:52.936134Z",
     "shell.execute_reply": "2022-11-04T13:09:52.935658Z",
     "shell.execute_reply.started": "2022-11-04T13:09:52.894535Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class INRSystem(LightningLite):\n",
    "    def run(\n",
    "        self,\n",
    "        epochs: int,\n",
    "        batch_size: int,\n",
    "        in_channels: int,\n",
    "        pred_channels: int,\n",
    "        encoder_kwargs: dict,\n",
    "        decoder_kwargs: dict,\n",
    "        train_dataset,\n",
    "        optim_kwargs: dict = dict(),\n",
    "        dataloader_kwargs: dict = dict(),\n",
    "        stage=\"train\",\n",
    "    ):\n",
    "        encoder = INREncoder(**{**encoder_kwargs, **{\"in_channels\": in_channels}})\n",
    "        decoder = ContRepDecoder(**decoder_kwargs)\n",
    "\n",
    "        #!DEBUG\n",
    "        # encoder = DummyINRDecoder(**{**encoder_kwargs, **{\"in_channels\": in_channels}})\n",
    "        #!\n",
    "        print(encoder)\n",
    "        print(decoder)\n",
    "\n",
    "        optim = torch.optim.AdamW(\n",
    "            itertools.chain(encoder.parameters(), decoder.parameters()), **optim_kwargs\n",
    "        )\n",
    "        encoder = self.setup(encoder)\n",
    "        decoder, optim = self.setup(decoder, optim)\n",
    "\n",
    "        loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "        train_dataloader = monai.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            **dataloader_kwargs,\n",
    "        )\n",
    "        train_dataloader = self.setup_dataloaders(train_dataloader)\n",
    "\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        out_dir = tmp_res_dir\n",
    "\n",
    "        losses = dict(\n",
    "            loss=list(),\n",
    "            epoch=list(),\n",
    "            step=list(),\n",
    "            encoder_grad_norm=list(),\n",
    "            decoder_grad_norm=list(),\n",
    "        )\n",
    "        step = 0\n",
    "        train_lr = False\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\nEpoch {epoch}\\n\", \"=\" * 10)\n",
    "            if epoch < (epochs // 10):\n",
    "                train_dataloader.dataset.set_select_tf_keys(\n",
    "                    add_keys=[\"lr_fodf\", \"lr_mask\"],\n",
    "                    remove_keys=[\"fodf\", \"mask\", \"fr_patch_extent_acpc\"],\n",
    "                )\n",
    "                train_lr = True\n",
    "            # elif epoch == (epochs // 10):\n",
    "            elif False:\n",
    "                train_dataloader.dataset.set_select_tf_keys(\n",
    "                    add_keys=[\"fodf\", \"mask\", \"fr_patch_extent_acpc\"],\n",
    "                    remove_keys=[\"lr_fodf\", \"lr_mask\"],\n",
    "                )\n",
    "                train_lr = False\n",
    "\n",
    "            for batch_dict in train_dataloader:\n",
    "                x = batch_dict[\"lr_dwi\"]\n",
    "                x_coords = batch_dict[\"lr_patch_extent_acpc\"]\n",
    "                x_vox_size = torch.atleast_2d(batch_dict[\"lr_vox_size\"])\n",
    "                if not train_lr:\n",
    "                    y = batch_dict[\"fodf\"]\n",
    "                    y_mask = batch_dict[\"mask\"].to(torch.bool)\n",
    "                    y_coords = batch_dict[\"fr_patch_extent_acpc\"]\n",
    "                    y_vox_size = torch.atleast_2d(batch_dict[\"vox_size\"])\n",
    "                else:\n",
    "                    y = batch_dict[\"lr_fodf\"]\n",
    "                    y_mask = batch_dict[\"lr_mask\"].to(torch.bool)\n",
    "                    y_coords = x_coords\n",
    "                    y_vox_size = x_vox_size\n",
    "                # print(y.shape)\n",
    "                # print(y_mask.shape)\n",
    "                # print(x.shape)\n",
    "                # print(x_coords.shape)\n",
    "                # print(y_coords.shape)\n",
    "\n",
    "                optim.zero_grad()\n",
    "                ctx_v = encoder(x)\n",
    "\n",
    "                pred_fodf = decoder(\n",
    "                    context_v=ctx_v,\n",
    "                    context_spatial_extent=x_coords,\n",
    "                    query_vox_size=y_vox_size,\n",
    "                    query_coord=y_coords,\n",
    "                )\n",
    "\n",
    "                y_mask_broad = torch.broadcast_to(y_mask, y.shape)\n",
    "                loss = loss_fn(pred_fodf[y_mask_broad], y[y_mask_broad])\n",
    "\n",
    "                self.backward(loss)\n",
    "                optim.step()\n",
    "\n",
    "                print(f\"| {loss.detach().cpu().item()}\", end=\" \", flush=True)\n",
    "                losses[\"loss\"].append(loss.detach().cpu().item())\n",
    "                losses[\"epoch\"].append(epoch)\n",
    "                losses[\"step\"].append(step)\n",
    "                losses[\"encoder_grad_norm\"].append(self._calc_grad_norm(encoder))\n",
    "                losses[\"decoder_grad_norm\"].append(self._calc_grad_norm(decoder))\n",
    "\n",
    "                if False:\n",
    "                    print(\"Overfitting to batch\")\n",
    "                    # plt.imshow(x[0, 7, :, 0].detach().cpu().numpy(), cmap=\"gray\")\n",
    "                    # plt.colorbar()\n",
    "                    # plt.show()\n",
    "\n",
    "                    # plt.imshow(y[0, 0, :, 0].detach().cpu().numpy(), cmap=\"gray\")\n",
    "                    # plt.colorbar()\n",
    "                    # plt.show()\n",
    "                    # plt.imshow(y_mask[0, 0, :, 0].detach().cpu().numpy(), cmap=\"gray\")\n",
    "                    # plt.colorbar()\n",
    "                    # plt.show()\n",
    "\n",
    "                    # fig = plt.figure(dpi=170, figsize=(5, 8))\n",
    "                    # pitn.viz.plot_vol_slices(\n",
    "                    #     x_coords[0].detach(),\n",
    "                    #     y_coords[0].detach(),\n",
    "                    #     slice_idx=(0.4, 0.5, 0.5),\n",
    "                    #     title=f\"Epoch {epoch} Step {step}\",\n",
    "                    #     vol_labels=[\"Source Coord\", \"Target Coord\"],\n",
    "                    #     channel_labels=[\"X\", \"Y\", \"Z\"],\n",
    "                    #     colorbars=\"each\",\n",
    "                    #     fig=fig,\n",
    "                    #     cmap=\"gray\",\n",
    "                    # )\n",
    "                    # plt.show()\n",
    "\n",
    "                    encoder, decoder, optim = self._overfit_batch(\n",
    "                        repeats=10,\n",
    "                        encoder=encoder,\n",
    "                        decoder=decoder,\n",
    "                        optim=optim,\n",
    "                        loss_fn=loss_fn,\n",
    "                        x=x,\n",
    "                        y=y,\n",
    "                        x_coords=x_coords,\n",
    "                        y_coords=y_coords,\n",
    "                        y_mask=y_mask,\n",
    "                        y_vox_size=y_vox_size,\n",
    "                    )\n",
    "                    fig = plt.figure(dpi=170, figsize=(5, 8))\n",
    "                    pitn.viz.plot_vol_slices(\n",
    "                        x[0, 7].detach(),\n",
    "                        pred_fodf_patch[0, 0].detach(),\n",
    "                        y[0, 0].detach(),\n",
    "                        y_mask[0, 0].detach(),\n",
    "                        slice_idx=(0.4, 0.5, 0.5),\n",
    "                        title=f\"epoch {epoch} step {step}\",\n",
    "                        vol_labels=[\"input\", \"pred\", \"target\", \"target mask\"],\n",
    "                        colorbars=\"each\",\n",
    "                        fig=fig,\n",
    "                        cmap=\"gray\",\n",
    "                    )\n",
    "                    plt.savefig(Path(out_dir) / f\"overfit_epoch_{epoch}.png\")\n",
    "                    #!DEBUG\n",
    "                    # return\n",
    "                    #!\n",
    "                step += 1\n",
    "            # Save some example predictions after each epoch\n",
    "            fig = plt.figure(dpi=150, figsize=(4, 6))\n",
    "            pitn.viz.plot_vol_slices(\n",
    "                x[0, 7].detach(),\n",
    "                pred_fodf[0, 0].detach() * y_mask[0, 0].detach(),\n",
    "                y[0, 0].detach(),\n",
    "                slice_idx=(0.4, 0.5, 0.5),\n",
    "                title=f\"Epoch {epoch} Step {step}\",\n",
    "                vol_labels=[\"Input\", \"Pred\", \"Target\"],\n",
    "                colorbars=\"each\",\n",
    "                fig=fig,\n",
    "                cmap=\"gray\",\n",
    "            )\n",
    "            plt.savefig(Path(out_dir) / f\"epoch_{epoch}_sample.png\")\n",
    "\n",
    "        print(\"=\" * 10)\n",
    "        losses = pd.DataFrame.from_dict(losses)\n",
    "        losses.to_csv(Path(out_dir) / \"train_losses.csv\")\n",
    "        losses.plot()\n",
    "\n",
    "    def _overfit_batch(\n",
    "        self,\n",
    "        repeats: int,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        optim,\n",
    "        loss_fn,\n",
    "        x,\n",
    "        y,\n",
    "        x_coords,\n",
    "        y_coords,\n",
    "        y_mask,\n",
    "        y_vox_size,\n",
    "    ):\n",
    "        optim.zero_grad()\n",
    "        vectorized_y_coords = einops.rearrange(y_coords, \"b c x y z -> (b x y z) c\")\n",
    "        vectorized_y_vox_size = einops.rearrange(\n",
    "            y_vox_size.expand(*y.shape[2:], -1, -1),\n",
    "            \"x y z b c -> (b x y z) c\",\n",
    "        )\n",
    "        y_mask_broad = torch.broadcast_to(y_mask, y.shape)\n",
    "        for i in range(repeats):\n",
    "            optim.zero_grad()\n",
    "            ctx_v = encoder(x)\n",
    "            ctx_v = pitn.nn.inr.linear_weighted_ctx_v(\n",
    "                ctx_v,\n",
    "                input_space_extent=x_coords,\n",
    "                target_space_extent=y_coords,\n",
    "                reindex_spatial_extents=True,\n",
    "            )\n",
    "            ctx_v = einops.rearrange(ctx_v, \"b c x y z -> (b x y z) c\")\n",
    "\n",
    "            pred_fodf = decoder(\n",
    "                query_coord=vectorized_y_coords,\n",
    "                context_v=ctx_v,\n",
    "                vox_size=vectorized_y_vox_size,\n",
    "            )\n",
    "            pred_fodf_patch = einops.rearrange(\n",
    "                pred_fodf,\n",
    "                \"(b x y z) c -> b c x y z\",\n",
    "                b=y.shape[0],\n",
    "                c=y.shape[1],\n",
    "                x=y.shape[2],\n",
    "                y=y.shape[3],\n",
    "                z=y.shape[4],\n",
    "            )\n",
    "\n",
    "            loss = loss_fn(pred_fodf_patch[y_mask_broad], y[y_mask_broad])\n",
    "            self.backward(loss)\n",
    "            optim.step()\n",
    "            if i % (repeats // 10) == 0:\n",
    "                print(\n",
    "                    f\"Overfit step {i} out of {repeats} loss {loss.detach().cpu().item()}\",\n",
    "                    end=\" \",\n",
    "                )\n",
    "        if repeats > 0:\n",
    "            optim.zero_grad()\n",
    "        return encoder, decoder, optim\n",
    "\n",
    "    @staticmethod\n",
    "    def _calc_grad_norm(model, norm_type=2):\n",
    "        # https://discuss.pytorch.org/t/check-the-norm-of-gradients/27961/5\n",
    "        total_norm = 0\n",
    "        parameters = [\n",
    "            p for p in model.parameters() if p.grad is not None and p.requires_grad\n",
    "        ]\n",
    "        for p in parameters:\n",
    "            param_norm = p.grad.detach().data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm**0.5\n",
    "        return total_norm\n",
    "\n",
    "    # def validate(self, model, val_dataset):\n",
    "    #     pass\n",
    "\n",
    "    # def test(self, model, test_dataset):\n",
    "    #     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a906f7",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f861ead2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T13:09:52.936959Z",
     "iopub.status.busy": "2022-11-04T13:09:52.936723Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tas6hh/miniconda/envs/pitn2/lib/python3.10/site-packages/torch/nn/modules/lazy.py:178: UserWarning:\n",
      "\n",
      "Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INREncoder(\n",
      "  (pre_conv): Conv3d(189, 100, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "  (activate_fn): ELU(alpha=1.0)\n",
      "  (cascade): DenseCascadeBlock3d(\n",
      "    (base_layers): ModuleList(\n",
      "      (0): DenseCascadeBlock3d(\n",
      "        (base_layers): ModuleList(\n",
      "          (0): ResBlock3dNoBN(\n",
      "            (conv1): Conv3d(100, 100, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (conv2): Conv3d(100, 100, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      "            (active_fn): ELU(alpha=1.0)\n",
      "          )\n",
      "        )\n",
      "        (combiner_convs): ModuleList(\n",
      "          (0): LazyConv3d(0, 100, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (combiner_convs): ModuleList(\n",
      "      (0): LazyConv3d(0, 100, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "    )\n",
      "  )\n",
      "  (post_conv): Conv3d(100, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=same, padding_mode=reflect)\n",
      ")\n",
      "ContRepDecoder(\n",
      "  (activate_fn): SiLU(inplace=True)\n",
      "  (lin_pre): Linear(in_features=96, out_features=128, bias=True)\n",
      "  (internal_res_repr): ModuleList(\n",
      "    (0): SkipMLPBlock(\n",
      "      (layers): ModuleList(\n",
      "        (0): ModuleDict(\n",
      "          (linear): Linear(in_features=284, out_features=128, bias=True)\n",
      "          (activate_fn): SiLU()\n",
      "          (norm): None\n",
      "        )\n",
      "        (1): ModuleDict(\n",
      "          (linear): Linear(in_features=284, out_features=128, bias=True)\n",
      "          (activate_fn): SiLU()\n",
      "          (norm): None\n",
      "        )\n",
      "        (2): ModuleDict(\n",
      "          (linear): Linear(in_features=284, out_features=128, bias=True)\n",
      "          (activate_fn): SiLU()\n",
      "          (norm): None\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): SkipMLPBlock(\n",
      "      (layers): ModuleList(\n",
      "        (0): ModuleDict(\n",
      "          (linear): Linear(in_features=284, out_features=128, bias=True)\n",
      "          (activate_fn): SiLU()\n",
      "          (norm): None\n",
      "        )\n",
      "        (1): ModuleDict(\n",
      "          (linear): Linear(in_features=284, out_features=128, bias=True)\n",
      "          (activate_fn): SiLU()\n",
      "          (norm): None\n",
      "        )\n",
      "        (2): ModuleDict(\n",
      "          (linear): Linear(in_features=284, out_features=128, bias=True)\n",
      "          (activate_fn): SiLU()\n",
      "          (norm): None\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lin_post): Linear(in_features=128, out_features=45, bias=True)\n",
      ")\n",
      "\n",
      "Epoch 0\n",
      " ==========\n",
      "| 0.9527841806411743 | 0.6402933597564697 | 0.28084301948547363 | 0.4904673993587494 | 0.25644177198410034 | 0.24130290746688843 | 0.14814616739749908 | 0.10666237026453018 | 0.11909376829862595 | 0.07782178372144699 | 0.06026565283536911 | 0.05320475995540619 | 0.05473097786307335 | 0.05835562199354172 | 0.044949110597372055 | 0.048991858959198 | 0.03802099451422691 | 0.03570788726210594 | 0.04378069192171097 | 0.026650305837392807 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_470352/196645146.py:184: UserWarning:\n",
      "\n",
      "This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      " ==========\n",
      "| 0.0308395903557539 | 0.02753922902047634 | 0.020307596772909164 | 0.018062978982925415 | 0.020366674289107323 | 0.019054917618632317 | 0.015209907665848732 | 0.026508266106247902 | 0.012492582201957703 | 0.014927837066352367 | 0.011450011283159256 | 0.014284051954746246 | 0.011542794294655323 | 0.010614550672471523 | 0.010652609169483185 | 0.008611832745373249 | 0.008238350972533226 | 0.00887028593569994 | 0.007632219232618809 | 0.010203140787780285 \n",
      "Epoch 2\n",
      " ==========\n",
      "| 0.007301986217498779 | 0.007978972047567368 | 0.006475443486124277 | 0.008561136201024055 | 0.005358384922146797 | 0.0051088156178593636 | 0.006120727397501469 | 0.00505606085062027 | 0.00522373104467988 | 0.0060277110897004604 | 0.005088084377348423 | 0.005304340273141861 | 0.005173579324036837 | 0.0037843186873942614 | 0.0045495787635445595 | 0.0038892836309969425 | 0.003687528893351555 | 0.004560996778309345 | 0.004772053565829992 | 0.0030943327583372593 \n",
      "Epoch 3\n",
      " ==========\n",
      "| 0.0037268013693392277 | 0.0031993291340768337 | 0.0035617754328995943 | 0.003424721769988537 | 0.003033537184819579 | 0.002590855583548546 | 0.0024675887543708086 | 0.002933409297838807 | 0.002494959393516183 | 0.0033692570868879557 | 0.0024047954939305782 | 0.0024372648913413286 | 0.002101049991324544 | 0.002548770047724247 | 0.002507974859327078 | 0.002426731400191784 | 0.0023454823531210423 | 0.0023932650219649076 | 0.0023888919968158007 | 0.002078795339912176 \n",
      "Epoch 4\n",
      " ==========\n",
      "| 0.002178052207455039 | 0.0018667143303900957 | 0.0019437193404883146 | 0.002019744599238038 | 0.0019640852697193623 | 0.0017835362814366817 | 0.0019815790001302958 | 0.0017360430210828781 | 0.0016335720429196954 | 0.0017343884101137519 | 0.0016925964737311006 | 0.0016195840435102582 | 0.0014386801049113274 | 0.001481842016801238 | 0.0015079102013260126 | 0.0018197341123595834 | 0.0018250758294016123 | 0.0013548156712204218 | 0.0017871859017759562 | 0.0012845529709011316 \n",
      "Epoch 5\n",
      " ==========\n",
      "| 0.001504511572420597 | 0.0014221008168533444 | 0.0013031895505264401 | 0.0013915888266637921 | 0.0015761101385578513 | 0.0014785885578021407 | 0.0014990641502663493 | 0.0015695692272856832 | 0.0012350066099315882 | 0.0012571855913847685 | 0.0012190132401883602 | 0.0011180061846971512 | 0.001184428227134049 | 0.001695073675364256 | 0.0011817456688731909 | 0.0012402316788211465 | 0.001396907027810812 | 0.0010539359645918012 | 0.001162348547950387 | 0.0012405813904479146 \n",
      "Epoch 6\n",
      " ==========\n",
      "| 0.0013025493826717138 | 0.001579604926519096 | 0.0011374816531315446 | 0.0012592488201335073 | 0.0010511628352105618 | 0.0012491141678765416 | 0.001246906933374703 | 0.0011038307566195726 | 0.0012144484790042043 | 0.0011359165655449033 | 0.0013165295822545886 | 0.001298847608268261 | 0.0009590814588591456 | 0.001160979736596346 | 0.0012580978218466043 | 0.0011224254267290235 | 0.001004580408334732 | 0.0012466629268601537 | 0.001125598675571382 | 0.0010347396600991488 \n",
      "Epoch 7\n",
      " ==========\n",
      "| 0.001100419438444078 | 0.0012415165547281504 | 0.0009596275631338358 | 0.001100972294807434 | 0.0009971745312213898 | 0.0011012742761522532 | 0.0010768483625724912 | 0.0011096964590251446 | 0.001042646006681025 | 0.0010547497076913714 | 0.0011649667285382748 | 0.0011421345407143235 | 0.0009998194873332977 | 0.001142216264270246 | 0.0011462796246632934 | 0.0009537851437926292 | 0.0009241960942745209 | 0.0010894292499870062 | 0.0011183539172634482 | 0.0010536076733842492 \n",
      "Epoch 8\n",
      " ==========\n",
      "| 0.000938253419008106 | 0.0007791372481733561 | 0.0010528814746066928 | 0.0008981080609373748 | 0.0008975833188742399 | 0.0009397428948432207 | 0.0010120684746652842 | 0.0009175455779768527 | 0.0009746105642989278 | 0.0010120951337739825 | 0.00096369709353894 | 0.0009552901028655469 | 0.000959309283643961 | 0.0008689119131304324 | 0.0008142634178511798 | 0.0010600208770483732 | 0.0008756382158026099 | 0.001103265443816781 | 0.0006964433123357594 | 0.001054649124853313 \n",
      "Epoch 9\n",
      " ==========\n",
      "| 0.0009184934315271676 | 0.0009339457610622048 | 0.0009813703363761306 | 0.0007949400460347533 | 0.0009701859671622515 | 0.000952443340793252 | 0.00102576136123389 | 0.0010254504159092903 | 0.0008764262893237174 | 0.001053507556207478 | 0.0008941299165599048 | 0.0008537167450413108 | 0.00083008041838184 | 0.0009538393933326006 | 0.0008003257680684328 | 0.0008916304796002805 | 0.0007670524646528065 | 0.0007689634803682566 | 0.0008305158116854727 | 0.000716512615326792 \n",
      "Epoch 10\n",
      " ==========\n",
      "| 0.0009712012833915651 | 0.0007745401235297322 | 0.0009356937371194363 | 0.0009148389217443764 | 0.0008490515174344182 | 0.0010092196753248572 | 0.0008504086872562766 | 0.0010221995180472732 | 0.0007273946539498866 | 0.0008002300164662302 | 0.0008235761779360473 | 0.000818856933619827 | 0.0008407894056290388 | 0.0007592837209813297 | 0.0008957265526987612 | 0.0009805537993088365 | 0.000880706065800041 | 0.0008781190263107419 | 0.0007436953601427376 | 0.0007866431842558086 \n",
      "Epoch 11\n",
      " ==========\n",
      "| 0.000915299984626472 | 0.0006976901204325259 | 0.0006600500200875103 | 0.0008540303097106516 | 0.0006995004951022565 | 0.0009623563964851201 | 0.0009135783766396344 | 0.000948648783378303 | 0.0009181952336803079 | 0.0007350598461925983 | 0.0008118748664855957 | 0.0008486409205943346 | 0.0008879366214387119 | 0.0009246105910278857 | 0.0008788996492512524 | 0.0008673188858665526 | 0.000897485064342618 | 0.0007153842016123235 | 0.0008040056563913822 | 0.0008830451406538486 \n",
      "Epoch 12\n",
      " ==========\n",
      "| 0.0007427099044434726 | 0.0008361306390725076 | 0.0008215689449571073 | 0.0009631082648411393 | 0.0010259817354381084 | 0.0010138684883713722 | 0.0008204580517485738 | 0.0010513031156733632 | 0.0007360710296779871 | 0.0007574781775474548 | 0.0005767665570601821 | 0.0007102822419255972 | 0.0007308365311473608 | 0.0008182000601664186 | 0.0006754992646165192 | 0.0007041206117719412 | 0.0007537654601037502 | 0.0008757005562074482 | 0.0006596497842110693 | 0.0008084759465418756 \n",
      "Epoch 13\n",
      " ==========\n",
      "| 0.0008549088961444795 | 0.0006918086437508464 | 0.000812344893347472 | 0.0007321742596104741 | 0.000695667346008122 | 0.0006129742250777781 | 0.0007330311345867813 | 0.000722961500287056 | 0.0008135676616802812 | 0.000888794893398881 | 0.0007490377174690366 | 0.0008533140062354505 | 0.0006752818007953465 | 0.0008740058401599526 | 0.0010240406263619661 | 0.000761682924348861 | 0.0006992567796260118 | 0.0007870260742492974 | 0.0007956625777296722 | 0.0007106295670382679 \n",
      "Epoch 14\n",
      " ==========\n",
      "| 0.0008252238039858639 | 0.0008396242628805339 | 0.0007422277121804655 | 0.0008249516249634326 | 0.000806034542620182 | 0.0006558324676007032 | 0.0006842727307230234 | 0.0007422402850352228 | 0.000681002507917583 | 0.0006443223101086915 | 0.000693146837875247 | 0.0008245367789641023 | 0.0008066142909228802 | 0.000647432345431298 | 0.0006613351870328188 | 0.00068618607474491 | 0.0007004509097896516 | 0.0010348394280299544 | 0.0007865719962865114 | 0.000720634707249701 \n",
      "Epoch 15\n",
      " ==========\n",
      "| 0.0008361366344615817 | 0.000984283396974206 | 0.0008089689654298127 | 0.000596014317125082 | 0.000805515912361443 | 0.0008246877696365118 | 0.0006918910657986999 | 0.0005947795580141246 | 0.0006735277711413801 | 0.0007846154039725661 | 0.000596389698330313 | 0.0007667986210435629 | 0.0005933545180596411 | 0.0009173323051072657 | 0.0006840128335170448 | 0.0005572388181462884 | 0.0010413072304800153 | 0.000736128247808665 | 0.0007187946466729045 | 0.0007100331131368876 \n",
      "Epoch 16\n",
      " ==========\n",
      "| 0.0005940562696196139 | 0.0008325878297910094 | 0.0006021112785674632 | 0.0007362756878137589 | 0.0006410093628801405 | 0.0006705221021547914 | 0.0008009694865904748 | 0.000721669290214777 | 0.0008094269433058798 | 0.0006032705423422158 | 0.0005834059556946158 | 0.0007155750645324588 | 0.000877422105986625 | 0.0005899068783037364 | 0.0007960349903441966 | 0.0008330956334248185 | 0.0007164158159866929 | 0.0007085088291205466 | 0.000987064209766686 | 0.0007331452216021717 \n",
      "Epoch 17\n",
      " ==========\n",
      "| 0.000759524351451546 | 0.0008764141821302474 | 0.0006818851106800139 | 0.0007546640117652714 | 0.0008043384877964854 | 0.0008801909280009568 | 0.0007708983612246811 | 0.0008724777144379914 | 0.0007352741085924208 | 0.0007194260251708329 | 0.0006466358317993581 | 0.0006708716391585767 | 0.0008223723853006959 | 0.0005959596019238234 | 0.0006967789377085865 | 0.0005175578990019858 | 0.0008021094836294651 | 0.0008099705446511507 | 0.0006416559917852283 | 0.0005402549868449569 \n",
      "Epoch 18\n",
      " ==========\n",
      "| 0.0009225850226357579 | 0.0006697186618112028 | 0.0005090109189040959 | 0.0006407570326700807 | 0.000938032113481313 | 0.0007189047173596919 | 0.0008289948455058038 | 0.0007268360350281 | 0.0004793224798049778 | 0.0007786555797792971 | 0.0005040777614340186 | 0.0005986810429021716 | 0.0006029009236954153 | 0.0007311383378691971 | 0.0006648842245340347 | 0.0007485367823392153 | 0.0008378624334000051 | 0.0006130111869424582 | 0.0007372669060714543 | 0.0006967139197513461 \n",
      "Epoch 19\n",
      " ==========\n",
      "| 0.0005470534088090062 | 0.0006093189585953951 | 0.0007100281654857099 | 0.0007497024489566684 | 0.0007156162755563855 | 0.0008925138972699642 | 0.0006580130429938436 | 0.0006315959035418928 | 0.0005744561785832047 | 0.0007097508641891181 | 0.0006169475382193923 | 0.0006766589940525591 | 0.000719747506082058 | 0.0007346187485381961 | 0.0007881111232563853 | 0.0005297712632454932 | 0.0005663787596859038 | 0.0007018814794719219 | 0.0005975382518954575 | 0.0006360699771903455 \n",
      "Epoch 20\n",
      " ==========\n",
      "| 0.0005628442740999162 | 0.0007013155263848603 | 0.0007201730040833354 | 0.0006945536006242037 | 0.000674764218274504 | 0.0007850226829759777 | 0.0005998248234391212 | 0.0006347952876240015 | 0.0007898149196989834 | 0.0006475320551544428 | 0.0006056861020624638 | 0.0008093080832622945 | 0.0005677980370819569 | 0.000629620102699846 | 0.0006010764627717435 | 0.0007207632879726589 | 0.0006263320683501661 | 0.000642644998151809 | 0.0005384110845625401 | 0.0007322322344407439 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_470352/196645146.py:172: RuntimeWarning:\n",
      "\n",
      "More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21\n",
      " ==========\n",
      "| 0.0006795972585678101 | 0.0007668048492632806 | 0.0006122965714894235 | 0.0005533007206395268 | 0.0006190292770043015 | 0.0006986210937611759 | 0.00044564588461071253 | 0.0005673359264619648 | 0.0007567541324533522 | 0.0007609880995005369 | 0.0005900044343434274 | 0.0005409957375377417 | 0.0007063096854835749 | 0.0006865128525532782 | 0.0005418523214757442 | 0.0006235397304408252 | 0.0005493772332556546 | 0.0007322803139686584 | 0.0006749627063982189 | 0.0005314092850312591 \n",
      "Epoch 22\n",
      " ==========\n",
      "| 0.0005490108160302043 | 0.0005660774768330157 | 0.0004939573700539768 | 0.0009291477617807686 | 0.000747477519325912 | 0.0006464893813244998 | 0.0007531027076765895 | 0.0005288303946144879 | 0.0006599028129130602 | 0.0007405333453789353 | 0.0006725339335389435 | 0.0004607127630151808 | 0.0006419484852813184 | 0.0006981276674196124 | 0.000543287256732583 | 0.0006577002932317555 | 0.0006122515187598765 | 0.0005403097602538764 | 0.0005631579551845789 | 0.0005978788831271231 \n",
      "Epoch 23\n",
      " ==========\n",
      "| 0.0006835901294834912 | 0.0005512897041626275 | 0.000657208904158324 | 0.0006333575001917779 | 0.0006471718661487103 | 0.0006225744727998972 | 0.0005925697041675448 | 0.000547376403119415 | 0.0007168475422076881 | 0.0004919290076941252 | 0.0006703223334625363 | 0.000563613255508244 | 0.0005891140317544341 | 0.0006728189182467759 | 0.0006644948734901845 | 0.00075106619624421 | 0.0007007964886724949 | 0.000733427528757602 | 0.0005090719787403941 | 0.000766355253290385 \n",
      "Epoch 24\n",
      " ==========\n",
      "| 0.0006051347008906305 | 0.0006919188890606165 | 0.0007480982458218932 | 0.0006306918803602457 | 0.0007337384158745408 | 0.0006729571614414454 | 0.0006216434412635863 | 0.00045869912719354033 | 0.0005609092186205089 | 0.0005296529270708561 | 0.0006072120741009712 | 0.0007250125054270029 | 0.000732695625629276 | 0.0006641645450145006 | 0.0007669231854379177 | 0.0005479390965774655 | 0.000735949317459017 | 0.0004398150194901973 | 0.0005394562031142414 | 0.0005950831109657884 \n",
      "Epoch 25\n",
      " ==========\n",
      "| 0.0007184446440078318 | 0.0005092123174108565 | 0.0005003096885047853 | 0.0005620882147923112 | 0.0005626279162243009 | 0.0005916249356232584 | 0.0005673713167198002 | 0.0006578032625839114 | 0.00047852430725470185 | 0.000574217177927494 | 0.0006914531113579869 | 0.000560285581741482 | 0.00045462619164027274 | 0.0005009788437746465 | 0.0008263416239060462 | 0.0005709616234526038 | 0.0004985167761333287 | 0.0005326929967850447 | 0.0005984179442748427 | 0.0007412864943034947 \n",
      "Epoch 26\n",
      " ==========\n",
      "| 0.0005769493291154504 | 0.00046683396794833243 | 0.0006052753305993974 | 0.0005239337333478034 | 0.0006816268432885408 | 0.000479064357932657 | 0.0005968307377770543 | 0.0006039297441020608 | 0.0005546095781028271 | 0.0005831862799823284 | 0.00046856183325871825 | 0.00045767167466692626 | 0.0005370887811295688 | 0.0005711468402296305 | 0.0006105510983616114 | 0.0007099338108673692 | 0.0005784507375210524 | 0.0005509559996426105 | 0.0004563354013953358 | 0.0005522388382814825 \n",
      "Epoch 27\n",
      " ==========\n",
      "| 0.000509949866682291 | 0.00046205829130485654 | 0.000591025804169476 | 0.0006095496355555952 | 0.0005677034496329725 | 0.0006263470277190208 | 0.0005920890835113823 | 0.0005770605057477951 | 0.0005451823817566037 | 0.0005213210242800415 | 0.0007190905162133276 | 0.000714989670086652 | 0.0006073320400901139 | 0.0005434628110378981 | 0.0007953897584229708 | 0.0005193378892727196 | 0.0005488634924404323 | 0.0006176159949973226 | 0.0005326866521500051 | 0.0005129877245053649 \n",
      "Epoch 28\n",
      " ==========\n",
      "| 0.0006379932165145874 | 0.0005425021518021822 | 0.0005855372874066234 | 0.0004920545034110546 | 0.0004580891109071672 | 0.000576814403757453 | 0.0005205451161600649 | 0.0006793986540287733 | 0.0005590604851022363 | 0.0005261372425593436 | 0.0005403468967415392 | 0.0005551749491132796 | 0.0005407151184044778 | 0.0004985775449313223 | 0.0005137685802765191 | 0.0005856418865732849 | 0.0003795319062191993 | 0.0004929373972117901 | 0.0006458595162257552 | 0.0005301035707816482 \n",
      "Epoch 29\n",
      " ==========\n",
      "| 0.0006125304498709738 | 0.0005441642715595663 | 0.0005684589268639684 | 0.0004600938700605184 | 0.0004103536775801331 | 0.0004945253604091704 | 0.0006673016468994319 | 0.0004947123816236854 | 0.0005442036781460047 | 0.0005806390545330942 | 0.0005790807772427797 | 0.0005415775813162327 | 0.0002382269303780049 | 0.0004447359824553132 | 0.0005328202969394624 | 0.0005523903528228402 | 0.0004907370312139392 | 0.0006324898567982018 | 0.0006166762905195355 | 0.0006062066531740129 \n",
      "Epoch 30\n",
      " ==========\n",
      "| 0.0006638425984419882 | 0.0005164094036445022 | 0.00046875199768692255 | 0.000528130098246038 | 0.0005958746769465506 | 0.0006333240307867527 | 0.0005547298933379352 | 0.0005516023957170546 | 0.000505087140481919 | 0.000555256090592593 | 0.0004576215287670493 | 0.00047319993609562516 | 0.0005913898348808289 | 0.0005745812668465078 | 0.0004261397698428482 | 0.0003838247212115675 | 0.0005439529777504504 | 0.00030845022411085665 | 0.0005215226556174457 | 0.0005651998217217624 \n",
      "Epoch 31\n",
      " ==========\n",
      "| 0.0005055016954429448 | 0.00046704604756087065 | 0.0003408411575946957 | 0.0005854624323546886 | 0.0004602606932166964 | 0.0005148471682332456 | 0.0005011804169043899 | 0.00045380971278063953 | 0.0005866129067726433 | 0.0004976792261004448 | 0.0005875580827705562 | 0.0006260746740736067 | 0.0009688515565358102 | 0.0005188125651329756 | 0.0005032122717238963 | 0.0006282305694185197 | 0.0008721506455913186 | 0.0006007121992297471 | 0.0004866774834226817 | 0.0013041086494922638 \n",
      "Epoch 32\n",
      " ==========\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the system, may be re-used between training and testing.\n",
    "model_system = INRSystem(accelerator=\"gpu\", devices=1, precision=32)\n",
    "\n",
    "if \"in_channels\" not in p.encoder:\n",
    "    in_channels = int(train_dataset[0][\"lr_dwi\"].shape[0])\n",
    "else:\n",
    "    in_channels = p.encoder.in_channels\n",
    "\n",
    "model_system.run(\n",
    "    p.train.max_epochs,\n",
    "    p.train.batch_size,\n",
    "    in_channels=in_channels,\n",
    "    pred_channels=p.decoder.out_features,\n",
    "    encoder_kwargs=p.encoder.to_dict(),\n",
    "    decoder_kwargs=p.decoder.to_dict(),\n",
    "    train_dataset=train_dataset,\n",
    "    dataloader_kwargs={\n",
    "        \"num_workers\": 10,\n",
    "        \"persistent_workers\": True,\n",
    "        \"prefetch_factor\": 3,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "705e455f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-04T13:33:47.884971Z",
     "iopub.status.busy": "2022-11-04T13:33:47.884348Z",
     "iopub.status.idle": "2022-11-04T13:33:48.264695Z",
     "shell.execute_reply": "2022-11-04T13:33:48.263265Z",
     "shell.execute_reply.started": "2022-11-04T13:33:47.884921Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(tmp_res_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_losses.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(losses\u001b[38;5;241m.\u001b[39mstep[\u001b[38;5;241m50\u001b[39m:], losses\u001b[38;5;241m.\u001b[39mloss[\u001b[38;5;241m50\u001b[39m:], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "losses = pd.read_csv(tmp_res_dir / \"train_losses.csv\")\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(losses.step[50:], losses.loss[50:], label=\"loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(losses.step[50:], losses.encoder_grad_norm[50:], label=\"encoder grad norm\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(losses.step[50:], losses.decoder_grad_norm[50:], label=\"decoder grad norm\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd642c35",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=100)\n",
    "plt.plot(losses.step[500:], losses.loss[500:], label=\"loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(losses.step[500:], losses.encoder_grad_norm[500:], label=\"encoder grad norm\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(losses.step[500:], losses.decoder_grad_norm[500:], label=\"decoder grad norm\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fd85a0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1d5c6b6",
   "metadata": {},
   "source": [
    "## Testing & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55056e76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dbcc36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python [conda env:miniconda-pitn2]",
   "language": "python",
   "name": "conda-env-miniconda-pitn2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc-autonumbering": true,
  "toc-showtags": true,
  "vscode": {
   "interpreter": {
    "hash": "f41faa2479836806c9664d670a156675ad0f09912fd4b0aed749f41e3cac86f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
