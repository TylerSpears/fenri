{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Continuous-Space Super-Resolution of fODFs in Diffusion MRI\n",
    "\n",
    "Code by:\n",
    "\n",
    "Tyler Spears - tas6hh@virginia.edu\n",
    "\n",
    "Dr. Tom Fletcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T17:40:12.031797Z",
     "iopub.status.busy": "2022-10-24T17:40:12.031146Z",
     "iopub.status.idle": "2022-10-24T17:40:12.094649Z",
     "shell.execute_reply": "2022-10-24T17:40:12.093315Z",
     "shell.execute_reply.started": "2022-10-24T17:40:12.031749Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Automatically re-import project-specific modules.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# imports\n",
    "import collections\n",
    "import functools\n",
    "import io\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "import copy\n",
    "import pdb\n",
    "import inspect\n",
    "import random\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import typing\n",
    "import zipfile\n",
    "import tempfile\n",
    "\n",
    "import dotenv\n",
    "\n",
    "# visualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data management libraries.\n",
    "import nibabel as nib\n",
    "import nibabel.processing\n",
    "from natsort import natsorted\n",
    "from pprint import pprint as ppr\n",
    "from box import Box\n",
    "\n",
    "# Computation & ML libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchio\n",
    "from pytorch_lightning.lite import LightningLite\n",
    "import monai\n",
    "import einops\n",
    "import torchinfo\n",
    "import skimage\n",
    "\n",
    "import pitn\n",
    "\n",
    "plt.rcParams.update({\"figure.autolayout\": True})\n",
    "plt.rcParams.update({\"figure.facecolor\": [1.0, 1.0, 1.0, 1.0]})\n",
    "\n",
    "# Set print options for ndarrays/tensors.\n",
    "np.set_printoptions(suppress=True, threshold=100, linewidth=88)\n",
    "torch.set_printoptions(sci_mode=False, threshold=100, linewidth=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T17:40:12.622391Z",
     "iopub.status.busy": "2022-10-24T17:40:12.621862Z",
     "iopub.status.idle": "2022-10-24T17:40:14.199862Z",
     "shell.execute_reply": "2022-10-24T17:40:14.198757Z",
     "shell.execute_reply.started": "2022-10-24T17:40:12.622346Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update notebook's environment variables with direnv.\n",
    "# This requires the python-dotenv package, and direnv be installed on the system\n",
    "# This will not work on Windows.\n",
    "# NOTE: This is kind of hacky, and not necessarily safe. Be careful...\n",
    "# Libraries needed on the python side:\n",
    "# - os\n",
    "# - subprocess\n",
    "# - io\n",
    "# - dotenv\n",
    "\n",
    "# Form command to be run in direnv's context. This command will print out\n",
    "# all environment variables defined in the subprocess/sub-shell.\n",
    "command = \"direnv exec {} /usr/bin/env\".format(os.getcwd())\n",
    "# Run command in a new subprocess.\n",
    "proc = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, cwd=os.getcwd())\n",
    "# Store and format the subprocess' output.\n",
    "proc_out = proc.communicate()[0].strip().decode(\"utf-8\")\n",
    "# Use python-dotenv to load the environment variables by using the output of\n",
    "# 'direnv exec ...' as a 'dummy' .env file.\n",
    "dotenv.load_dotenv(stream=io.StringIO(proc_out), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T17:40:14.238892Z",
     "iopub.status.busy": "2022-10-24T17:40:14.238626Z",
     "iopub.status.idle": "2022-10-24T17:40:14.267386Z",
     "shell.execute_reply": "2022-10-24T17:40:14.266259Z",
     "shell.execute_reply.started": "2022-10-24T17:40:14.238878Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch setup\n",
    "# allow for CUDA usage, if available\n",
    "if torch.cuda.is_available():\n",
    "    # Pick only one device for the default, may use multiple GPUs for training later.\n",
    "    if \"CUDA_PYTORCH_DEVICE_IDX\" in os.environ.keys():\n",
    "        dev_idx = int(os.environ[\"CUDA_PYTORCH_DEVICE_IDX\"])\n",
    "    else:\n",
    "        dev_idx = 0\n",
    "    device = torch.device(f\"cuda:{dev_idx}\")\n",
    "    print(\"CUDA Device IDX \", dev_idx)\n",
    "    torch.cuda.set_device(device)\n",
    "    print(\"CUDA Current Device \", torch.cuda.current_device())\n",
    "    print(\"CUDA Device properties: \", torch.cuda.get_device_properties(device))\n",
    "    # Activate cudnn benchmarking to optimize convolution algorithm speed.\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        print(\"CuDNN convolution optimization enabled.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# keep device as the cpu\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T17:40:15.892013Z",
     "iopub.status.busy": "2022-10-24T17:40:15.891483Z",
     "iopub.status.idle": "2022-10-24T17:40:16.019585Z",
     "shell.execute_reply": "2022-10-24T17:40:16.018122Z",
     "shell.execute_reply.started": "2022-10-24T17:40:15.891968Z"
    },
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr cap\n",
    "# Capture output and save to log. Needs to be at the *very first* line of the cell.\n",
    "# Watermark\n",
    "%load_ext watermark\n",
    "%watermark --author \"Tyler Spears\" --updated --iso8601  --python --machine --iversions --githash\n",
    "if torch.cuda.is_available():\n",
    "    # GPU information\n",
    "    try:\n",
    "        gpu_info = pitn.utils.system.get_gpu_specs()\n",
    "        print(gpu_info)\n",
    "    except NameError:\n",
    "        print(\"CUDA Version: \", torch.version.cuda)\n",
    "else:\n",
    "    print(\"CUDA not in use, falling back to CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T17:40:16.554104Z",
     "iopub.status.busy": "2022-10-24T17:40:16.553534Z",
     "iopub.status.idle": "2022-10-24T17:40:16.600832Z",
     "shell.execute_reply": "2022-10-24T17:40:16.599980Z",
     "shell.execute_reply.started": "2022-10-24T17:40:16.554055Z"
    },
    "tags": [
     "keep_output",
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "# cap is defined in an ipython magic command\n",
    "print(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment & Parameters Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T17:40:17.982369Z",
     "iopub.status.busy": "2022-10-24T17:40:17.981837Z",
     "iopub.status.idle": "2022-10-24T17:40:18.032189Z",
     "shell.execute_reply": "2022-10-24T17:40:18.031367Z",
     "shell.execute_reply.started": "2022-10-24T17:40:17.982323Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = Box(default_box=True)\n",
    "# Experiment defaults, can be overridden in a config file.\n",
    "\n",
    "# General experiment-wide params\n",
    "###############################################\n",
    "p.experiment_name = \"sr_debug\"\n",
    "p.override_experiment_name = False\n",
    "p.results_dir = \"/data/srv/outputs/pitn/results/runs\"\n",
    "p.tmp_results_dir = \"/data/srv/outputs/pitn/results/tmp\"\n",
    "p.train_val_test_split_file = random.choice(\n",
    "    list(Path(\"./data_splits\").glob(\"HCP*train-val-test_split*.csv\"))\n",
    ")\n",
    "###############################################\n",
    "p.train = dict(\n",
    "    in_patch_size=(32, 32, 32),\n",
    "    batch_size=1,\n",
    "    samples_per_subj_per_epoch=2,\n",
    "    max_epochs=2,\n",
    "    loss=\"mse\",\n",
    ")\n",
    "\n",
    "# Network/model parameters.\n",
    "p.encoder = dict(\n",
    "    interior_channels=75,\n",
    "    # (number of SH orders (l) + 1) * X that is as close to 100 as possible.\n",
    "    out_channels=16 * 6,\n",
    "    n_res_units=3,\n",
    "    n_dense_units=3,\n",
    "    activate_fn=\"elu\",\n",
    ")\n",
    "p.decoder = dict(\n",
    "    n_coord_features=3,\n",
    "    n_context_features=p.encoder.out_channels,\n",
    "    n_context_groups=6,\n",
    "    out_features=45,\n",
    "    internal_features=126,\n",
    "    n_layers=5,\n",
    "    activate_fn=\"elu\",\n",
    ")\n",
    "\n",
    "# If a config file exists, override the defaults with those values.\n",
    "try:\n",
    "    if \"PITN_CONFIG\" in os.environ.keys():\n",
    "        config_fname = Path(os.environ[\"PITN_CONFIG\"])\n",
    "    else:\n",
    "        config_fname = pitn.utils.system.get_file_glob_unique(Path(\".\"), r\"config.*\")\n",
    "    f_type = config_fname.suffix.casefold()\n",
    "    if f_type in {\".yaml\", \".yml\"}:\n",
    "        f_params = Box.from_yaml(filename=config_fname)\n",
    "    elif f_type == \".json\":\n",
    "        f_params = Box.from_json(filename=config_fname)\n",
    "    elif f_type == \".toml\":\n",
    "        f_params = Box.from_toml(filename=config_fname)\n",
    "    else:\n",
    "        raise RuntimeError()\n",
    "\n",
    "    p.merge_update(f_params)\n",
    "\n",
    "except:\n",
    "    print(\"WARNING: Config file not loaded\")\n",
    "    pass\n",
    "\n",
    "# Remove the default_box behavior now that params have been fully read in.\n",
    "_p = Box(default_box=False)\n",
    "_p.merge_update(p)\n",
    "p = _p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T17:40:18.522043Z",
     "iopub.status.busy": "2022-10-24T17:40:18.521484Z",
     "iopub.status.idle": "2022-10-24T17:40:18.572969Z",
     "shell.execute_reply": "2022-10-24T17:40:18.571865Z",
     "shell.execute_reply.started": "2022-10-24T17:40:18.521995Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tvt_split = pd.read_csv(p.train_val_test_split_file)\n",
    "p.train.subj_ids = tvt_split[tvt_split.split == \"train\"].subj_id.tolist()\n",
    "p.val = dict()\n",
    "p.val.subj_ids = tvt_split[tvt_split.split == \"val\"].subj_id.tolist()\n",
    "p.test = dict()\n",
    "p.test.subj_ids = tvt_split[tvt_split.split == \"test\"].subj_id.tolist()\n",
    "\n",
    "# Ensure that no test subj ids are in either the training or validation sets.\n",
    "# However, we can have overlap between training and validation.\n",
    "assert len(set(p.train.subj_ids) & set(p.test.subj_ids)) == 0\n",
    "assert len(set(p.val.subj_ids) & set(p.test.subj_ids)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T17:40:20.360039Z",
     "iopub.status.busy": "2022-10-24T17:40:20.359448Z",
     "iopub.status.idle": "2022-10-24T17:40:20.408891Z",
     "shell.execute_reply": "2022-10-24T17:40:20.408024Z",
     "shell.execute_reply.started": "2022-10-24T17:40:20.359989Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hcp_full_res_data_dir = Path(\"/data/srv/data/pitn/hcp\")\n",
    "hcp_full_res_fodf_dir = Path(\"/data/srv/outputs/pitn/hcp/full-res/fodf\")\n",
    "hcp_low_res_data_dir = Path(\"/data/srv/outputs/pitn/hcp/downsample/scale-2.00mm/vol\")\n",
    "\n",
    "assert hcp_full_res_data_dir.exists()\n",
    "assert hcp_full_res_fodf_dir.exists()\n",
    "assert hcp_low_res_data_dir.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T17:40:21.120965Z",
     "iopub.status.busy": "2022-10-24T17:40:21.120298Z",
     "iopub.status.idle": "2022-10-24T17:40:21.173205Z",
     "shell.execute_reply": "2022-10-24T17:40:21.172657Z",
     "shell.execute_reply.started": "2022-10-24T17:40:21.120915Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_tfs = list()\n",
    "# Load images\n",
    "vol_reader = monai.data.NibabelReader(as_closest_canonical=True, dtype=np.float32)\n",
    "load_tfs.append(\n",
    "    monai.transforms.LoadImaged(\n",
    "        (\"lr_dwi\", \"fodf\", \"lr_mask\", \"mask\", \"fivett\"),\n",
    "        reader=vol_reader,\n",
    "        dtype=np.float32,\n",
    "        meta_key_postfix=\"meta\",\n",
    "        ensure_channel_first=True,\n",
    "        simple_keys=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "grad_file_reader = monai.transforms.Lambdad(\n",
    "    (\"lr_bval\", \"lr_bvec\"), lambda f: np.loadtxt(str(f)), overwrite=True\n",
    ")\n",
    "load_tfs.append(grad_file_reader)\n",
    "\n",
    "# Data conversion\n",
    "load_tfs.append(\n",
    "    monai.transforms.ToTensord(\n",
    "        (\"lr_dwi\", \"fodf\", \"lr_mask\", \"mask\", \"fivett\"), track_meta=True\n",
    "    )\n",
    ")\n",
    "load_tfs.append(monai.transforms.ToTensord((\"lr_bval\", \"lr_bvec\"), track_meta=False))\n",
    "load_tfs.append(\n",
    "    monai.transforms.CastToTyped((\"lr_mask\", \"mask\", \"fivett\"), dtype=torch.uint8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T17:40:21.627943Z",
     "iopub.status.busy": "2022-10-24T17:40:21.627395Z",
     "iopub.status.idle": "2022-10-24T17:40:21.713150Z",
     "shell.execute_reply": "2022-10-24T17:40:21.711798Z",
     "shell.execute_reply.started": "2022-10-24T17:40:21.627896Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Random sampling transformations.\n",
    "sample_tfs = list()\n",
    "\n",
    "# Dilate the lr mask by 1/2 patch shape to allow for uniform sampling of the patch\n",
    "# centers.\n",
    "dilate_tf = pitn.transforms.BinaryDilated(\n",
    "    [\"lr_mask\"],\n",
    "    footprint=skimage.morphology.ball(max(p.train.in_patch_size) // 4),\n",
    "    write_to_keys=[\"lr_sampling_mask\"],\n",
    ")\n",
    "rescale_tf = monai.transforms.Lambdad(\n",
    "    \"lr_sampling_mask\", lambda m: m / torch.sum(m, (1, 2, 3), keepdim=True)\n",
    ")\n",
    "sample_tfs.append(dilate_tf)\n",
    "sample_tfs.append(rescale_tf)\n",
    "\n",
    "# Randomly crop ROIs from the LR input and match to the same spatial coordinates from\n",
    "# the full-res fODFs.\n",
    "# Save the low-res affine as its own field.\n",
    "def extract_affine(d: dict, src_vol_key, write_key: str):\n",
    "    aff = d[src_vol_key].affine\n",
    "    d[write_key] = torch.clone(aff).to(torch.float32)\n",
    "    return d\n",
    "\n",
    "\n",
    "sample_tfs.append(\n",
    "    functools.partial(\n",
    "        extract_affine, src_vol_key=\"lr_dwi\", write_key=\"affine_lrvox2acpc\"\n",
    "    )\n",
    ")\n",
    "\n",
    "sample_crop_tf = monai.transforms.RandWeightedCropd(\n",
    "    [\"lr_dwi\", \"lr_mask\"],\n",
    "    \"lr_sampling_mask\",\n",
    "    spatial_size=p.train.in_patch_size,\n",
    "    num_samples=p.train.samples_per_subj_per_epoch,\n",
    "    # num_samples=2,\n",
    ")\n",
    "sample_tfs.append(sample_crop_tf)\n",
    "\n",
    "# print_tf = monai.transforms.Lambdad(\n",
    "#     [\"lr_dwi\", \"lr_mask\"], lambda x: print(type(x), x.shape), overwrite=False\n",
    "# )\n",
    "# sample_tfs.append(print_tf)\n",
    "\n",
    "# ~~Randomly shuffle the order of DWIs and the associated bvals/bvecs.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T17:40:22.237644Z",
     "iopub.status.busy": "2022-10-24T17:40:22.236863Z",
     "iopub.status.idle": "2022-10-24T17:40:22.299009Z",
     "shell.execute_reply": "2022-10-24T17:40:22.297966Z",
     "shell.execute_reply.started": "2022-10-24T17:40:22.237596Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transforms for extracting features for the network.\n",
    "feat_tfs = list()\n",
    "\n",
    "\n",
    "def extract_lr_patch_info(lr_dwi, affine_lrvox2acpc, patch_size: tuple):\n",
    "    # Extract low-resolution input information\n",
    "    patch_center_lrvox = torch.clone(\n",
    "        lr_dwi.meta[\"crop_center\"].as_tensor().to(torch.int)\n",
    "    )\n",
    "    affine_lrvox2acpc = torch.as_tensor(affine_lrvox2acpc).to(torch.float32)\n",
    "    vox_extent = list()\n",
    "    for patch_dim_len, patch_center in zip(patch_size, patch_center_lrvox):\n",
    "        half_patch_start = math.ceil(patch_dim_len // 2)\n",
    "        half_patch_end = math.floor(patch_dim_len // 2)\n",
    "        vox_extent.append(\n",
    "            torch.arange(\n",
    "                patch_center - half_patch_start, patch_center + half_patch_end\n",
    "            ).to(patch_center_lrvox)\n",
    "        )\n",
    "    vox_extent = torch.stack(vox_extent, dim=-1)\n",
    "    patch_extent_lrvox = vox_extent\n",
    "    # Calculate acpc-space coordinates of the vox extent.\n",
    "    acpc_extent = (affine_lrvox2acpc[:3, :3] @ vox_extent.T.to(affine_lrvox2acpc)) + (\n",
    "        affine_lrvox2acpc[:3, 3:4]\n",
    "    )\n",
    "    acpc_extent = acpc_extent.T\n",
    "    lr_patch_extent_acpc = acpc_extent\n",
    "\n",
    "    return dict(\n",
    "        patch_center_lrvox=patch_center_lrvox,\n",
    "        lr_patch_extent_acpc=lr_patch_extent_acpc,\n",
    "        patch_extent_lrvox=patch_extent_lrvox,\n",
    "    )\n",
    "\n",
    "\n",
    "extract_lr_patch_meta_tf = monai.transforms.adaptor(\n",
    "    functools.partial(extract_lr_patch_info, patch_size=p.train.in_patch_size),\n",
    "    outputs={\n",
    "        \"patch_center_lrvox\": \"patch_center_lrvox\",\n",
    "        \"lr_patch_extent_acpc\": \"lr_patch_extent_acpc\",\n",
    "        \"patch_extent_lrvox\": \"patch_extent_lrvox\",\n",
    "    },\n",
    ")\n",
    "feat_tfs.append(extract_lr_patch_meta_tf)\n",
    "\n",
    "\n",
    "def extract_full_res_patch_info(\n",
    "    fodf,\n",
    "    lr_patch_extent_acpc,\n",
    "):\n",
    "    # Extract full-resolution information.\n",
    "    affine_vox2acpc = torch.clone(torch.as_tensor(fodf.affine, dtype=torch.float32))\n",
    "    affine_acpc2vox = torch.inverse(affine_vox2acpc)\n",
    "    lr_patch_extent_acpc = lr_patch_extent_acpc.to(affine_acpc2vox)\n",
    "    patch_extent_vox = (affine_acpc2vox[:3, :3] @ lr_patch_extent_acpc.T) + (\n",
    "        affine_acpc2vox[:3, 3:4]\n",
    "    )\n",
    "\n",
    "    patch_extent_vox = patch_extent_vox.T\n",
    "    # Calculate the spatial bounds of the full-res patch to be *within* the coordinates\n",
    "    # of the low-res input, otherwise the network cannot be given distance-weighted\n",
    "    # inputs for the borders of the full-res patch.\n",
    "    l_bound = torch.ceil(patch_extent_vox.min(dim=0).values).to(torch.int)\n",
    "    u_bound = torch.floor(patch_extent_vox.max(dim=0).values).to(torch.int)\n",
    "    fr_patch_shape = u_bound - l_bound\n",
    "    if (fr_patch_shape != fr_patch_shape.max()).any():\n",
    "        target_size = fr_patch_shape.max()\n",
    "        vol_shape = torch.tensor(fodf.shape[1:])\n",
    "        for dim, dim_size in enumerate(fr_patch_shape):\n",
    "            if dim_size != target_size:\n",
    "                diff = target_size - dim_size\n",
    "                # Try to increase the upper bound first.\n",
    "                if u_bound[dim] + diff <= vol_shape[dim]:\n",
    "                    u_bound[dim] = u_bound[dim] + diff\n",
    "                elif l_bound[dim] - diff >= 0:\n",
    "                    l_bound[dim] = l_bound[dim] - diff\n",
    "                else:\n",
    "                    raise RuntimeError(\n",
    "                        \"ERROR: Non-isotropic full-res patch shape\", f\"{fr_patch_shape}\"\n",
    "                    )\n",
    "        fr_patch_shape = u_bound - l_bound\n",
    "    # vol_shape = torch.tensor(fodf.shape[1:])\n",
    "    # if (fr_patch_shape != fr_patch_shape.max()).any():\n",
    "    #     raise RuntimeError(\n",
    "    #         \"ERROR: Expected patch to be same size in all dims, got FR patch\",\n",
    "    #         f\"shape {tuple(fr_patch_shape)}\",\n",
    "    #     )\n",
    "    # if np.asarray(u_bound > vol_shape).any() or np.asarray(l_bound < 0).any():\n",
    "    #     raise RuntimeError(\n",
    "    #         \"ERROR: Patch spatial extent out of bounds,\",\n",
    "    #         f\"lower bound {l_bound}, upper bound {u_bound}, limits {vol_shape}\",\n",
    "    #     )\n",
    "\n",
    "    # Store the vox upper and lower bounds now for later patch extraction from the full-\n",
    "    # res fodf and mask (*much* faster to index into a tensor with a range() than each\n",
    "    # individual index).\n",
    "    fr_patch_vox_lu_bound = torch.stack([l_bound, u_bound], dim=-1)\n",
    "    extent_vox_l = list()\n",
    "    for l, u in zip(l_bound.cpu().tolist(), u_bound.cpu().tolist()):\n",
    "        extent_vox_l.append(torch.arange(l, u).to(torch.int))\n",
    "    patch_extent_vox = torch.stack(extent_vox_l, dim=-1).to(torch.int32)\n",
    "\n",
    "    fr_patch_extent_acpc = (\n",
    "        affine_vox2acpc[:3, :3] @ patch_extent_vox.T.to(affine_vox2acpc)\n",
    "    ) + (affine_vox2acpc[:3, 3:4])\n",
    "    fr_patch_extent_acpc = fr_patch_extent_acpc.T\n",
    "    fr_patch_extent_acpc = fr_patch_extent_acpc\n",
    "\n",
    "    return dict(\n",
    "        affine_vox2acpc=affine_vox2acpc,\n",
    "        fr_patch_vox_lu_bound=fr_patch_vox_lu_bound,\n",
    "        fr_patch_extent_acpc=fr_patch_extent_acpc,\n",
    "    )\n",
    "\n",
    "\n",
    "extract_full_res_patch_meta_tf = monai.transforms.adaptor(\n",
    "    extract_full_res_patch_info,\n",
    "    outputs={\n",
    "        \"affine_vox2acpc\": \"affine_vox2acpc\",\n",
    "        \"fr_patch_vox_lu_bound\": \"fr_patch_vox_lu_bound\",\n",
    "        \"fr_patch_extent_acpc\": \"fr_patch_extent_acpc\",\n",
    "    },\n",
    ")\n",
    "feat_tfs.append(extract_full_res_patch_meta_tf)\n",
    "\n",
    "# Slice into fodf and full-res mask with the LR patch's spatial extent.\n",
    "class CropSamplefODFMask:\n",
    "    def __init__(self, fr_patch_vox_lu_bound_key, vol_key_map: dict):\n",
    "        self._lu_key = fr_patch_vox_lu_bound_key\n",
    "        self._vol_key_map = vol_key_map\n",
    "\n",
    "    def __call__(self, data_dict: dict):\n",
    "        # Find start and end of the ROI\n",
    "        lu_bound = data_dict[self._lu_key]\n",
    "        roi_start = lu_bound[:, 0]\n",
    "        roi_end = lu_bound[:, 1]\n",
    "        # Crop vols with the SpatialCrop transform.\n",
    "        cropper = monai.transforms.SpatialCropd(\n",
    "            list(self._vol_key_map.keys()), roi_start=roi_start, roi_end=roi_end\n",
    "        )\n",
    "        to_crop = {v: data_dict[v] for v in self._vol_key_map.keys()}\n",
    "        cropped = cropper(to_crop)\n",
    "        # If crops are are cube shaped, then at least one of the roi indices is either\n",
    "        # 1) < 0, or 2) > bounds of the entire volume. Check for that.\n",
    "        sample_vol = list(cropped.values())[0]\n",
    "        if not (torch.as_tensor(sample_vol.shape[1:]) == sample_vol.shape[1]).all():\n",
    "            pad_pre = torch.zeros_like(roi_start)\n",
    "            pad_pre[roi_start < 0] = torch.abs(roi_start[roi_start < 0])\n",
    "            dim_limits = torch.as_tensor(\n",
    "                data_dict[list(cropped.keys())[0]].shape[1:]\n",
    "            ).to(torch.int32)\n",
    "\n",
    "            pad_post = torch.zeros_like(roi_end)\n",
    "            pad_post[roi_end > dim_limits] = (roi_end - dim_limits)[\n",
    "                roi_end > dim_limits\n",
    "            ]\n",
    "            padder = monai.transforms.BorderPadd(\n",
    "                keys=list(self._vol_key_map.keys()),\n",
    "                spatial_border=list(\n",
    "                    itertools.chain.from_iterable(\n",
    "                        zip(pad_post.tolist(), pad_pre.tolist())\n",
    "                    )\n",
    "                ),\n",
    "                mode=\"constant\",\n",
    "                value=0,\n",
    "            )\n",
    "            cropped = padder(cropped)\n",
    "            sample_vol = list(cropped.values())[0]\n",
    "            assert (torch.as_tensor(sample_vol.shape[1:]) == sample_vol.shape[1]).all()\n",
    "\n",
    "        # Store the cropped vols into the data dict with the (possibly) new keys.\n",
    "        for old_v in cropped.keys():\n",
    "            data_dict[self._vol_key_map[old_v]] = cropped[old_v]\n",
    "\n",
    "        return data_dict\n",
    "\n",
    "\n",
    "crop_fr_tf = CropSamplefODFMask(\n",
    "    \"fr_patch_vox_lu_bound\", {\"fodf\": \"fodf\", \"mask\": \"mask\"}\n",
    ")\n",
    "\n",
    "feat_tfs.append(crop_fr_tf)\n",
    "\n",
    "# Remove unnecessary items from the data dict.\n",
    "# Sub-select keys to free memory.\n",
    "select_k_tf = monai.transforms.SelectItemsd(\n",
    "    [\n",
    "        \"subj_id\",\n",
    "        \"lr_dwi\",\n",
    "        # \"lr_mask\",\n",
    "        # \"lr_bval\",\n",
    "        # \"lr_bvec\",\n",
    "        \"fodf\",\n",
    "        \"mask\",\n",
    "        \"lr_patch_extent_acpc\",\n",
    "        \"fr_patch_extent_acpc\",\n",
    "    ]\n",
    ")\n",
    "feat_tfs.append(select_k_tf)\n",
    "\n",
    "# to_cuda_tf = monai.transforms.ToDeviced(\n",
    "#     [\n",
    "#         \"lr_dwi\",\n",
    "#         \"lr_mask\",\n",
    "#         \"lr_bval\",\n",
    "#         \"lr_bvec\",\n",
    "#         \"fodf\",\n",
    "#         \"mask\",\n",
    "#         \"lr_patch_extent_acpc\",\n",
    "#         \"fr_patch_extent_acpc\",\n",
    "#     ],\n",
    "#     device=device,\n",
    "# )\n",
    "# feat_tfs.append(to_cuda_tf)\n",
    "\n",
    "# Generate the Cartesian product of the spatial coordinates of each \"training voxel\".\n",
    "vox_physical_coords_tf = monai.transforms.Lambdad(\n",
    "    [\n",
    "        \"lr_patch_extent_acpc\",\n",
    "        \"fr_patch_extent_acpc\",\n",
    "    ],\n",
    "    lambda c: einops.rearrange(\n",
    "        torch.cartesian_prod(*c.T),\n",
    "        \"(p1 p2 p3) d -> d p1 p2 p3\",\n",
    "        p1=c.shape[0],\n",
    "        p2=c.shape[0],\n",
    "        p3=c.shape[0],\n",
    "    ).to(torch.float32),\n",
    "    overwrite=True,\n",
    ")\n",
    "feat_tfs.append(vox_physical_coords_tf)\n",
    "\n",
    "\n",
    "# def overwrite_metatensor_with_tensor(x):\n",
    "#     try:\n",
    "#         ret = x.as_tensor()\n",
    "#     except AttributeError:\n",
    "#         ret = x\n",
    "#     return ret\n",
    "\n",
    "\n",
    "# from_metatensor_to_tensor_tf = monai.transforms.Lambdad(\n",
    "#     [\n",
    "#         \"lr_dwi\",\n",
    "#         \"lr_mask\",\n",
    "#         \"fodf\",\n",
    "#         \"mask\",\n",
    "#         \"lr_patch_extent_acpc\",\n",
    "#         \"fr_patch_extent_acpc\",\n",
    "#     ],\n",
    "#     overwrite_metatensor_with_tensor,\n",
    "# )\n",
    "# feat_tfs.append(from_metatensor_to_tensor_tf)\n",
    "# Convert all MetaTensors to regular Tensors.\n",
    "to_tensor_tf = monai.transforms.ToTensord(\n",
    "    [\n",
    "        \"lr_dwi\",\n",
    "        \"fodf\",\n",
    "        \"mask\",\n",
    "        \"lr_patch_extent_acpc\",\n",
    "        \"fr_patch_extent_acpc\",\n",
    "    ],\n",
    "    track_meta=False,\n",
    ")\n",
    "feat_tfs.append(to_tensor_tf)\n",
    "# ~~Generate features from each DWI and the associated bval and bvec.~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Patch-Based Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T17:40:23.234255Z",
     "iopub.status.busy": "2022-10-24T17:40:23.233717Z",
     "iopub.status.idle": "2022-10-24T17:41:17.383408Z",
     "shell.execute_reply": "2022-10-24T17:41:17.382854Z",
     "shell.execute_reply.started": "2022-10-24T17:40:23.234211Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pre_patch_transforms = monai.transforms.Compose(load_tfs + sample_tfs[:-1])\n",
    "tf_patch_sampler = sample_tfs[-1]\n",
    "per_patch_transforms = monai.transforms.Compose(feat_tfs)\n",
    "\n",
    "uncached_dataset = pitn.data.datasets.HCPfODFINRDataset(\n",
    "    subj_ids=p.train.subj_ids,\n",
    "    dwi_root_dir=hcp_full_res_data_dir,\n",
    "    fodf_root_dir=hcp_full_res_fodf_dir,\n",
    "    lr_dwi_root_dir=hcp_low_res_data_dir,\n",
    "    transform=None,\n",
    ")\n",
    "\n",
    "with warnings.catch_warnings(record=True) as warn_list:\n",
    "    # cache_dataset = monai.data.CacheDataset(\n",
    "    #     uncached_dataset, transform=pre_patch_transforms, copy_cache=False\n",
    "    # )\n",
    "    #!DEBUG\n",
    "    cache_dataset = monai.data.CacheDataset(\n",
    "        uncached_dataset[:10], transform=pre_patch_transforms, copy_cache=False\n",
    "    )\n",
    "train_dataset = monai.data.PatchDataset(\n",
    "    cache_dataset,\n",
    "    patch_func=tf_patch_sampler,\n",
    "    samples_per_image=p.train.samples_per_subj_per_epoch,\n",
    "    transform=per_patch_transforms,\n",
    ")\n",
    "print(\"=\" * 10)\n",
    "print(\"Warnings caught:\")\n",
    "[\n",
    "    warnings.showwarning(w.message, w.category, w.filename, w.lineno, w.file, w.line)\n",
    "    for w in warn_list\n",
    "]\n",
    "print(\"=\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dataset[0][\"fodf\"].meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation & Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T17:41:56.858163Z",
     "iopub.status.busy": "2022-10-24T17:41:56.855819Z",
     "iopub.status.idle": "2022-10-24T17:41:56.963112Z",
     "shell.execute_reply": "2022-10-24T17:41:56.961754Z",
     "shell.execute_reply.started": "2022-10-24T17:41:56.857943Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encoding model\n",
    "class INREncoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        interior_channels: int,\n",
    "        out_channels: int,\n",
    "        n_res_units: int,\n",
    "        n_dense_units: int,\n",
    "        activate_fn,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.interior_channels = interior_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        if isinstance(activate_fn, str):\n",
    "            activate_fn = pitn.utils.torch_lookups.activation[activate_fn]\n",
    "\n",
    "        # Pad to maintain the same input shape.\n",
    "        self.pre_conv = torch.nn.Conv3d(\n",
    "            self.in_channels,\n",
    "            self.interior_channels,\n",
    "            kernel_size=3,\n",
    "            padding=\"same\",\n",
    "            padding_mode=\"reflect\",\n",
    "        )\n",
    "\n",
    "        # Construct the densely-connected cascading layers.\n",
    "        # Create n_dense_units number of dense units.\n",
    "        top_level_units = list()\n",
    "        for _ in range(n_dense_units):\n",
    "            # Create n_res_units number of residual units for every dense unit.\n",
    "            res_layers = list()\n",
    "            for _ in range(n_res_units):\n",
    "                res_layers.append(\n",
    "                    pitn.nn.layers.ResBlock3dNoBN(\n",
    "                        self.interior_channels,\n",
    "                        kernel_size=3,\n",
    "                        activate_fn=activate_fn,\n",
    "                        padding=\"same\",\n",
    "                        padding_mode=\"reflect\",\n",
    "                    )\n",
    "                )\n",
    "            top_level_units.append(\n",
    "                pitn.nn.layers.DenseCascadeBlock3d(self.interior_channels, *res_layers)\n",
    "            )\n",
    "        self._activation_fn_init = activate_fn\n",
    "        self.activate_fn = activate_fn()\n",
    "\n",
    "        # Wrap everything into a densely-connected cascade.\n",
    "        self.cascade = pitn.nn.layers.DenseCascadeBlock3d(\n",
    "            self.interior_channels, *top_level_units\n",
    "        )\n",
    "\n",
    "        self.post_conv = torch.nn.Conv3d(\n",
    "            self.interior_channels,\n",
    "            self.out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=\"same\",\n",
    "            padding_mode=\"reflect\",\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        y = self.pre_conv(x)\n",
    "        y = self.activate_fn(y)\n",
    "        y = self.cascade(y)\n",
    "        y = self.activate_fn(y)\n",
    "        y = self.post_conv(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T17:41:57.470938Z",
     "iopub.status.busy": "2022-10-24T17:41:57.469987Z",
     "iopub.status.idle": "2022-10-24T17:41:57.554371Z",
     "shell.execute_reply": "2022-10-24T17:41:57.552775Z",
     "shell.execute_reply.started": "2022-10-24T17:41:57.470831Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# INR/Decoder model\n",
    "class ContRepDecoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_coord_features: int,\n",
    "        n_context_features: int,\n",
    "        n_context_groups: int,\n",
    "        out_features: int,\n",
    "        internal_features: int,\n",
    "        n_layers: int,\n",
    "        activate_fn,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_coord_features = n_coord_features\n",
    "        self.n_context_features = n_context_features\n",
    "        self.n_context_groups = n_context_groups\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # self.in_group_norm = torch.nn.GroupNorm(\n",
    "        #     self.n_context_groups, self.n_context_features, affine=True\n",
    "        # )\n",
    "\n",
    "        self.dense_repr = pitn.nn.inr.ResMLP(\n",
    "            self.n_coord_features + self.n_context_features,\n",
    "            self.out_features,\n",
    "            internal_size=internal_features,\n",
    "            n_layers=n_layers,\n",
    "            activate_fn=activate_fn,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query_coord,\n",
    "        context_v,\n",
    "    ):\n",
    "        # norm_v = self.in_group_norm(context_v)\n",
    "        norm_v = context_v\n",
    "        y = torch.cat((norm_v, query_coord), dim=1)\n",
    "        y = self.dense_repr(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T17:42:22.975028Z",
     "iopub.status.busy": "2022-10-24T17:42:22.973685Z",
     "iopub.status.idle": "2022-10-24T17:42:23.150923Z",
     "shell.execute_reply": "2022-10-24T17:42:23.149933Z",
     "shell.execute_reply.started": "2022-10-24T17:42:22.974898Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class INRSystem(LightningLite):\n",
    "    def run(\n",
    "        self,\n",
    "        epochs: int,\n",
    "        batch_size: int,\n",
    "        in_channels: int,\n",
    "        pred_channels: int,\n",
    "        encoder_kwargs: dict,\n",
    "        decoder_kwargs: dict,\n",
    "        train_dataset,\n",
    "        optim_kwargs: dict = dict(),\n",
    "        dataloader_kwargs: dict = dict(),\n",
    "        stage=\"train\",\n",
    "        logger=None,\n",
    "    ):\n",
    "        encoder = INREncoder(**{**encoder_kwargs, **{\"in_channels\": in_channels}})\n",
    "        decoder = ContRepDecoder(**decoder_kwargs)\n",
    "\n",
    "        optim = torch.optim.AdamW(\n",
    "            itertools.chain(encoder.parameters(), decoder.parameters()), **optim_kwargs\n",
    "        )\n",
    "        encoder = self.setup(encoder)\n",
    "        decoder, optim = self.setup(decoder, optim)\n",
    "\n",
    "        loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "        train_dataloader = monai.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            **dataloader_kwargs,\n",
    "        )\n",
    "        train_dataloader = self.setup_dataloaders(train_dataloader)\n",
    "\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        out_dir = tempfile.mkdtemp(dir=\"/tmp\")\n",
    "        print(out_dir)\n",
    "        losses = dict(\n",
    "            loss=list(),\n",
    "            epoch=list(),\n",
    "            step=list(),\n",
    "            encoder_grad_norm=list(),\n",
    "            decoder_grad_norm=list(),\n",
    "        )\n",
    "        step = 0\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch}\\n\", \"=\" * 10)\n",
    "            for batch_dict in train_dataloader:\n",
    "                x = batch_dict[\"lr_dwi\"]\n",
    "                # b_size = x.shape[0]\n",
    "                # spatial_size = torch.prod(torch.as_tensor(x.shape[2:]))\n",
    "                y = batch_dict[\"fodf\"]\n",
    "                y_mask = batch_dict[\"mask\"].to(torch.bool)\n",
    "                x_coords = batch_dict[\"lr_patch_extent_acpc\"]\n",
    "                y_coords = batch_dict[\"fr_patch_extent_acpc\"]\n",
    "                # print(y.shape)\n",
    "                # print(y_mask.shape)\n",
    "                # print(x.shape)\n",
    "                # print(x_coords.shape)\n",
    "                # print(y_coords.shape)\n",
    "\n",
    "                optim.zero_grad()\n",
    "                ctx_v = encoder(x)\n",
    "                ctx_v = pitn.nn.inr.linear_weighted_ctx_v(\n",
    "                    ctx_v, input_space_extent=x_coords, target_space_extent=y_coords\n",
    "                )\n",
    "                ctx_v = einops.rearrange(ctx_v, \"b c x y z -> (b x y z) c\")\n",
    "                vectorized_y_coords = einops.rearrange(\n",
    "                    y_coords, \"b c x y z -> (b x y z) c\"\n",
    "                )\n",
    "                pred_fodf = decoder(\n",
    "                    query_coord=vectorized_y_coords,\n",
    "                    context_v=ctx_v,\n",
    "                    input_meshgrid_indexing=\"ij\",\n",
    "                )\n",
    "                pred_fodf_patch = einops.rearrange(\n",
    "                    pred_fodf,\n",
    "                    \"(b x y z) c -> b c x y z\",\n",
    "                    b=y.shape[0],\n",
    "                    c=y.shape[1],\n",
    "                    x=y.shape[2],\n",
    "                    y=y.shape[3],\n",
    "                    z=y.shape[4],\n",
    "                )\n",
    "                y_mask_broad = torch.broadcast_to(y_mask, y.shape)\n",
    "                loss = loss_fn(pred_fodf_patch[y_mask_broad], y[y_mask_broad])\n",
    "                self.backward(loss)\n",
    "                optim.step()\n",
    "\n",
    "                print(f\"| {loss.detach().cpu().item()}\", end=\" \")\n",
    "                losses[\"loss\"].append(loss.detach().cpu().item())\n",
    "                losses[\"epoch\"].append(epoch)\n",
    "                losses[\"step\"].append(step)\n",
    "                losses[\"encoder_grad_norm\"].append(self._calc_grad_norm(encoder))\n",
    "                losses[\"decoder_grad_norm\"].append(self._calc_grad_norm(decoder))\n",
    "\n",
    "                if step == 0:\n",
    "                    print(\"Overfitting to batch\")\n",
    "                    # plt.imshow(x[0, 7, :, 0].detach().cpu().numpy(), cmap=\"gray\")\n",
    "                    # plt.colorbar()\n",
    "                    # plt.show()\n",
    "\n",
    "                    # plt.imshow(y[0, 0, :, 0].detach().cpu().numpy(), cmap=\"gray\")\n",
    "                    # plt.colorbar()\n",
    "                    # plt.show()\n",
    "                    # plt.imshow(y_mask[0, 0, :, 0].detach().cpu().numpy(), cmap=\"gray\")\n",
    "                    # plt.colorbar()\n",
    "                    # plt.show()\n",
    "\n",
    "                    # fig = plt.figure(dpi=170, figsize=(5, 8))\n",
    "                    # pitn.viz.plot_vol_slices(\n",
    "                    #     x_coords[0].detach(),\n",
    "                    #     y_coords[0].detach(),\n",
    "                    #     slice_idx=(0.4, 0.5, 0.5),\n",
    "                    #     title=f\"Epoch {epoch} Step {step}\",\n",
    "                    #     vol_labels=[\"Source Coord\", \"Target Coord\"],\n",
    "                    #     channel_labels=[\"X\", \"Y\", \"Z\"],\n",
    "                    #     colorbars=\"each\",\n",
    "                    #     fig=fig,\n",
    "                    #     cmap=\"gray\",\n",
    "                    # )\n",
    "                    # plt.show()\n",
    "\n",
    "                    encoder, decoder, optim = self._overfit_batch(\n",
    "                        repeats=10,\n",
    "                        encoder=encoder,\n",
    "                        decoder=decoder,\n",
    "                        optim=optim,\n",
    "                        loss_fn=loss_fn,\n",
    "                        x=x,\n",
    "                        y=y,\n",
    "                        x_coords=x_coords,\n",
    "                        y_coords=y_coords,\n",
    "                        y_mask=y_mask,\n",
    "                    )\n",
    "                    # fig = plt.figure(dpi=170, figsize=(5, 8))\n",
    "                    # pitn.viz.plot_vol_slices(\n",
    "                    #     x[0, 7].detach(),\n",
    "                    #     pred_fodf_patch[0, 0].detach(),\n",
    "                    #     y[0, 0].detach(),\n",
    "                    #     y_mask[0, 0].detach(),\n",
    "                    #     slice_idx=(0.4, 0.5, 0.5),\n",
    "                    #     title=f\"epoch {epoch} step {step}\",\n",
    "                    #     vol_labels=[\"input\", \"pred\", \"target\", \"target mask\"],\n",
    "                    #     colorbars=\"each\",\n",
    "                    #     fig=fig,\n",
    "                    #     cmap=\"gray\",\n",
    "                    # )\n",
    "                    # plt.savefig(path(out_dir) / f\"overfit_epoch_{epoch}.png\")\n",
    "                step += 1\n",
    "            # Save some example predictions after each epoch\n",
    "            fig = plt.figure(dpi=150, figsize=(4, 6))\n",
    "            pitn.viz.plot_vol_slices(\n",
    "                x[0, 7].detach(),\n",
    "                pred_fodf_patch[0, 0].detach(),\n",
    "                y[0, 0].detach(),\n",
    "                slice_idx=(0.4, 0.5, 0.5),\n",
    "                title=f\"Epoch {epoch} Step {step}\",\n",
    "                vol_labels=[\"Input\", \"Pred\", \"Target\"],\n",
    "                colorbars=\"each\",\n",
    "                fig=fig,\n",
    "                cmap=\"gray\",\n",
    "            )\n",
    "            plt.savefig(Path(out_dir) / f\"epoch_{epoch}_sample.png\")\n",
    "\n",
    "        print(\"=\" * 10)\n",
    "        losses = pd.DataFrame.from_dict(losses)\n",
    "        losses.to_csv(Path(out_dir) / \"train_losses.csv\")\n",
    "        losses.plot()\n",
    "\n",
    "    def _overfit_batch(\n",
    "        self,\n",
    "        repeats: int,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        optim,\n",
    "        loss_fn,\n",
    "        x,\n",
    "        y,\n",
    "        x_coords,\n",
    "        y_coords,\n",
    "        y_mask,\n",
    "    ):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        for i in range(repeats):\n",
    "            optim.zero_grad()\n",
    "            ctx_v = encoder(x)\n",
    "            ctx_v = pitn.nn.inr.linear_weighted_ctx_v(\n",
    "                ctx_v,\n",
    "                input_space_extent=x_coords,\n",
    "                target_space_extent=y_coords,\n",
    "                input_meshgrid_indexing=\"ij\",\n",
    "            )\n",
    "            ctx_v = einops.rearrange(ctx_v, \"b c x y z -> (b x y z) c\")\n",
    "            vectorized_y_coords = einops.rearrange(y_coords, \"b c x y z -> (b x y z) c\")\n",
    "            pred_fodf = decoder(query_coord=vectorized_y_coords, context_v=ctx_v)\n",
    "            pred_fodf_patch = einops.rearrange(\n",
    "                pred_fodf,\n",
    "                \"(b x y z) c -> b c x y z\",\n",
    "                b=y.shape[0],\n",
    "                c=y.shape[1],\n",
    "                x=y.shape[2],\n",
    "                y=y.shape[3],\n",
    "                z=y.shape[4],\n",
    "            )\n",
    "            y_mask_broad = torch.broadcast_to(y_mask, y.shape)\n",
    "            loss = loss_fn(pred_fodf_patch[y_mask_broad], y[y_mask_broad])\n",
    "            self.backward(loss)\n",
    "            optim.step()\n",
    "            if i % (repeats // 10) == 0:\n",
    "                print(f\"Overfit step {i} out of {repeats}\", end=\" \")\n",
    "        if repeats > 0:\n",
    "            optim.zero_grad()\n",
    "        return encoder, decoder, optim\n",
    "\n",
    "    @staticmethod\n",
    "    def _calc_grad_norm(model, norm_type=2):\n",
    "        # https://discuss.pytorch.org/t/check-the-norm-of-gradients/27961/5\n",
    "        total_norm = 0\n",
    "        parameters = [\n",
    "            p for p in model.parameters() if p.grad is not None and p.requires_grad\n",
    "        ]\n",
    "        for p in parameters:\n",
    "            param_norm = p.grad.detach().data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm**0.5\n",
    "        return total_norm\n",
    "\n",
    "    # def validate(self, model, val_dataset):\n",
    "    #     pass\n",
    "\n",
    "    # def test(self, model, test_dataset):\n",
    "    #     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-24T17:45:27.174111Z",
     "iopub.status.busy": "2022-10-24T17:45:27.172688Z",
     "iopub.status.idle": "2022-10-24T17:48:33.222085Z",
     "shell.execute_reply": "2022-10-24T17:48:33.218967Z",
     "shell.execute_reply.started": "2022-10-24T17:45:27.173982Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the system, may be re-used between training and testing.\n",
    "model_system = INRSystem(accelerator=\"gpu\", devices=1, precision=32)\n",
    "\n",
    "if \"in_channels\" not in p.encoder:\n",
    "    in_channels = int(train_dataset[0][\"lr_dwi\"].shape[0])\n",
    "else:\n",
    "    in_channels = p.encoder.in_channels\n",
    "\n",
    "model_system.run(\n",
    "    p.train.max_epochs,\n",
    "    p.train.batch_size,\n",
    "    in_channels=in_channels,\n",
    "    pred_channels=p.decoder.out_features,\n",
    "    encoder_kwargs=p.encoder.to_dict(),\n",
    "    decoder_kwargs=p.decoder.to_dict(),\n",
    "    train_dataset=train_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pd.read_csv(\"/tmp/tmpmy96sfzj/train_losses.csv\")\n",
    "losses\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(losses.step[50:], losses.loss[50:], label=\"loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(losses.step[50:], losses.encoder_grad_norm[50:], label=\"encoder grad norm\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(dpi=100)\n",
    "plt.plot(losses.step[50:], losses.decoder_grad_norm[50:], label=\"decoder grad norm\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda-pitn2]",
   "language": "python",
   "name": "conda-env-miniconda-pitn2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "f41faa2479836806c9664d670a156675ad0f09912fd4b0aed749f41e3cac86f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
