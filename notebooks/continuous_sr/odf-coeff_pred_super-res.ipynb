{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd74049d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a36db76",
   "metadata": {},
   "source": [
    "# Prediction in Arbitrary Resolution for fODF INR\n",
    "\n",
    "Code by:\n",
    "\n",
    "Tyler Spears - tas6hh@virginia.edu\n",
    "\n",
    "Dr. Tom Fletcher"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2969a8f6",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "762276a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Automatically re-import project-specific modules.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# imports\n",
    "import collections\n",
    "import copy\n",
    "import datetime\n",
    "import functools\n",
    "import inspect\n",
    "import io\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import pdb\n",
    "import random\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "import typing\n",
    "import warnings\n",
    "import zipfile\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from pprint import pprint as ppr\n",
    "\n",
    "import aim\n",
    "import dotenv\n",
    "import einops\n",
    "\n",
    "# visualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import monai\n",
    "\n",
    "# Data management libraries.\n",
    "import nibabel as nib\n",
    "import nibabel.processing\n",
    "\n",
    "# Computation & ML libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import skimage\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from box import Box\n",
    "from icecream import ic\n",
    "from natsort import natsorted\n",
    "\n",
    "import pitn\n",
    "\n",
    "from inr_networks import INREncoder, Decoder\n",
    "\n",
    "plt.rcParams.update({\"figure.autolayout\": True})\n",
    "plt.rcParams.update({\"figure.facecolor\": [1.0, 1.0, 1.0, 1.0]})\n",
    "plt.rcParams.update({\"image.cmap\": \"gray\"})\n",
    "\n",
    "# Set print options for ndarrays/tensors.\n",
    "np.set_printoptions(suppress=True, threshold=100, linewidth=88)\n",
    "torch.set_printoptions(sci_mode=False, threshold=100, linewidth=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37b5dfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "direnv: loading ~/Projects/pitn/.envrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update notebook's environment variables with direnv.\n",
    "# This requires the python-dotenv package, and direnv be installed on the system\n",
    "# This will not work on Windows.\n",
    "# NOTE: This is kind of hacky, and not necessarily safe. Be careful...\n",
    "# Libraries needed on the python side:\n",
    "# - os\n",
    "# - subprocess\n",
    "# - io\n",
    "# - dotenv\n",
    "\n",
    "# Form command to be run in direnv's context. This command will print out\n",
    "# all environment variables defined in the subprocess/sub-shell.\n",
    "command = \"direnv exec {} /usr/bin/env\".format(os.getcwd())\n",
    "# Run command in a new subprocess.\n",
    "proc = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, cwd=os.getcwd())\n",
    "# Store and format the subprocess' output.\n",
    "proc_out = proc.communicate()[0].strip().decode(\"utf-8\")\n",
    "# Use python-dotenv to load the environment variables by using the output of\n",
    "# 'direnv exec ...' as a 'dummy' .env file.\n",
    "dotenv.load_dotenv(stream=io.StringIO(proc_out), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "873dbf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Device IDX  1\n",
      "CUDA Current Device  1\n",
      "CUDA Device properties:  _CudaDeviceProperties(name='NVIDIA RTX A5000', major=8, minor=6, total_memory=24245MB, multi_processor_count=64)\n",
      "CuDNN convolution optimization enabled.\n",
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "# torch setup\n",
    "# allow for CUDA usage, if available\n",
    "if torch.cuda.is_available():\n",
    "    # Pick only one device for the default, may use multiple GPUs for training later.\n",
    "    if \"CUDA_PYTORCH_DEVICE_IDX\" in os.environ.keys():\n",
    "        dev_idx = int(os.environ[\"CUDA_PYTORCH_DEVICE_IDX\"])\n",
    "    else:\n",
    "        dev_idx = 1\n",
    "    device = torch.device(f\"cuda:{dev_idx}\")\n",
    "    print(\"CUDA Device IDX \", dev_idx)\n",
    "    torch.cuda.set_device(device)\n",
    "    print(\"CUDA Current Device \", torch.cuda.current_device())\n",
    "    print(\"CUDA Device properties: \", torch.cuda.get_device_properties(device))\n",
    "    # The flag below controls whether to allow TF32 on matmul. This flag defaults to False\n",
    "    # in PyTorch 1.12 and later.\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    # See\n",
    "    # <https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices>\n",
    "    # for details.\n",
    "\n",
    "    # Activate cudnn benchmarking to optimize convolution algorithm speed.\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        print(\"CuDNN convolution optimization enabled.\")\n",
    "        # The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# keep device as the cpu\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e010db0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr cap\n",
    "# Capture output and save to log. Needs to be at the *very first* line of the cell.\n",
    "# Watermark\n",
    "%load_ext watermark\n",
    "%watermark --author \"Tyler Spears\" --updated --iso8601  --python --machine --iversions --githash\n",
    "if torch.cuda.is_available():\n",
    "    # GPU information\n",
    "    try:\n",
    "        gpu_info = pitn.utils.system.get_gpu_specs()\n",
    "        print(gpu_info)\n",
    "    except NameError:\n",
    "        print(\"CUDA Version: \", torch.version.cuda)\n",
    "else:\n",
    "    print(\"CUDA not in use, falling back to CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a36a034a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Tyler Spears\n",
      "\n",
      "Last updated: 2023-06-19T23:16:59.107177-04:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.9\n",
      "IPython version      : 8.4.0\n",
      "\n",
      "Compiler    : GCC 11.3.0\n",
      "OS          : Linux\n",
      "Release     : 5.19.0-41-generic\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 20\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: aaed9ec4776e4f352c5569a9295bfa47cb662d9d\n",
      "\n",
      "pitn      : 0.0.post1.dev291+ge9e5f58.d20230531\n",
      "einops    : 0.6.0\n",
      "monai     : 1.2.0\n",
      "torch     : 1.13.1\n",
      "nibabel   : 5.1.0\n",
      "sys       : 3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:20:04) [GCC 11.3.0]\n",
      "seaborn   : 0.12.1\n",
      "numpy     : 1.23.4\n",
      "matplotlib: 3.5.2\n",
      "skimage   : 0.19.3\n",
      "aim       : 3.14.4\n",
      "pandas    : 1.5.2\n",
      "\n",
      "==================================================GPU Specs==================================================\n",
      "  id  Name              Driver Version      CUDA Version  Total Memory    uuid\n",
      "----  ----------------  ----------------  --------------  --------------  ----------------------------------------\n",
      "   0  NVIDIA RTX A5000  525.105.17                  11.7  24564.0MB       GPU-ed20d87f-e88e-692f-0b56-548b8a05ddea\n",
      "   1  NVIDIA RTX A5000  525.105.17                  11.7  24564.0MB       GPU-0636ee40-2eab-9533-1be7-dbbadade95c4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cap is defined in an ipython magic command\n",
    "try:\n",
    "    print(cap)\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2267f363",
   "metadata": {},
   "source": [
    "## Experiment & Parameters Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcf50036",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Config file not loaded\n"
     ]
    }
   ],
   "source": [
    "p = Box(default_box=True)\n",
    "# Experiment defaults, can be overridden in a config file.\n",
    "\n",
    "# General experiment-wide params\n",
    "###############################################\n",
    "p.target_vox_spacing = 1.0\n",
    "p.results_dir = \"/data/srv/outputs/pitn/results/runs\"\n",
    "p.tmp_results_dir = \"/data/srv/outputs/pitn/results/tmp\"\n",
    "# p.train_val_test_split_file = random.choice(\n",
    "#     list(Path(\"./data_splits\").glob(\"HCP*train-val-test_split*.csv\"))\n",
    "# )\n",
    "p.model_weight_f = str(\n",
    "    Path(p.tmp_results_dir)\n",
    "    / \"2023-06-18T20_34_58\"\n",
    "    / \"state_dict_epoch_99_step_25000.pt\"\n",
    ")\n",
    "###############################################\n",
    "# kwargs for the sub-selection function to go from full DWI -> low-res DWI.\n",
    "# See `sub_select_dwi_from_bval` function in `pitn`.\n",
    "p.bval_sub_sample_fn_kwargs = dict(\n",
    "    shells_to_remove=[2000],\n",
    "    within_shell_idx_to_keep={\n",
    "        0: range(0, 9),\n",
    "        1000: range(0, 45),\n",
    "        3000: range(0, 45),\n",
    "    },\n",
    ")\n",
    "# 1.25mm -> 2.0mm\n",
    "p.baseline_lr_spacing_scale = 1.6\n",
    "p.scale_prefilter_kwargs = dict(\n",
    "    sigma_scale_coeff=2.5,\n",
    "    sigma_truncate=4.0,\n",
    ")\n",
    "p.test.subj_ids = list(\n",
    "    map(\n",
    "        str,\n",
    "        [\n",
    "            581450,\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Network/model parameters.\n",
    "p.encoder = dict(\n",
    "    interior_channels=80,\n",
    "    out_channels=128,\n",
    "    n_res_units=3,\n",
    "    n_dense_units=3,\n",
    "    activate_fn=\"relu\",\n",
    "    input_coord_channels=True,\n",
    ")\n",
    "p.decoder = dict(\n",
    "    context_v_features=128,\n",
    "    in_features=p.encoder.out_channels,\n",
    "    out_features=45,\n",
    "    m_encode_num_freqs=36,\n",
    "    sigma_encode_scale=3.0,\n",
    ")\n",
    "\n",
    "\n",
    "# If a config file exists, override the defaults with those values.\n",
    "try:\n",
    "    if \"PITN_CONFIG\" in os.environ.keys():\n",
    "        config_fname = Path(os.environ[\"PITN_CONFIG\"])\n",
    "    else:\n",
    "        config_fname = pitn.utils.system.get_file_glob_unique(Path(\".\"), r\"config.*\")\n",
    "    f_type = config_fname.suffix.casefold()\n",
    "    if f_type in {\".yaml\", \".yml\"}:\n",
    "        f_params = Box.from_yaml(filename=config_fname)\n",
    "    elif f_type == \".json\":\n",
    "        f_params = Box.from_json(filename=config_fname)\n",
    "    elif f_type == \".toml\":\n",
    "        f_params = Box.from_toml(filename=config_fname)\n",
    "    else:\n",
    "        raise RuntimeError()\n",
    "\n",
    "    p.merge_update(f_params)\n",
    "\n",
    "except:\n",
    "    print(\"WARNING: Config file not loaded\")\n",
    "    pass\n",
    "\n",
    "# Remove the default_box behavior now that params have been fully read in.\n",
    "_p = Box(default_box=False)\n",
    "_p.merge_update(p)\n",
    "p = _p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c636105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tvt_split = pd.read_csv(p.train_val_test_split_file)\n",
    "# p.test.subj_ids = natsorted(tvt_split[tvt_split.split == \"test\"].subj_id.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33cd2dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| p.to_dict(): {'baseline_lr_spacing_scale': 1.6,\n",
      "                  'bval_sub_sample_fn_kwargs': {'shells_to_remove': [2000],\n",
      "                                                'within_shell_idx_to_keep': {0: range(0, 9),\n",
      "                                                                             1000: range(0, 45),\n",
      "                                                                             3000: range(0, 45)}},\n",
      "                  'decoder': {'context_v_features': 128,\n",
      "                              'in_features': 128,\n",
      "                              'm_encode_num_freqs': 36,\n",
      "                              'out_features': 45,\n",
      "                              'sigma_encode_scale': 3.0},\n",
      "                  'encoder': {'activate_fn': 'relu',\n",
      "                              'input_coord_channels': True,\n",
      "                              'interior_channels': 80,\n",
      "                              'n_dense_units': 3,\n",
      "                              'n_res_units': 3,\n",
      "                              'out_channels': 128},\n",
      "                  'model_weight_f': '/data/srv/outputs/pitn/results/tmp/2023-06-18T20_34_58/state_dict_epoch_99_step_25000.pt',\n",
      "                  'results_dir': '/data/srv/outputs/pitn/results/runs',\n",
      "                  'scale_prefilter_kwargs': {'sigma_scale_coeff': 2.5, 'sigma_truncate': 4.0},\n",
      "                  'target_vox_spacing': 1.0,\n",
      "                  'test': {'subj_ids': ['581450']},\n",
      "                  'tmp_results_dir': '/data/srv/outputs/pitn/results/tmp'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'target_vox_spacing': 1.0,\n",
       " 'results_dir': '/data/srv/outputs/pitn/results/runs',\n",
       " 'tmp_results_dir': '/data/srv/outputs/pitn/results/tmp',\n",
       " 'model_weight_f': '/data/srv/outputs/pitn/results/tmp/2023-06-18T20_34_58/state_dict_epoch_99_step_25000.pt',\n",
       " 'bval_sub_sample_fn_kwargs': {'shells_to_remove': [2000],\n",
       "  'within_shell_idx_to_keep': {0: range(0, 9),\n",
       "   1000: range(0, 45),\n",
       "   3000: range(0, 45)}},\n",
       " 'baseline_lr_spacing_scale': 1.6,\n",
       " 'scale_prefilter_kwargs': {'sigma_scale_coeff': 2.5, 'sigma_truncate': 4.0},\n",
       " 'test': {'subj_ids': ['581450']},\n",
       " 'encoder': {'interior_channels': 80,\n",
       "  'out_channels': 128,\n",
       "  'n_res_units': 3,\n",
       "  'n_dense_units': 3,\n",
       "  'activate_fn': 'relu',\n",
       "  'input_coord_channels': True},\n",
       " 'decoder': {'context_v_features': 128,\n",
       "  'in_features': 128,\n",
       "  'out_features': 45,\n",
       "  'm_encode_num_freqs': 36,\n",
       "  'sigma_encode_scale': 3.0}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(p.to_dict())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d328918",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2df5115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hcp_full_res_data_dir = Path(\"/data/srv/data/pitn/hcp\")\n",
    "hcp_full_res_fodf_dir = Path(\"/data/srv/outputs/pitn/hcp/full-res/fodf\")\n",
    "\n",
    "assert hcp_full_res_data_dir.exists()\n",
    "assert hcp_full_res_fodf_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8789bb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hte bval/bvec sub-sample scheme according to the parameter dict kwargs.\n",
    "bval_sub_sample_fn = partial(\n",
    "    pitn.data.datasets2.sub_select_dwi_from_bval,\n",
    "    **p.bval_sub_sample_fn_kwargs.to_dict(),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f0c9dea",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe00f5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 1/1 [01:08<00:00, 68.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Warnings caught:\n",
      "\n",
      "==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings(record=True) as warn_list:\n",
    "\n",
    "    test_dataset = pitn.data.datasets2.HCPfODFINRDataset(\n",
    "        subj_ids=p.test.subj_ids,\n",
    "        dwi_root_dir=hcp_full_res_data_dir,\n",
    "        fodf_root_dir=hcp_full_res_fodf_dir,\n",
    "        transform=pitn.data.datasets2.HCPfODFINRDataset.default_pre_sample_tf(\n",
    "            sample_mask_key=\"wm_mask\",\n",
    "            bval_sub_sample_fn=bval_sub_sample_fn,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    test_dataset = pitn.data.datasets2.HCPfODFINRWholeBrainDataset(\n",
    "        test_dataset,\n",
    "        transform=pitn.data.datasets2.HCPfODFINRWholeBrainDataset.default_vol_tf(\n",
    "            baseline_iso_scale_factor_lr_spacing_mm_low_high=p.baseline_lr_spacing_scale,\n",
    "            scale_prefilter_kwargs=p.scale_prefilter_kwargs,\n",
    "        ),\n",
    "    )\n",
    "    test_dataset = monai.data.CacheDataset(\n",
    "        test_dataset,\n",
    "        cache_num=1,\n",
    "        transform=None,\n",
    "        progress=True,\n",
    "        copy_cache=False,\n",
    "        num_workers=1,\n",
    "    )\n",
    "\n",
    "print(\"=\" * 10)\n",
    "print(\"Warnings caught:\")\n",
    "ws = \"\\n\".join(\n",
    "    [\n",
    "        warnings.formatwarning(\n",
    "            w.message, w.category, w.filename, w.lineno, w.file, w.line\n",
    "        )\n",
    "        for w in warn_list\n",
    "    ]\n",
    ")\n",
    "ws = \"\\n\".join(filter(lambda s: bool(s.strip()), ws.splitlines()))\n",
    "print(ws, flush=True)\n",
    "print(\"=\" * 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55964e62",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7c783e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchwise_masked_mse(y_pred, y, mask):\n",
    "    masked_y_pred = y_pred.clone()\n",
    "    masked_y = y.clone()\n",
    "    masked_y_pred[~mask] = torch.nan\n",
    "    masked_y[~mask] = torch.nan\n",
    "    se = F.mse_loss(masked_y_pred, masked_y, reduction=\"none\")\n",
    "    se = se.reshape(se.shape[0], -1)\n",
    "    mse = torch.nanmean(se, dim=1)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6c430bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "# Break ISO format because many programs don't like having colons ':' in a filename.\n",
    "ts = ts.replace(\":\", \"_\")\n",
    "experiment_name = f\"{ts}_inr-pred-test_super-res_non-init-decoder_blank-pred\"\n",
    "tmp_res_dir = Path(p.tmp_results_dir) / experiment_name\n",
    "tmp_res_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72ad03bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| sum(p.numel() for p in encoder.parameters()): 4667306\n",
      "ic| sum(p.numel() for p in decoder.parameters()): 291885\n",
      "ic| 128 * 128 + 6*(350 * 128 ) + 128 * 45: 290944\n",
      "ic| 128 * 256 + 3*(256*256) + 256*45: 240896\n",
      "ic| 128 * 128 + 6*(350 * 128 ) + 128 * 45: 290944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "290944"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(sum(p.numel() for p in encoder.parameters()))\n",
    "ic(sum(p.numel() for p in decoder.parameters()))\n",
    "ic(128 * 128 + 6 * (350 * 128) + 128 * 45)\n",
    "ic(128 * 256 + 3 * (256 * 256) + 256 * 45)\n",
    "ic(128 * 128 + 6 * (350 * 128) + 128 * 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f611aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([99, 77, 100, 77])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0][\"lr_dwi\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25699732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| sum(p.numel() for p in e.parameters()): 3761178\n",
      "ic| sum(p.numel() for p in d.parameters()): 142125\n",
      "ic| e_p + d_p: 3903303\n",
      "ic| sum(p.numel() for p in r.parameters()): 966483\n"
     ]
    }
   ],
   "source": [
    "e = INREncoder(\n",
    "    in_channels=99 + 3,\n",
    "    n_res_units=3,\n",
    "    n_dense_units=3,\n",
    "    activate_fn=\"relu\",\n",
    "    input_coord_channels=True,\n",
    "    interior_channels=72,\n",
    "    out_channels=96,\n",
    ")\n",
    "e(torch.randn(1, 102, 36, 36, 36, dtype=torch.float32))\n",
    "e_p = ic(sum(p.numel() for p in e.parameters()))\n",
    "d = Decoder(\n",
    "    context_v_features=96,\n",
    "    in_features=96,\n",
    "    out_features=45,\n",
    "    m_encode_num_freqs=20,\n",
    "    sigma_encode_scale=3.0,\n",
    ")\n",
    "d_p = ic(sum(p.numel() for p in d.parameters()))\n",
    "ic(e_p + d_p)\n",
    "r = INREncoder(\n",
    "    interior_channels=48,\n",
    "    out_channels=9,\n",
    "    n_res_units=2,\n",
    "    n_dense_units=2,\n",
    "    in_channels=96,\n",
    "    activate_fn=\"relu\",\n",
    "    input_coord_channels=False,\n",
    ")\n",
    "r(torch.randn(1, 96, 36, 36, 36, dtype=torch.float32))\n",
    "r_p = ic(sum(p.numel() for p in r.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49181a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mambaforge/envs/pitn/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning:\n",
      "\n",
      "Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting inference\n",
      "Starting 581450\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=186'>187</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=187'>188</a>\u001b[0m     (tmp_res_dir \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSTOPPED\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mtouch()\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=188'>189</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=189'>190</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=190'>191</a>\u001b[0m     (tmp_res_dir \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mFAILED\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mtouch()\n",
      "\u001b[1;32m/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=137'>138</a>\u001b[0m fn_coordify \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: einops\u001b[39m.\u001b[39mrearrange(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=138'>139</a>\u001b[0m     x, \u001b[39m\"\u001b[39m\u001b[39mb coord x y z -> b x y z coord\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=139'>140</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=140'>141</a>\u001b[0m \u001b[39m# Keep the whole volume on the CPU, and only transfer the sliding windows\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=141'>142</a>\u001b[0m \u001b[39m# to the GPU.\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=142'>143</a>\u001b[0m pred_fodf \u001b[39m=\u001b[39m monai\u001b[39m.\u001b[39;49minferers\u001b[39m.\u001b[39;49msliding_window_inference(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=143'>144</a>\u001b[0m     sr_slide_window\u001b[39m.\u001b[39;49mcpu(),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=144'>145</a>\u001b[0m     roi_size\u001b[39m=\u001b[39;49m(\u001b[39m96\u001b[39;49m, \u001b[39m96\u001b[39;49m, \u001b[39m96\u001b[39;49m),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=145'>146</a>\u001b[0m     sw_batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=146'>147</a>\u001b[0m     predictor\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m q: decoder(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=147'>148</a>\u001b[0m         \u001b[39m# Rearrange back into coord-last format.\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=148'>149</a>\u001b[0m         query_world_coord\u001b[39m=\u001b[39;49mfn_coordify(q[:, :\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\u001b[39m.\u001b[39;49mto(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=149'>150</a>\u001b[0m             device, non_blocking\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=150'>151</a>\u001b[0m         ),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=151'>152</a>\u001b[0m         query_world_coord_mask\u001b[39m=\u001b[39;49mfn_coordify(q[:, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m:]\u001b[39m.\u001b[39;49mbool())\u001b[39m.\u001b[39;49mto(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=152'>153</a>\u001b[0m             device, non_blocking\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=153'>154</a>\u001b[0m         ),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=154'>155</a>\u001b[0m         context_v\u001b[39m=\u001b[39;49mctx_v,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=155'>156</a>\u001b[0m         context_world_coord_grid\u001b[39m=\u001b[39;49mx_coords,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=156'>157</a>\u001b[0m         affine_context_vox2world\u001b[39m=\u001b[39;49mx_affine_vox2world,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=157'>158</a>\u001b[0m         affine_query_vox2world\u001b[39m=\u001b[39;49msr_affine_vox2world,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=158'>159</a>\u001b[0m         context_vox_size_world\u001b[39m=\u001b[39;49mx_vox_size,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=159'>160</a>\u001b[0m         query_vox_size_world\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mones_like(x_vox_size)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=160'>161</a>\u001b[0m         \u001b[39m*\u001b[39;49m p\u001b[39m.\u001b[39;49mtarget_vox_spacing,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=161'>162</a>\u001b[0m     )\u001b[39m.\u001b[39;49mcpu(),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=162'>163</a>\u001b[0m     overlap\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=163'>164</a>\u001b[0m     padding_mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mreplicate\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=164'>165</a>\u001b[0m )\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=165'>166</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDone with inference subject \u001b[39m\u001b[39m{\u001b[39;00msubj_id\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, flush\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=166'>167</a>\u001b[0m \u001b[39m# Mask out the prediction, otherwise the file size will be considerably\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=167'>168</a>\u001b[0m \u001b[39m# larger.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mambaforge/envs/pitn/lib/python3.10/site-packages/monai/inferers/utils.py:223\u001b[0m, in \u001b[0;36msliding_window_inference\u001b[0;34m(inputs, roi_size, sw_batch_size, predictor, overlap, mode, sigma_scale, padding_mode, cval, sw_device, device, progress, roi_weight_map, process_fn, buffer_steps, buffer_dim, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     win_data \u001b[39m=\u001b[39m inputs[unravel_slice[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39mto(sw_device)\n\u001b[0;32m--> 223\u001b[0m seg_prob_out \u001b[39m=\u001b[39m predictor(win_data, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# batched patch\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[39m# convert seg_prob_out to tuple seg_tuple, this does not allocate new memory.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m dict_keys, seg_tuple \u001b[39m=\u001b[39m _flatten_struct(seg_prob_out)\n",
      "\u001b[1;32m/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb Cell 22\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=137'>138</a>\u001b[0m fn_coordify \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: einops\u001b[39m.\u001b[39mrearrange(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=138'>139</a>\u001b[0m     x, \u001b[39m\"\u001b[39m\u001b[39mb coord x y z -> b x y z coord\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=139'>140</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=140'>141</a>\u001b[0m \u001b[39m# Keep the whole volume on the CPU, and only transfer the sliding windows\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=141'>142</a>\u001b[0m \u001b[39m# to the GPU.\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=142'>143</a>\u001b[0m pred_fodf \u001b[39m=\u001b[39m monai\u001b[39m.\u001b[39minferers\u001b[39m.\u001b[39msliding_window_inference(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=143'>144</a>\u001b[0m     sr_slide_window\u001b[39m.\u001b[39mcpu(),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=144'>145</a>\u001b[0m     roi_size\u001b[39m=\u001b[39m(\u001b[39m96\u001b[39m, \u001b[39m96\u001b[39m, \u001b[39m96\u001b[39m),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=145'>146</a>\u001b[0m     sw_batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=146'>147</a>\u001b[0m     predictor\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m q: decoder(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=147'>148</a>\u001b[0m         \u001b[39m# Rearrange back into coord-last format.\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=148'>149</a>\u001b[0m         query_world_coord\u001b[39m=\u001b[39mfn_coordify(q[:, :\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\u001b[39m.\u001b[39;49mto(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=149'>150</a>\u001b[0m             device, non_blocking\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=150'>151</a>\u001b[0m         ),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=151'>152</a>\u001b[0m         query_world_coord_mask\u001b[39m=\u001b[39mfn_coordify(q[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mbool())\u001b[39m.\u001b[39mto(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=152'>153</a>\u001b[0m             device, non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=153'>154</a>\u001b[0m         ),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=154'>155</a>\u001b[0m         context_v\u001b[39m=\u001b[39mctx_v,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=155'>156</a>\u001b[0m         context_world_coord_grid\u001b[39m=\u001b[39mx_coords,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=156'>157</a>\u001b[0m         affine_context_vox2world\u001b[39m=\u001b[39mx_affine_vox2world,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=157'>158</a>\u001b[0m         affine_query_vox2world\u001b[39m=\u001b[39msr_affine_vox2world,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=158'>159</a>\u001b[0m         context_vox_size_world\u001b[39m=\u001b[39mx_vox_size,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=159'>160</a>\u001b[0m         query_vox_size_world\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mones_like(x_vox_size)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=160'>161</a>\u001b[0m         \u001b[39m*\u001b[39m p\u001b[39m.\u001b[39mtarget_vox_spacing,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=161'>162</a>\u001b[0m     )\u001b[39m.\u001b[39mcpu(),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=162'>163</a>\u001b[0m     overlap\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=163'>164</a>\u001b[0m     padding_mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreplicate\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=164'>165</a>\u001b[0m )\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=165'>166</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDone with inference subject \u001b[39m\u001b[39m{\u001b[39;00msubj_id\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, flush\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=166'>167</a>\u001b[0m \u001b[39m# Mask out the prediction, otherwise the file size will be considerably\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/odf-coeff_pred_super-res.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=167'>168</a>\u001b[0m \u001b[39m# larger.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = \"INR\"\n",
    "model_pred_res_dir = tmp_res_dir / model\n",
    "# model_pred_res_dir.mkdir(exist_ok=True)\n",
    "# with open(model_pred_res_dir / \"model_description.txt\", \"x\") as f:\n",
    "#     f.write(f\"model weights file: {str(p.model_weight_f)}\\n\")\n",
    "#     f.write(f\"encoder parameters: \\n{str(p.encoder.to_dict())}\\n\")\n",
    "#     f.write(f\"decoder parameters: \\n{str(p.decoder.to_dict())}\\n\")\n",
    "\n",
    "# Wrap the entire loop in a try...except statement to save out a failure indicator file.\n",
    "try:\n",
    "    system_state_dict = torch.load(p.model_weight_f)\n",
    "    encoder_state_dict = system_state_dict[\"encoder\"]\n",
    "\n",
    "    decoder_state_dict = system_state_dict[\"decoder\"]\n",
    "\n",
    "    if \"in_channels\" not in p.encoder:\n",
    "        in_channels = int(test_dataset[0][\"lr_dwi\"].shape[0]) + 3\n",
    "    else:\n",
    "        in_channels = p.encoder.in_channels\n",
    "\n",
    "    encoder = INREncoder(**{**p.encoder.to_dict(), **{\"in_channels\": in_channels}})\n",
    "    # encoder.load_state_dict(encoder_state_dict) #!DEBUG\n",
    "    encoder.to(device)\n",
    "\n",
    "    decoder = Decoder(**p.decoder.to_dict())\n",
    "    # decoder.load_state_dict(decoder_state_dict) #!DEBUG\n",
    "    decoder.to(device)\n",
    "    del (\n",
    "        system_state_dict,\n",
    "        encoder_state_dict,\n",
    "        decoder_state_dict,\n",
    "    )\n",
    "    # with open(model_pred_res_dir / \"model_description.txt\", \"a\") as f:\n",
    "    #     f.write(f\"encoder layers: \\n{str(encoder)}\\n\")\n",
    "    #     f.write(f\"decoder layers: \\n{str(decoder)}\\n\")\n",
    "\n",
    "    test_dataloader = monai.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=1,\n",
    "    )\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(\"Starting inference\", flush=True)\n",
    "        for batch_dict in test_dataloader:\n",
    "            subj_id = batch_dict[\"subj_id\"]\n",
    "            if len(subj_id) == 1:\n",
    "                subj_id = subj_id[0]\n",
    "            print(f\"Starting {subj_id}\", flush=True)\n",
    "\n",
    "            x = batch_dict[\"lr_dwi\"].to(device)\n",
    "            batch_size = x.shape[0]\n",
    "            x_mask = batch_dict[\"lr_brain_mask\"].to(torch.bool).to(device)\n",
    "            x_affine_vox2world = batch_dict[\"affine_lr_vox2world\"].to(device)\n",
    "            x_vox_size = batch_dict[\"lr_vox_size\"].to(device)\n",
    "            x_coords = pitn.affine.affine_coordinate_grid(\n",
    "                x_affine_vox2world, tuple(x.shape[2:])\n",
    "            )\n",
    "            # Fix an edge case in the affine_coordinate_grid function.\n",
    "            if batch_size == 1:\n",
    "                if x_coords.shape[0] != 1:\n",
    "                    x_coords.unsqueeze_(0)\n",
    "\n",
    "            # Calculate the new coordinates given the target vox spacing.\n",
    "            input_vox_size = x_vox_size.flatten().cpu().numpy()[0]\n",
    "            scale_x2sr = p.target_vox_spacing / input_vox_size\n",
    "            # We don't need a particular voxel buffer here, only that the SR fov is\n",
    "            # totally contained within the src fov by some amount > 0.\n",
    "            sr_affine_vox2world = pitn.data.datasets2._random_iso_center_scale_affine(\n",
    "                x_affine_vox2world[0].cpu(),\n",
    "                x[0].cpu(),\n",
    "                scale_low=scale_x2sr,\n",
    "                scale_high=scale_x2sr,\n",
    "                n_delta_buffer_scaled_vox=0,\n",
    "            )\n",
    "            sr_affine_vox2world = sr_affine_vox2world[None]\n",
    "            sr_spatial_shape = pitn.affine.transform_coords(\n",
    "                x_coords[0, -1, -1, -1].to(sr_affine_vox2world),\n",
    "                torch.linalg.inv(sr_affine_vox2world),\n",
    "            )\n",
    "            sr_spatial_shape = tuple(\n",
    "                torch.floor(sr_spatial_shape).int().cpu().numpy().tolist()\n",
    "            )\n",
    "            # Be careful to keep all the \"super-sized\" tensors on the cpu!\n",
    "            sr_coords = pitn.affine.affine_coordinate_grid(\n",
    "                sr_affine_vox2world.to(torch.float32).cpu(), sr_spatial_shape\n",
    "            )\n",
    "\n",
    "            # Fix an edge case in the affine_coordinate_grid function.\n",
    "            if batch_size == 1:\n",
    "                if sr_coords.shape[0] != 1:\n",
    "                    sr_coords.unsqueeze_(0)\n",
    "\n",
    "            # Interpolate a mask for the sr volume.\n",
    "            sr_mask = pitn.affine.sample_vol(\n",
    "                x_mask.cpu(),\n",
    "                sr_coords.cpu(),\n",
    "                affine_vox2mm=x_affine_vox2world.cpu(),\n",
    "                mode=\"nearest\",\n",
    "                align_corners=True,\n",
    "            )\n",
    "\n",
    "            # Concatenate the input world coordinates as input features into the\n",
    "            # encoder. Mask out the x coordinates that are not to be considered.\n",
    "            x_coord_mask = einops.rearrange(x_mask, \"b 1 x y z -> b x y z 1\")\n",
    "            x_coords_encoder = einops.rearrange(\n",
    "                x_coords * x_coord_mask, \"b x y z coord -> b coord x y z\"\n",
    "            )\n",
    "            x = torch.cat([x, x_coords_encoder], dim=1)\n",
    "            ctx_v = encoder(x)\n",
    "            #!DEBUG>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "            # meds = torch.median(\n",
    "            #     ctx_v.reshape(ctx_v.shape[1], -1), dim=1\n",
    "            # ).values.reshape(1, ctx_v.shape[1], 1, 1, 1)\n",
    "            ctx_v = ctx_v * 0  # + meds\n",
    "\n",
    "            # sr_coords = sr_coords * 0\n",
    "            #!DEBUG<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "            # Whole-volume inference is memory-prohibitive, so use a sliding\n",
    "            # window inference method on the encoded volume.\n",
    "            # Transform y_coords into a coordinates-first shape, for the interface, and\n",
    "            # attach the mask for compatibility with the sliding inference function.\n",
    "            sr_slide_window = torch.cat(\n",
    "                [\n",
    "                    einops.rearrange(sr_coords.cpu(), \"b x y z coord -> b coord x y z\"),\n",
    "                    sr_mask.to(sr_coords).cpu(),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "            fn_coordify = lambda x: einops.rearrange(\n",
    "                x, \"b coord x y z -> b x y z coord\"\n",
    "            )\n",
    "            # Keep the whole volume on the CPU, and only transfer the sliding windows\n",
    "            # to the GPU.\n",
    "            pred_fodf = monai.inferers.sliding_window_inference(\n",
    "                sr_slide_window.cpu(),\n",
    "                roi_size=(96, 96, 96),\n",
    "                sw_batch_size=batch_size,\n",
    "                predictor=lambda q: decoder(\n",
    "                    # Rearrange back into coord-last format.\n",
    "                    query_world_coord=fn_coordify(q[:, :-1]).to(\n",
    "                        device, non_blocking=True\n",
    "                    ),\n",
    "                    query_world_coord_mask=fn_coordify(q[:, -1:].bool()).to(\n",
    "                        device, non_blocking=True\n",
    "                    ),\n",
    "                    context_v=ctx_v,\n",
    "                    context_world_coord_grid=x_coords,\n",
    "                    affine_context_vox2world=x_affine_vox2world,\n",
    "                    affine_query_vox2world=sr_affine_vox2world,\n",
    "                    context_vox_size_world=x_vox_size,\n",
    "                    query_vox_size_world=torch.ones_like(x_vox_size)\n",
    "                    * p.target_vox_spacing,\n",
    "                ).cpu(),\n",
    "                overlap=0,\n",
    "                padding_mode=\"replicate\",\n",
    "            ).cpu()\n",
    "            print(f\"Done with inference subject {subj_id}\", flush=True)\n",
    "            # Mask out the prediction, otherwise the file size will be considerably\n",
    "            # larger.\n",
    "            pred_fodf *= sr_mask.cpu()\n",
    "            # Write out prediction to a .nii.gz file.\n",
    "            pred_f = (\n",
    "                model_pred_res_dir\n",
    "                / f\"{subj_id}_{model}_prediction_{input_vox_size}mm-to-{p.target_vox_spacing}mm.nii.gz\"\n",
    "            )\n",
    "            pred_affine = sr_affine_vox2world[0].cpu().numpy()\n",
    "            pred_fodf_vol = einops.rearrange(\n",
    "                pred_fodf.detach().cpu().numpy(), \"1 c x y z -> x y z c\"\n",
    "            ).astype(np.float32)\n",
    "            # pred_im = nib.Nifti1Image(pred_fodf_vol, affine=pred_affine)\n",
    "\n",
    "            # print(f\"Saving prediction {subj_id}\", flush=True)\n",
    "            # nib.save(pred_im, pred_f)\n",
    "            fig = plt.figure(dpi=150)\n",
    "            pitn.viz.plot_fodf_coeff_slices(\n",
    "                pred_fodf[0], interpolation=\"nearest\", fig=fig\n",
    "            )\n",
    "            print(f\"Finished {subj_id}\", flush=True)\n",
    "\n",
    "except KeyboardInterrupt as e:\n",
    "    (tmp_res_dir / \"STOPPED\").touch()\n",
    "    raise e\n",
    "except Exception as e:\n",
    "    (tmp_res_dir / \"FAILED\").touch()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f640446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "comment_magics": true,
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
