{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73408619",
   "metadata": {},
   "source": [
    "# Tractgraphy Evaluation for Continuous fODF Interpolation\n",
    "\n",
    "Code by:\n",
    "\n",
    "Tyler Spears - tas6hh@virginia.edu\n",
    "\n",
    "Dr. Tom Fletcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64ef468",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8d7897f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T20:13:53.001100Z",
     "iopub.status.busy": "2022-12-01T20:13:53.000440Z",
     "iopub.status.idle": "2022-12-01T20:13:53.076346Z",
     "shell.execute_reply": "2022-12-01T20:13:53.075315Z",
     "shell.execute_reply.started": "2022-12-01T20:13:53.001052Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-08 14:46:18,035 - Created a temporary directory at /tmp/tmpdp_b2_8g\n",
      "2022-12-08 14:46:18,037 - Writing /tmp/tmpdp_b2_8g/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "# Automatically re-import project-specific modules.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# imports\n",
    "import collections\n",
    "import copy\n",
    "import datetime\n",
    "import functools\n",
    "import inspect\n",
    "import io\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import pdb\n",
    "import random\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "import typing\n",
    "import warnings\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from pprint import pprint as ppr\n",
    "\n",
    "import dotenv\n",
    "import einops\n",
    "\n",
    "# visualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import monai\n",
    "from pytorch_lightning.lite import LightningLite\n",
    "\n",
    "# Data management libraries.\n",
    "import nibabel as nib\n",
    "import nibabel.processing\n",
    "import transforms3d\n",
    "\n",
    "# Computation & ML libraries.\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import skimage\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from box import Box\n",
    "from icecream import ic\n",
    "from natsort import natsorted\n",
    "import dipy\n",
    "import dipy.data\n",
    "import dipy.io\n",
    "import dipy.io.stateful_tractogram\n",
    "import dipy.io.streamline\n",
    "import dipy.reconst\n",
    "import dipy.reconst.csdeconv\n",
    "import dipy.reconst.shm\n",
    "import dipy.direction\n",
    "import dipy.direction.pmf\n",
    "import dipy.tracking\n",
    "import dipy.tracking.local_tracking\n",
    "import dipy.tracking.streamline\n",
    "import dipy.tracking.stopping_criterion\n",
    "\n",
    "from dipy.viz import has_fury\n",
    "\n",
    "if has_fury:\n",
    "    from dipy.viz import window, actor, colormap\n",
    "\n",
    "import pitn\n",
    "\n",
    "plt.rcParams.update({\"figure.autolayout\": True})\n",
    "plt.rcParams.update({\"figure.facecolor\": [1.0, 1.0, 1.0, 1.0]})\n",
    "plt.rcParams.update({\"image.cmap\": \"gray\"})\n",
    "\n",
    "# Set print options for ndarrays/tensors.\n",
    "np.set_printoptions(suppress=True, threshold=100, linewidth=88)\n",
    "torch.set_printoptions(sci_mode=False, threshold=100, linewidth=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c68d20e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T19:34:44.232247Z",
     "iopub.status.busy": "2022-12-01T19:34:44.230892Z",
     "iopub.status.idle": "2022-12-01T19:34:45.789600Z",
     "shell.execute_reply": "2022-12-01T19:34:45.788466Z",
     "shell.execute_reply.started": "2022-12-01T19:34:44.232196Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "direnv: loading ~/Projects/pitn/.envrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update notebook's environment variables with direnv.\n",
    "# This requires the python-dotenv package, and direnv be installed on the system\n",
    "# This will not work on Windows.\n",
    "# NOTE: This is kind of hacky, and not necessarily safe. Be careful...\n",
    "# Libraries needed on the python side:\n",
    "# - os\n",
    "# - subprocess\n",
    "# - io\n",
    "# - dotenv\n",
    "\n",
    "# Form command to be run in direnv's context. This command will print out\n",
    "# all environment variables defined in the subprocess/sub-shell.\n",
    "command = \"direnv exec {} /usr/bin/env\".format(os.getcwd())\n",
    "# Run command in a new subprocess.\n",
    "proc = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, cwd=os.getcwd())\n",
    "# Store and format the subprocess' output.\n",
    "proc_out = proc.communicate()[0].strip().decode(\"utf-8\")\n",
    "# Use python-dotenv to load the environment variables by using the output of\n",
    "# 'direnv exec ...' as a 'dummy' .env file.\n",
    "dotenv.load_dotenv(stream=io.StringIO(proc_out), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26871439",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T19:34:45.791168Z",
     "iopub.status.busy": "2022-12-01T19:34:45.790867Z",
     "iopub.status.idle": "2022-12-01T19:34:45.934893Z",
     "shell.execute_reply": "2022-12-01T19:34:45.933542Z",
     "shell.execute_reply.started": "2022-12-01T19:34:45.791142Z"
    },
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr cap\n",
    "# Capture output and save to log. Needs to be at the *very first* line of the cell.\n",
    "# Watermark\n",
    "%load_ext watermark\n",
    "%watermark --author \"Tyler Spears\" --updated --iso8601  --python --machine --iversions --githash\n",
    "if torch.cuda.is_available():\n",
    "    # GPU information\n",
    "    try:\n",
    "        gpu_info = pitn.utils.system.get_gpu_specs()\n",
    "        print(gpu_info)\n",
    "    except NameError:\n",
    "        print(\"CUDA Version: \", torch.version.cuda)\n",
    "else:\n",
    "    print(\"CUDA not in use, falling back to CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb719750",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T19:34:45.938060Z",
     "iopub.status.busy": "2022-12-01T19:34:45.937653Z",
     "iopub.status.idle": "2022-12-01T19:34:45.987872Z",
     "shell.execute_reply": "2022-12-01T19:34:45.986832Z",
     "shell.execute_reply.started": "2022-12-01T19:34:45.938023Z"
    },
    "tags": [
     "active-ipynb",
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Tyler Spears\n",
      "\n",
      "Last updated: 2022-12-08T14:46:19.051033-05:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.8\n",
      "IPython version      : 8.4.0\n",
      "\n",
      "Compiler    : GCC 10.4.0\n",
      "OS          : Linux\n",
      "Release     : 5.15.0-53-generic\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 20\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: b2184751d5e3d4179c63f8891d0455254791c437\n",
      "\n",
      "torch       : 1.12.1\n",
      "seaborn     : 0.12.1\n",
      "pandas      : 1.5.2\n",
      "nibabel     : 4.0.1\n",
      "sys         : 3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:23:14) [GCC 10.4.0]\n",
      "pitn        : 0.0.post1.dev206+gf002231.d20220911\n",
      "dipy        : 1.5.0\n",
      "transforms3d: 0.4.1\n",
      "einops      : 0.4.1\n",
      "SimpleITK   : 2.2.0\n",
      "matplotlib  : 3.5.2\n",
      "monai       : 1.0.0\n",
      "numpy       : 1.23.4\n",
      "skimage     : 0.19.3\n",
      "fury        : 0.8.0\n",
      "\n",
      "==================================================GPU Specs==================================================\n",
      "  id  Name              Driver Version      CUDA Version  Total Memory    uuid\n",
      "----  ----------------  ----------------  --------------  --------------  ----------------------------------------\n",
      "   0  NVIDIA RTX A5000  520.56.06                   11.6  24564.0MB       GPU-ed20d87f-e88e-692f-0b56-548b8a05ddea\n",
      "   1  NVIDIA RTX A5000  520.56.06                   11.6  24564.0MB       GPU-0636ee40-2eab-9533-1be7-dbbadade95c4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cap is defined in an ipython magic command\n",
    "try:\n",
    "    print(cap)\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d3a5c6",
   "metadata": {},
   "source": [
    "## Experiment & Parameters Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cd75175",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T19:34:45.989091Z",
     "iopub.status.busy": "2022-12-01T19:34:45.988848Z",
     "iopub.status.idle": "2022-12-01T19:34:46.038257Z",
     "shell.execute_reply": "2022-12-01T19:34:46.037481Z",
     "shell.execute_reply.started": "2022-12-01T19:34:45.989068Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Config file not loaded\n"
     ]
    }
   ],
   "source": [
    "p = Box(default_box=True)\n",
    "# Experiment defaults, can be overridden in a config file.\n",
    "\n",
    "# General experiment-wide params\n",
    "###############################################\n",
    "# p.experiment_name = \"interpolation_baseline\"\n",
    "p.override_experiment_name = False\n",
    "p.results_dir = \"/data/srv/outputs/pitn/results/runs\"\n",
    "p.tmp_results_dir = \"/data/srv/outputs/pitn/results/tmp\"\n",
    "\n",
    "p.tvt_split_files = list(Path(\"./data_splits\").glob(\"HCP*train-val-test_split*.csv\"))\n",
    "\n",
    "# If a config file exists, override the defaults with those values.\n",
    "try:\n",
    "    if \"PITN_CONFIG\" in os.environ.keys():\n",
    "        config_fname = Path(os.environ[\"PITN_CONFIG\"])\n",
    "    else:\n",
    "        config_fname = pitn.utils.system.get_file_glob_unique(Path(\".\"), r\"config.*\")\n",
    "    f_type = config_fname.suffix.casefold()\n",
    "    if f_type in {\".yaml\", \".yml\"}:\n",
    "        f_params = Box.from_yaml(filename=config_fname)\n",
    "    elif f_type == \".json\":\n",
    "        f_params = Box.from_json(filename=config_fname)\n",
    "    elif f_type == \".toml\":\n",
    "        f_params = Box.from_toml(filename=config_fname)\n",
    "    else:\n",
    "        raise RuntimeError()\n",
    "\n",
    "    p.merge_update(f_params)\n",
    "\n",
    "except:\n",
    "    print(\"WARNING: Config file not loaded\")\n",
    "    pass\n",
    "\n",
    "# Remove the default_box behavior now that params have been fully read in.\n",
    "_p = Box(default_box=False)\n",
    "_p.merge_update(p)\n",
    "p = _p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79e24dee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T19:34:46.040341Z",
     "iopub.status.busy": "2022-12-01T19:34:46.039611Z",
     "iopub.status.idle": "2022-12-01T19:34:46.087888Z",
     "shell.execute_reply": "2022-12-01T19:34:46.086584Z",
     "shell.execute_reply.started": "2022-12-01T19:34:46.040295Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "subj_ids = set()\n",
    "for f in p.tvt_split_files:\n",
    "    split = pd.read_csv(f)\n",
    "    split_subjs = set(split.subj_id.tolist())\n",
    "    subj_ids = subj_ids | split_subjs\n",
    "\n",
    "p.subj_ids = natsorted(list(subj_ids))\n",
    "p.viz_subjs = random.sample(p.subj_ids, k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1047b3",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6a12cb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T19:34:46.088807Z",
     "iopub.status.busy": "2022-12-01T19:34:46.088643Z",
     "iopub.status.idle": "2022-12-01T19:34:46.112615Z",
     "shell.execute_reply": "2022-12-01T19:34:46.111868Z",
     "shell.execute_reply.started": "2022-12-01T19:34:46.088792Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hcp_full_res_data_dir = Path(\"/data/srv/data/pitn/hcp\")\n",
    "hcp_full_res_fodf_dir = Path(\"/data/srv/outputs/pitn/hcp/full-res/fodf\")\n",
    "hcp_low_res_data_dir = Path(\"/data/srv/outputs/pitn/hcp/downsample/scale-2.00mm/vol\")\n",
    "hcp_low_res_fodf_dir = Path(\"/data/srv/outputs/pitn/hcp/downsample/scale-2.00mm/fodf\")\n",
    "\n",
    "assert hcp_full_res_data_dir.exists()\n",
    "assert hcp_full_res_fodf_dir.exists()\n",
    "assert hcp_low_res_data_dir.exists()\n",
    "assert hcp_low_res_fodf_dir.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bf5a57",
   "metadata": {},
   "source": [
    "### Datasets & DataLoader Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9105038",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T19:34:46.113412Z",
     "iopub.status.busy": "2022-12-01T19:34:46.113258Z",
     "iopub.status.idle": "2022-12-01T19:34:47.188342Z",
     "shell.execute_reply": "2022-12-01T19:34:47.187097Z",
     "shell.execute_reply.started": "2022-12-01T19:34:46.113399Z"
    },
    "lines_to_next_cell": 1,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Warnings caught:\n",
      "\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings(record=True) as warn_list:\n",
    "\n",
    "    # Test dataset.\n",
    "    # The test dataset won't be cached, as each image should only be loaded once.\n",
    "    test_paths_dataset = pitn.data.datasets.HCPfODFINRDataset(\n",
    "        subj_ids=p.subj_ids,\n",
    "        dwi_root_dir=hcp_full_res_data_dir,\n",
    "        fodf_root_dir=hcp_full_res_fodf_dir,\n",
    "        lr_dwi_root_dir=hcp_low_res_data_dir,\n",
    "        lr_fodf_root_dir=hcp_low_res_fodf_dir,\n",
    "        transform=pitn.data.datasets.HCPfODFINRDataset.default_pre_sample_tf(\n",
    "            0, skip_sample_mask=True\n",
    "        ),\n",
    "    )\n",
    "    test_dataset = pitn.data.datasets.HCPfODFINRWholeVolDataset(\n",
    "        test_paths_dataset,\n",
    "        transform=pitn.data.datasets.HCPfODFINRWholeVolDataset.default_tf(),\n",
    "    )\n",
    "\n",
    "print(\"=\" * 10)\n",
    "print(\"Warnings caught:\")\n",
    "ws = \"\\n\".join(\n",
    "    [\n",
    "        warnings.formatwarning(\n",
    "            w.message, w.category, w.filename, w.lineno, w.file, w.line\n",
    "        )\n",
    "        for w in warn_list\n",
    "    ]\n",
    ")\n",
    "ws = \"\\n\".join(filter(lambda s: bool(s.strip()), ws.splitlines()))\n",
    "print(ws, flush=True)\n",
    "print(\"=\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bcee062-1299-46ae-8ff4-33c0502a442a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T19:34:47.191041Z",
     "iopub.status.busy": "2022-12-01T19:34:47.190022Z",
     "iopub.status.idle": "2022-12-01T19:34:47.237728Z",
     "shell.execute_reply": "2022-12-01T19:34:47.237013Z",
     "shell.execute_reply.started": "2022-12-01T19:34:47.190992Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataloader = monai.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    pin_memory=False,\n",
    "    num_workers=0,\n",
    "    # num_workers=3,\n",
    "    # prefetch_factor=3,\n",
    "    # persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343cddbb",
   "metadata": {},
   "source": [
    "## Tractography Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb7f98b7-579f-41d1-8fdf-430921787e2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T19:34:47.239682Z",
     "iopub.status.busy": "2022-12-01T19:34:47.239450Z",
     "iopub.status.idle": "2022-12-01T19:34:47.269352Z",
     "shell.execute_reply": "2022-12-01T19:34:47.268442Z",
     "shell.execute_reply.started": "2022-12-01T19:34:47.239667Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_table = {\"subj_id\": list(), \"mse_mean\": list(), \"mse_var\": list()}\n",
    "sampling_sphere = dipy.data.HemiSphere.from_sphere(\n",
    "    dipy.data.get_sphere(\"repulsion724\").subdivide(1)\n",
    ")\n",
    "gfa_stopping_threshold = 0.25\n",
    "pmf_threshold = 0.1\n",
    "max_angle = 20.0\n",
    "step_size = 0.2\n",
    "max_cross = 4\n",
    "maxlen = 500\n",
    "seed_density = 1\n",
    "\n",
    "sh_transform_mat, _, _ = dipy.reconst.csdeconv.real_sh_descoteaux(\n",
    "    sh_order=8,\n",
    "    theta=sampling_sphere.theta,\n",
    "    phi=sampling_sphere.phi,\n",
    "    full_basis=False,\n",
    "    legacy=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617f5742-1a2f-4bb4-96ab-7c3209cfb106",
   "metadata": {},
   "source": [
    "### Linear Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b427b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearInterpPmfGen(dipy.direction.pmf.SHCoeffPmfGen):\n",
    "    def __init__(self, subj_odf_coeffs, sphere, affine_vox2world, itk_im):\n",
    "        self._subj_odf_coeffs = subj_odf_coeffs.copy().astype(np.double)\n",
    "        dipy.direction.pmf.SHCoeffPmfGen.__init__(\n",
    "            self, self._subj_odf_coeffs, sphere, basis_type=None\n",
    "        )\n",
    "\n",
    "        self._sphere = sphere\n",
    "        self._theta = sphere.theta\n",
    "        self._phi = sphere.phi\n",
    "        self.im = itk_im\n",
    "        self.affine_vox2world = affine_vox2world\n",
    "\n",
    "        self.sh_transform_mat, _, _ = dipy.reconst.csdeconv.real_sh_descoteaux(\n",
    "            sh_order=8, theta=self._theta, phi=self._phi, full_basis=False, legacy=False\n",
    "        )\n",
    "\n",
    "    def batched_sphere_sample(self, odf_coeffs):\n",
    "        orig_spatial_shape = tuple(odf_coeffs.shape[:-1])\n",
    "        coeffs = odf_coeffs.reshape(-1, odf_coeffs.shape[-1])\n",
    "        samples = np.matmul(coeffs, self.sh_transform_mat.T[None])\n",
    "        samples = samples.reshape(*orig_spatial_shape, -1)\n",
    "        samples = np.clip(samples, 0, np.inf)\n",
    "        return samples\n",
    "\n",
    "    def get_pmf(self, point):\n",
    "        # The `point` var is given in voxel space! Need to change to physical space\n",
    "        # first!\n",
    "        point = np.asarray(point)\n",
    "        phys_point = (\n",
    "            self.affine_vox2world[:3, :3] @ point[:, None]\n",
    "        ) + self.affine_vox2world[:3, 3:4]\n",
    "        phys_point = phys_point.flatten().astype(np.double)\n",
    "        interp_coeffs = self.im.EvaluateAtPhysicalPoint(phys_point, sitk.sitkLinear)\n",
    "        interp_coeffs = np.array(interp_coeffs)[None]\n",
    "        pmf = interp_coeffs @ self.sh_transform_mat.T\n",
    "        pmf = np.clip(pmf, 0, np.inf)\n",
    "        pmf = pmf.flatten()\n",
    "        return pmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8769d06-c386-49f3-be35-8c3f0f434fc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T19:36:36.081036Z",
     "iopub.status.busy": "2022-12-01T19:36:36.080382Z",
     "iopub.status.idle": "2022-12-01T19:47:58.915992Z",
     "shell.execute_reply": "2022-12-01T19:47:58.915468Z",
     "shell.execute_reply.started": "2022-12-01T19:36:36.080985Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SHOW_WARNINGS = False\n",
    "\n",
    "RUN_NAME = \"linear_interpolation_baseline\"\n",
    "ts = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "# Break ISO format because many programs don't like having colons ':' in a filename.\n",
    "ts = ts.replace(\":\", \"_\")\n",
    "# tmp_res_dir = Path(p.tmp_results_dir) / \"_\".join([ts, RUN_NAME])\n",
    "# tmp_res_dir.mkdir(parents=True)\n",
    "\n",
    "lin_results = copy.deepcopy(result_table)\n",
    "with warnings.catch_warnings(record=True) as warn_list:\n",
    "    for subj_dict in test_dataloader:\n",
    "        print(\"Loaded subject\")\n",
    "        subj_id = subj_dict[\"subj_id\"][0]\n",
    "        x = subj_dict[\"lr_fodf\"][0].cpu()\n",
    "        mask = subj_dict[\"mask\"][0].bool().cpu()\n",
    "        x_affine = subj_dict[\"affine_lrvox2acpc\"][0].cpu().numpy().astype(np.double)\n",
    "        x_transl, x_rot, x_zoom, x_shear = transforms3d.affines.decompose(x_affine)\n",
    "\n",
    "        x_np = einops.rearrange(x, \"c z y x -> z y x c\").numpy()\n",
    "        x = sitk.GetImageFromArray(x_np)\n",
    "        x.SetSpacing(tuple(x_zoom))\n",
    "        x.SetOrigin(tuple(x_transl))\n",
    "        x.SetDirection(tuple(x_rot.flatten()))\n",
    "\n",
    "        fivett_f = pitn.data.datasets.HCPfODFINRDataset.get_fodf_subj_dict(\n",
    "            subj_id, hcp_low_res_fodf_dir\n",
    "        )[\"fivett\"]\n",
    "        fivett = nib.load(fivett_f)\n",
    "        seeds = dipy.tracking.utils.seeds_from_mask(\n",
    "            fivett.get_fdata().astype(int)[..., 0] == 1,\n",
    "            affine=fivett.affine,\n",
    "            density=seed_density,\n",
    "        )\n",
    "        print(\"Created seeds\")\n",
    "        print(seeds.shape)\n",
    "        pmf_gen = LinearInterpPmfGen(\n",
    "            x_np.astype(np.double),\n",
    "            sphere=sampling_sphere,\n",
    "            affine_vox2world=x_affine,\n",
    "            itk_im=x,\n",
    "        )\n",
    "        odf_samples_vol = pmf_gen.batched_sphere_sample(x_np)\n",
    "        their_pmf_gen = dipy.direction.pmf.SHCoeffPmfGen(\n",
    "            x_np.astype(np.double), sampling_sphere, None\n",
    "        )\n",
    "        print(\"Sampled odf coefficients in data.\")\n",
    "        gfa_x = dipy.reconst.odf.gfa(odf_samples_vol)\n",
    "        print(\"Created gen. fractional anisotropy image.\")\n",
    "        stopping_criterion = (\n",
    "            dipy.tracking.stopping_criterion.ThresholdStoppingCriterion(\n",
    "                gfa_x, gfa_stopping_threshold\n",
    "            )\n",
    "        )\n",
    "        direction_getter = dipy.direction.ClosestPeakDirectionGetter(\n",
    "            # pmf_gen,\n",
    "            their_pmf_gen,\n",
    "            max_angle=max_angle,\n",
    "            sphere=sampling_sphere,\n",
    "            pmf_threshold=pmf_threshold,\n",
    "        )\n",
    "        streamline_gen = dipy.tracking.local_tracking.LocalTracking(\n",
    "            direction_getter,\n",
    "            stopping_criterion=stopping_criterion,\n",
    "            seeds=seeds,\n",
    "            affine=x_affine,\n",
    "            step_size=step_size,\n",
    "            maxlen=maxlen,\n",
    "            max_cross=max_cross,\n",
    "        )\n",
    "        streamlines = dipy.tracking.streamline.Streamlines(streamline_gen)\n",
    "        tractogram = dipy.io.stateful_tractogram.StatefulTractogram(\n",
    "            streamlines,\n",
    "            nib.Nifti1Image(x_np, affine=x_affine),\n",
    "            dipy.io.stateful_tractogram.Space.RASMM,\n",
    "        )\n",
    "\n",
    "        # dipy.io.streamline.save_tck(tractogram, f\"test_streamline_subj-{subj_id}.tck\")\n",
    "        dipy.io.streamline.save_tck(tractogram, f\"test_streamline_subj-{subj_id}.tck\")\n",
    "        break\n",
    "        # if int(subj_id) in p.viz_subjs:\n",
    "        #     print(\"Creating prediction viz\")\n",
    "        #     with mpl.rc_context({\"font.size\": 6.0}):\n",
    "        #         fig = plt.figure(dpi=175, figsize=(9, 5))\n",
    "        #         fig, _ = pitn.viz.plot_fodf_coeff_slices(\n",
    "        #             y_pred,\n",
    "        #             y,\n",
    "        #             y_pred - y,\n",
    "        #             fig=fig,\n",
    "        #             fodf_vol_labels=(\"Predicted\", \"Target\", \"Pred - GT\"),\n",
    "        #             imshow_kwargs={\"interpolation\": \"antialiased\", \"cmap\": \"gray\"},\n",
    "        #         )\n",
    "        #         fig_fname = f\"subj_{subj_id}_{RUN_NAME}_viz.png\"\n",
    "        #         fig.savefig(tmp_res_dir / fig_fname)\n",
    "        #         plt.close(fig)\n",
    "\n",
    "# pd.DataFrame.from_dict(lin_results).to_csv(tmp_res_dir / f\"run_results_{RUN_NAME}.csv\")\n",
    "# shutil.copytree(tmp_res_dir, Path(p.results_dir) / tmp_res_dir.name)\n",
    "\n",
    "if SHOW_WARNINGS:\n",
    "    print(\"=\" * 10)\n",
    "    print(\"Warnings caught:\")\n",
    "    ws = \"\\n\".join(\n",
    "        [\n",
    "            warnings.formatwarning(\n",
    "                w.message, w.category, w.filename, w.lineno, w.file, w.line\n",
    "            )\n",
    "            for w in warn_list\n",
    "        ]\n",
    "    )\n",
    "    ws = \"\\n\".join(filter(lambda s: bool(s.strip()), ws.splitlines()))\n",
    "    print(ws, flush=True)\n",
    "    print(\"=\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bb3f67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "617f5742-1a2f-4bb4-96ab-7c3209cfb106",
   "metadata": {},
   "source": [
    "### INR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac3e9b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weight_f = Path(\n",
    "    \"/data/srv/outputs/pitn/results/runs/2022-12-06T21_40_10__fixed_ensemble_split_03/state_dict_epoch_174_step_35000.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "af01e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definitions\n",
    "\n",
    "\n",
    "class INREncoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        interior_channels: int,\n",
    "        out_channels: int,\n",
    "        n_res_units: int,\n",
    "        n_dense_units: int,\n",
    "        activate_fn,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.interior_channels = interior_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        if isinstance(activate_fn, str):\n",
    "            activate_fn = pitn.utils.torch_lookups.activation[activate_fn]\n",
    "\n",
    "        self._activation_fn_init = activate_fn\n",
    "        self.activate_fn = activate_fn()\n",
    "\n",
    "        # Pad to maintain the same input shape.\n",
    "        self.pre_conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(\n",
    "                self.in_channels,\n",
    "                self.in_channels,\n",
    "                kernel_size=1,\n",
    "                padding=\"same\",\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "            self.activate_fn,\n",
    "            torch.nn.Conv3d(\n",
    "                self.in_channels,\n",
    "                self.interior_channels,\n",
    "                kernel_size=3,\n",
    "                padding=\"same\",\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Construct the densely-connected cascading layers.\n",
    "        # Create n_dense_units number of dense units.\n",
    "        top_level_units = list()\n",
    "        for _ in range(n_dense_units):\n",
    "            # Create n_res_units number of residual units for every dense unit.\n",
    "            res_layers = list()\n",
    "            for _ in range(n_res_units):\n",
    "                res_layers.append(\n",
    "                    pitn.nn.layers.ResBlock3dNoBN(\n",
    "                        self.interior_channels,\n",
    "                        kernel_size=3,\n",
    "                        activate_fn=activate_fn,\n",
    "                        padding=\"same\",\n",
    "                        padding_mode=\"reflect\",\n",
    "                    )\n",
    "                )\n",
    "            top_level_units.append(\n",
    "                pitn.nn.layers.DenseCascadeBlock3d(self.interior_channels, *res_layers)\n",
    "            )\n",
    "\n",
    "        # Wrap everything into a densely-connected cascade.\n",
    "        self.cascade = pitn.nn.layers.DenseCascadeBlock3d(\n",
    "            self.interior_channels, *top_level_units\n",
    "        )\n",
    "\n",
    "        self.post_conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(\n",
    "                self.interior_channels,\n",
    "                self.interior_channels,\n",
    "                kernel_size=5,\n",
    "                padding=\"same\",\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "            self.activate_fn,\n",
    "            torch.nn.Conv3d(\n",
    "                self.interior_channels,\n",
    "                self.out_channels,\n",
    "                kernel_size=3,\n",
    "                padding=\"same\",\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "            self.activate_fn,\n",
    "            torch.nn.ReplicationPad3d((1, 0, 1, 0, 1, 0)),\n",
    "            torch.nn.AvgPool3d(kernel_size=2, stride=1),\n",
    "            torch.nn.Conv3d(\n",
    "                self.out_channels,\n",
    "                self.out_channels,\n",
    "                kernel_size=1,\n",
    "                padding=\"same\",\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "        )\n",
    "        # self.post_conv = torch.nn.Conv3d(\n",
    "        #     self.interior_channels,\n",
    "        #     self.out_channels,\n",
    "        #     kernel_size=3,\n",
    "        #     padding=\"same\",\n",
    "        #     padding_mode=\"reflect\",\n",
    "        # )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        y = self.pre_conv(x)\n",
    "        y = self.activate_fn(y)\n",
    "        y = self.cascade(y)\n",
    "        y = self.activate_fn(y)\n",
    "        y = self.post_conv(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class ReducedDecoder(torch.nn.Module):\n",
    "\n",
    "    TARGET_COORD_EPSILON = 1e-7\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        context_v_features: int,\n",
    "        out_features: int,\n",
    "        m_encode_num_freqs: int,\n",
    "        sigma_encode_scale: float,\n",
    "        in_features=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Determine the number of input features needed for the MLP.\n",
    "        # The order for concatenation is\n",
    "        # 1) ctx feats over the low-res input space, unfolded over a 3x3x3 window\n",
    "        # ~~2) target voxel shape~~\n",
    "        # 3) absolute coords of this forward pass' prediction target\n",
    "        # 4) absolute coords of the high-res target voxel\n",
    "        # ~~5) relative coords between high-res target coords and this forward pass'\n",
    "        #    prediction target, normalized by low-res voxel shape~~\n",
    "        # 6) encoding of relative coords\n",
    "        self.context_v_features = context_v_features\n",
    "        self.ndim = 3\n",
    "        self.m_encode_num_freqs = m_encode_num_freqs\n",
    "        self.sigma_encode_scale = torch.as_tensor(sigma_encode_scale)\n",
    "        self.n_encode_features = self.ndim * 2 * self.m_encode_num_freqs\n",
    "        self.n_coord_features = 2 * self.ndim + self.n_encode_features\n",
    "        self.internal_features = self.context_v_features + self.n_coord_features\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # \"Swish\" function, recommended in MeshFreeFlowNet\n",
    "        activate_cls = torch.nn.SiLU\n",
    "        self.activate_fn = activate_cls(inplace=True)\n",
    "        # Optional resizing linear layer, if the input size should be different than\n",
    "        # the hidden layer size.\n",
    "        if self.in_features is not None:\n",
    "            self.lin_pre = torch.nn.Linear(self.in_features, self.context_v_features)\n",
    "            self.norm_pre = None\n",
    "        else:\n",
    "            self.lin_pre = None\n",
    "            self.norm_pre = None\n",
    "        self.norm_pre = None\n",
    "\n",
    "        # Internal hidden layers are two res MLPs.\n",
    "        self.internal_res_repr = torch.nn.ModuleList(\n",
    "            [\n",
    "                pitn.nn.inr.SkipMLPBlock(\n",
    "                    n_context_features=self.context_v_features,\n",
    "                    n_coord_features=self.n_coord_features,\n",
    "                    n_dense_layers=3,\n",
    "                    activate_fn=activate_cls,\n",
    "                )\n",
    "                for _ in range(2)\n",
    "            ]\n",
    "        )\n",
    "        self.lin_post = torch.nn.Linear(self.context_v_features, self.out_features)\n",
    "\n",
    "    def encode_relative_coord(self, coords):\n",
    "        c = einops.rearrange(coords, \"b d x y z -> (b x y z) d\")\n",
    "        sigma = self.sigma_encode_scale.expand_as(c).to(c)[..., None]\n",
    "        encode_pos = pitn.nn.inr.fourier_position_encoding(\n",
    "            c, sigma_scale=sigma, m_num_freqs=self.m_encode_num_freqs\n",
    "        )\n",
    "\n",
    "        encode_pos = einops.rearrange(\n",
    "            encode_pos,\n",
    "            \"(b x y z) d -> b d x y z\",\n",
    "            x=coords.shape[2],\n",
    "            y=coords.shape[3],\n",
    "            z=coords.shape[4],\n",
    "        )\n",
    "        return encode_pos\n",
    "\n",
    "    def sub_grid_forward(\n",
    "        self,\n",
    "        context_val,\n",
    "        context_coord,\n",
    "        query_coord,\n",
    "        context_vox_size,\n",
    "        query_vox_size,\n",
    "        return_rel_context_coord=False,\n",
    "    ):\n",
    "        # Take relative coordinate difference between the current context\n",
    "        # coord and the query coord.\n",
    "        # rel_context_coord = context_coord - query_coord + self.TARGET_COORD_EPSILON\n",
    "        rel_context_coord = torch.clamp_min(\n",
    "            context_coord - query_coord,\n",
    "            (-context_vox_size / 2) + self.TARGET_COORD_EPSILON,\n",
    "        )\n",
    "        # Also normalize to [0, 1)\n",
    "        # Coordinates are located in the center of the voxel. By the way\n",
    "        # the context vector is being constructed surrounding the query\n",
    "        # coord, the query coord is always within 1.5 x vox_size of the\n",
    "        # context (low-res space) coordinate. So, subtract the\n",
    "        # batch-and-channel-wise minimum, and divide by the known upper\n",
    "        # bound.\n",
    "        rel_norm_context_coord = (\n",
    "            rel_context_coord\n",
    "            - torch.amin(rel_context_coord, dim=(2, 3, 4), keepdim=True)\n",
    "        ) / (1.5 * context_vox_size)\n",
    "        assert (rel_norm_context_coord >= 0).all() and (\n",
    "            rel_norm_context_coord < 1.0\n",
    "        ).all()\n",
    "        encoded_rel_norm_context_coord = self.encode_relative_coord(\n",
    "            rel_norm_context_coord\n",
    "        )\n",
    "\n",
    "        # Perform forward pass of the MLP.\n",
    "        if self.norm_pre is not None:\n",
    "            context_val = self.norm_pre(context_val)\n",
    "        context_feats = einops.rearrange(context_val, \"b c x y z -> (b x y z) c\")\n",
    "\n",
    "        # q_vox_size = query_vox_size.expand_as(rel_norm_context_coord)\n",
    "        coord_feats = (\n",
    "            # q_vox_size,\n",
    "            context_coord,\n",
    "            query_coord,\n",
    "            # rel_norm_context_coord,\n",
    "            encoded_rel_norm_context_coord,\n",
    "        )\n",
    "        coord_feats = torch.cat(coord_feats, dim=1)\n",
    "        spatial_layout = {\n",
    "            \"b\": coord_feats.shape[0],\n",
    "            \"x\": coord_feats.shape[2],\n",
    "            \"y\": coord_feats.shape[3],\n",
    "            \"z\": coord_feats.shape[4],\n",
    "        }\n",
    "\n",
    "        coord_feats = einops.rearrange(coord_feats, \"b c x y z -> (b x y z) c\")\n",
    "        x_coord = coord_feats\n",
    "        sub_grid_pred = context_feats\n",
    "\n",
    "        if self.lin_pre is not None:\n",
    "            sub_grid_pred = self.lin_pre(sub_grid_pred)\n",
    "            sub_grid_pred = self.activate_fn(sub_grid_pred)\n",
    "\n",
    "        for l in self.internal_res_repr:\n",
    "            sub_grid_pred, x_coord = l(sub_grid_pred, x_coord)\n",
    "        sub_grid_pred = self.lin_post(sub_grid_pred)\n",
    "        sub_grid_pred = einops.rearrange(\n",
    "            sub_grid_pred, \"(b x y z) c -> b c x y z\", **spatial_layout\n",
    "        )\n",
    "        if return_rel_context_coord:\n",
    "            ret = (sub_grid_pred, rel_context_coord)\n",
    "        else:\n",
    "            ret = sub_grid_pred\n",
    "        return ret\n",
    "\n",
    "    def equal_space_forward(self, context_v, context_spatial_extent, context_vox_size):\n",
    "        return self.sub_grid_forward(\n",
    "            context_val=context_v,\n",
    "            context_coord=context_spatial_extent,\n",
    "            query_coord=context_spatial_extent,\n",
    "            context_vox_size=context_vox_size,\n",
    "            query_vox_size=context_vox_size,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        context_v,\n",
    "        context_spatial_extent,\n",
    "        query_vox_size,\n",
    "        query_coord,\n",
    "    ) -> torch.Tensor:\n",
    "        if query_vox_size.ndim == 2:\n",
    "            query_vox_size = query_vox_size[:, :, None, None, None]\n",
    "        context_vox_size = torch.abs(\n",
    "            context_spatial_extent[..., 1, 1, 1] - context_spatial_extent[..., 0, 0, 0]\n",
    "        )\n",
    "        context_vox_size = context_vox_size[:, :, None, None, None]\n",
    "\n",
    "        # If the context space and the query coordinates are equal, then we are actually\n",
    "        # just mapping within the same physical space to the same coordinates. So,\n",
    "        # linear interpolation would just zero-out all surrounding predicted voxels,\n",
    "        # and would be a massive waste of computation.\n",
    "        if (\n",
    "            (context_spatial_extent.shape == query_coord.shape)\n",
    "            and torch.isclose(context_spatial_extent, query_coord).all()\n",
    "            and torch.isclose(query_vox_size, context_vox_size).all()\n",
    "        ):\n",
    "            y = self.equal_space_forward(\n",
    "                context_v=context_v,\n",
    "                context_spatial_extent=context_spatial_extent,\n",
    "                context_vox_size=context_vox_size,\n",
    "            )\n",
    "        # More commonly, the input space will not equal the output space, and the\n",
    "        # prediction will need to be interpolated.\n",
    "        else:\n",
    "            batch_size = query_coord.shape[0]\n",
    "            # Construct a grid of nearest indices in context space by sampling a grid of\n",
    "            # *indices* given the coordinates in mm.\n",
    "            # The channel dim is just repeated for every\n",
    "            # channel, so that doesn't need to be in the idx grid.\n",
    "            nearest_coord_idx = torch.stack(\n",
    "                torch.meshgrid(\n",
    "                    *[\n",
    "                        torch.arange(0, context_spatial_extent.shape[i])\n",
    "                        for i in (0, 2, 3, 4)\n",
    "                    ],\n",
    "                    indexing=\"ij\",\n",
    "                ),\n",
    "                dim=1,\n",
    "            ).to(context_spatial_extent)\n",
    "\n",
    "            # Find the nearest grid point, where the batch+spatial dims are the\n",
    "            # \"channels.\"\n",
    "            nearest_coord_idx = pitn.nn.inr.weighted_ctx_v(\n",
    "                encoded_feat_vol=nearest_coord_idx,\n",
    "                input_space_extent=context_spatial_extent,\n",
    "                target_space_extent=query_coord,\n",
    "                reindex_spatial_extents=True,\n",
    "                sample_mode=\"nearest\",\n",
    "            ).to(torch.long)\n",
    "            # Expand along channel dimension for raw indexing.\n",
    "            nearest_coord_idx = einops.rearrange(\n",
    "                nearest_coord_idx, \"b dim i j k -> dim (b i j k)\"\n",
    "            )\n",
    "            batch_idx = nearest_coord_idx[0]\n",
    "\n",
    "            # Use the world coordinates to determine the necessary voxel coordinate\n",
    "            # offsets such that the offsets enclose the query point.\n",
    "            # World coordinate in the low-res input grid that is closest to the\n",
    "            # query coordinate.\n",
    "            phys_coords_0 = context_spatial_extent[\n",
    "                batch_idx,\n",
    "                :,\n",
    "                nearest_coord_idx[1],\n",
    "                nearest_coord_idx[2],\n",
    "                nearest_coord_idx[3],\n",
    "            ]\n",
    "\n",
    "            phys_coords_0 = einops.rearrange(\n",
    "                phys_coords_0,\n",
    "                \"(b x y z) c -> b c x y z\",\n",
    "                b=batch_size,\n",
    "                c=query_coord.shape[1],\n",
    "                x=query_coord.shape[2],\n",
    "                y=query_coord.shape[3],\n",
    "                z=query_coord.shape[4],\n",
    "            )\n",
    "            # Determine the quadrants that the query point lies in relative to the\n",
    "            # context grid. We only care about the spatial/non-batch coordinates.\n",
    "            surround_query_point_quadrants = (\n",
    "                query_coord - self.TARGET_COORD_EPSILON - phys_coords_0\n",
    "            )\n",
    "            # 3 x batch_and_spatial_size\n",
    "            # The signs of the \"query coordinate - grid coordinate\" should match the\n",
    "            # direction the indexing should go for the nearest voxels to the query.\n",
    "            surround_offsets_vox = einops.rearrange(\n",
    "                surround_query_point_quadrants.sign(), \"b dim i j k -> dim (b i j k)\"\n",
    "            ).to(torch.int8)\n",
    "            del surround_query_point_quadrants\n",
    "\n",
    "            # Now, find sum of distances to normalize the distance-weighted weight vector\n",
    "            # for in-place 'linear interpolation.'\n",
    "            inv_dist_total = torch.zeros_like(phys_coords_0)\n",
    "            inv_dist_total = (inv_dist_total[:, 0])[:, None]\n",
    "            surround_offsets_vox_volume_order = einops.rearrange(\n",
    "                surround_offsets_vox,\n",
    "                \"dim (b i j k) -> b dim i j k\",\n",
    "                b=batch_size,\n",
    "                i=query_coord.shape[2],\n",
    "                j=query_coord.shape[3],\n",
    "                k=query_coord.shape[4],\n",
    "            )\n",
    "            for (\n",
    "                offcenter_indicate_i,\n",
    "                offcenter_indicate_j,\n",
    "                offcenter_indicate_k,\n",
    "            ) in itertools.product((0, 1), (0, 1), (0, 1)):\n",
    "                phys_coords_offset = torch.ones_like(phys_coords_0)\n",
    "                phys_coords_offset[:, 0] *= (\n",
    "                    offcenter_indicate_i * surround_offsets_vox_volume_order[:, 0]\n",
    "                ) * context_vox_size[:, 0]\n",
    "                phys_coords_offset[:, 1] *= (\n",
    "                    offcenter_indicate_j * surround_offsets_vox_volume_order[:, 1]\n",
    "                ) * context_vox_size[:, 1]\n",
    "                phys_coords_offset[:, 2] *= (\n",
    "                    offcenter_indicate_k * surround_offsets_vox_volume_order[:, 2]\n",
    "                ) * context_vox_size[:, 2]\n",
    "                # phys_coords_offset = context_vox_size * phys_coords_offset\n",
    "                phys_coords = phys_coords_0 + phys_coords_offset\n",
    "                inv_dist_total += 1 / torch.linalg.vector_norm(\n",
    "                    query_coord - phys_coords, ord=2, dim=1, keepdim=True\n",
    "                )\n",
    "            # Potentially free some memory here.\n",
    "            del phys_coords\n",
    "            del phys_coords_0\n",
    "            del phys_coords_offset\n",
    "            del surround_offsets_vox_volume_order\n",
    "\n",
    "            y_weighted_accumulate = None\n",
    "            # Build the low-res representation one sub-window voxel index at a time.\n",
    "            # The indicators specify if the current voxel index that surrounds the\n",
    "            # query coordinate should be \"off the center voxel\" or not. If not, then\n",
    "            # the center voxel (read: no voxel offset from the center) is selected\n",
    "            # (for that dimension).\n",
    "            for (\n",
    "                offcenter_indicate_i,\n",
    "                offcenter_indicate_j,\n",
    "                offcenter_indicate_k,\n",
    "            ) in itertools.product((0, 1), (0, 1), (0, 1)):\n",
    "                # Rebuild indexing tuple for each element of the sub-window\n",
    "                i_idx = nearest_coord_idx[1] + (\n",
    "                    offcenter_indicate_i * surround_offsets_vox[0]\n",
    "                )\n",
    "                j_idx = nearest_coord_idx[2] + (\n",
    "                    offcenter_indicate_j * surround_offsets_vox[1]\n",
    "                )\n",
    "                k_idx = nearest_coord_idx[3] + (\n",
    "                    offcenter_indicate_k * surround_offsets_vox[2]\n",
    "                )\n",
    "                context_val = context_v[batch_idx, :, i_idx, j_idx, k_idx]\n",
    "                context_val = einops.rearrange(\n",
    "                    context_val,\n",
    "                    \"(b x y z) c -> b c x y z\",\n",
    "                    b=batch_size,\n",
    "                    x=query_coord.shape[2],\n",
    "                    y=query_coord.shape[3],\n",
    "                    z=query_coord.shape[4],\n",
    "                )\n",
    "                context_coord = context_spatial_extent[\n",
    "                    batch_idx, :, i_idx, j_idx, k_idx\n",
    "                ]\n",
    "                context_coord = einops.rearrange(\n",
    "                    context_coord,\n",
    "                    \"(b x y z) c -> b c x y z\",\n",
    "                    b=batch_size,\n",
    "                    x=query_coord.shape[2],\n",
    "                    y=query_coord.shape[3],\n",
    "                    z=query_coord.shape[4],\n",
    "                )\n",
    "\n",
    "                sub_grid_pred_ijk = self.sub_grid_forward(\n",
    "                    context_val=context_val,\n",
    "                    context_coord=context_coord,\n",
    "                    query_coord=query_coord,\n",
    "                    context_vox_size=context_vox_size,\n",
    "                    query_vox_size=query_vox_size,\n",
    "                    return_rel_context_coord=False,\n",
    "                )\n",
    "                # Initialize the accumulated prediction after finding the\n",
    "                # output size; easier than trying to pre-compute it.\n",
    "                if y_weighted_accumulate is None:\n",
    "                    y_weighted_accumulate = torch.zeros_like(sub_grid_pred_ijk)\n",
    "\n",
    "                # Weigh this cell's prediction by the inverse of the distance\n",
    "                # from the cell physical coordinate to the true target\n",
    "                # physical coordinate. Normalize the weight by the inverse\n",
    "                # \"sum of the inverse distances\" found before.\n",
    "                w = (\n",
    "                    1\n",
    "                    / torch.linalg.vector_norm(\n",
    "                        query_coord - context_coord, ord=2, dim=1, keepdim=True\n",
    "                    )\n",
    "                ) / inv_dist_total\n",
    "\n",
    "                # Accumulate weighted cell predictions to eventually create\n",
    "                # the final prediction.\n",
    "                y_weighted_accumulate += w * sub_grid_pred_ijk\n",
    "                del sub_grid_pred_ijk\n",
    "\n",
    "            y = y_weighted_accumulate\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class INRSystemLoader(LightningLite):\n",
    "    def run(self, checkpoint_state_dict_f):\n",
    "\n",
    "        encoder = INREncoder(\n",
    "            in_channels=189,\n",
    "            interior_channels=80,\n",
    "            out_channels=128,\n",
    "            n_res_units=3,\n",
    "            n_dense_units=3,\n",
    "            activate_fn=\"relu\",\n",
    "        )\n",
    "        encoder = self.setup(encoder)\n",
    "        decoder = ReducedDecoder(\n",
    "            context_v_features=128,\n",
    "            in_features=128,\n",
    "            out_features=45,\n",
    "            m_encode_num_freqs=36,\n",
    "            sigma_encode_scale=3.0,\n",
    "        )\n",
    "        decoder = self.setup(decoder)\n",
    "        checkpoint = self.load(checkpoint_state_dict_f)\n",
    "        encoder.load_state_dict(checkpoint[\"encoder\"])\n",
    "        decoder.load_state_dict(checkpoint[\"decoder\"])\n",
    "        encoder = self.to_device(encoder)\n",
    "        decoder = self.to_device(decoder)\n",
    "\n",
    "        return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b427b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class INRPredictionPmfGen(dipy.direction.pmf.SHCoeffPmfGen):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_model,\n",
    "        decoder_model,\n",
    "        subj_dwi_data,\n",
    "        subj_dwi_coord_grid,\n",
    "        sphere,\n",
    "        affine_lrvox2world: np.ndarray,\n",
    "        query_vox_size,\n",
    "    ):\n",
    "        self.device = encoder_model.device\n",
    "        encoder_model.eval()\n",
    "        self.decoder = decoder_model.to(self.device)\n",
    "        self.decoder.eval()\n",
    "        self.query_vox_size = (\n",
    "            torch.as_tensor(query_vox_size).to(self.device).reshape(1, -1, 1, 1, 1)\n",
    "        )\n",
    "        self.subj_dwi_coord_grid = torch.as_tensor(subj_dwi_coord_grid).to(self.device)\n",
    "\n",
    "        self.sphere = sphere\n",
    "        self.theta, self.phi = pitn.odf.get_torch_sample_sphere_coords(\n",
    "            self.sphere, self.device, self.subj_dwi_coord_grid.dtype\n",
    "        )\n",
    "        self.theta = self.theta.to(self.device)\n",
    "        self.phi = self.phi.to(self.device)\n",
    "        self.affine_lrvox2world = affine_lrvox2world\n",
    "        self.sh_order = 8\n",
    "\n",
    "        self.encoded_dwi = encoder_model(subj_dwi_data)\n",
    "\n",
    "    def batched_sphere_sample(self, odf_coeffs):\n",
    "        coeffs = torch.as_tensor(odf_coeffs).to(self.device)\n",
    "        samples = pitn.odf.sample_sphere_coords(\n",
    "            coeffs, theta=self.theta, phi=self.phi, sh_order=self.sh_order\n",
    "        )\n",
    "        return samples\n",
    "\n",
    "    def get_pmf(self, point):\n",
    "        # The `point` var is given in voxel space! Need to change to physical space\n",
    "        # first!\n",
    "        point = np.asarray(point)\n",
    "        phys_point = (\n",
    "            self.affine_lrvox2world[:3, :3] @ point[:, None]\n",
    "        ) + self.affine_lrvox2world[:3, 3:4]\n",
    "        phys_point = torch.from_numpy(phys_point.flatten().astype(np.double))\n",
    "        query_coord = phys_point.reshape(1, -1, 1, 1, 1).to(self.subj_dwi_coord_grid)\n",
    "\n",
    "        pred_odf_coeff = self.decoder(\n",
    "            self.encoded_dwi,\n",
    "            context_spatial_extent=self.subj_dwi_coord_grid,\n",
    "            query_vox_size=self.query_vox_size,\n",
    "            query_coord=query_coord,\n",
    "        )\n",
    "\n",
    "        pmf = pitn.odf.sample_sphere_coords(\n",
    "            pred_odf_coeff, theta=self.theta, phi=self.phi, sh_order=self.sh_order\n",
    "        )\n",
    "        pmf = pmf.flatten().detach().cpu().numpy().astype(np.double)\n",
    "\n",
    "        return pmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e8769d06-c386-49f3-be35-8c3f0f434fc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T19:36:36.081036Z",
     "iopub.status.busy": "2022-12-01T19:36:36.080382Z",
     "iopub.status.idle": "2022-12-01T19:47:58.915992Z",
     "shell.execute_reply": "2022-12-01T19:47:58.915468Z",
     "shell.execute_reply.started": "2022-12-01T19:36:36.080985Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/tas6hh/Projects/pitn/notebooks/continuous_sr/tractography_eval.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/tractography_eval.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m ts \u001b[39m=\u001b[39m ts\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/tractography_eval.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# tmp_res_dir = Path(p.tmp_results_dir) / \"_\".join([ts, RUN_NAME])\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/tractography_eval.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# tmp_res_dir.mkdir(parents=True)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/tractography_eval.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m loader_system \u001b[39m=\u001b[39m INRSystemLoader(accelerator\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpu\u001b[39;49m\u001b[39m\"\u001b[39;49m, devices\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, precision\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/tractography_eval.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m encoder, decoder \u001b[39m=\u001b[39m loader_system\u001b[39m.\u001b[39mrun(model_weight_f)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btanager.cpe.virginia.edu/home/tas6hh/Projects/pitn/notebooks/continuous_sr/tractography_eval.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(encoder\u001b[39m.\u001b[39mdevice, decoder\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda/envs/pitn2/lib/python3.10/site-packages/pytorch_lightning/lite/lite.py:84\u001b[0m, in \u001b[0;36mLightningLite.__init__\u001b[0;34m(self, accelerator, strategy, devices, num_nodes, precision, plugins, gpus, tpu_cores)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_accelerator_support(accelerator)\n\u001b[1;32m     83\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_strategy_support(strategy)\n\u001b[0;32m---> 84\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_connector \u001b[39m=\u001b[39m AcceleratorConnector(\n\u001b[1;32m     85\u001b[0m     num_processes\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     86\u001b[0m     devices\u001b[39m=\u001b[39;49mdevices,\n\u001b[1;32m     87\u001b[0m     tpu_cores\u001b[39m=\u001b[39;49mtpu_cores,\n\u001b[1;32m     88\u001b[0m     ipus\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     89\u001b[0m     accelerator\u001b[39m=\u001b[39;49maccelerator,\n\u001b[1;32m     90\u001b[0m     strategy\u001b[39m=\u001b[39;49mstrategy,\n\u001b[1;32m     91\u001b[0m     gpus\u001b[39m=\u001b[39;49mgpus,\n\u001b[1;32m     92\u001b[0m     num_nodes\u001b[39m=\u001b[39;49mnum_nodes,\n\u001b[1;32m     93\u001b[0m     sync_batchnorm\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# TODO: add support?\u001b[39;49;00m\n\u001b[1;32m     94\u001b[0m     benchmark\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     95\u001b[0m     replace_sampler_ddp\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     96\u001b[0m     deterministic\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     97\u001b[0m     precision\u001b[39m=\u001b[39;49mprecision,\n\u001b[1;32m     98\u001b[0m     amp_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mnative\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     99\u001b[0m     amp_level\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    100\u001b[0m     plugins\u001b[39m=\u001b[39;49mplugins,\n\u001b[1;32m    101\u001b[0m     auto_select_gpus\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    102\u001b[0m )\n\u001b[1;32m    103\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_connector\u001b[39m.\u001b[39mstrategy\n\u001b[1;32m    104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy\u001b[39m.\u001b[39maccelerator\n",
      "File \u001b[0;32m~/miniconda/envs/pitn2/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:212\u001b[0m, in \u001b[0;36mAcceleratorConnector.__init__\u001b[0;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, amp_type, amp_level, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, auto_select_gpus, num_processes, tpu_cores, ipus, gpus)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_flag \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_choose_auto_accelerator()\n\u001b[1;32m    211\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_flag \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 212\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_flag \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_choose_gpu_accelerator_backend()\n\u001b[1;32m    214\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_parallel_devices_and_init_accelerator()\n\u001b[1;32m    216\u001b[0m \u001b[39m# 3. Instantiate ClusterEnvironment\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/pitn2/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:515\u001b[0m, in \u001b[0;36mAcceleratorConnector._choose_gpu_accelerator_backend\u001b[0;34m()\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[39mif\u001b[39;00m MPSAccelerator\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m    514\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 515\u001b[0m \u001b[39mif\u001b[39;00m CUDAAccelerator\u001b[39m.\u001b[39;49mis_available():\n\u001b[1;32m    516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    518\u001b[0m \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39mNo supported gpu backend found!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda/envs/pitn2/lib/python3.10/site-packages/pytorch_lightning/accelerators/cuda.py:91\u001b[0m, in \u001b[0;36mCUDAAccelerator.is_available\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_available\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m---> 91\u001b[0m     \u001b[39mreturn\u001b[39;00m device_parser\u001b[39m.\u001b[39;49mnum_cuda_devices() \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda/envs/pitn2/lib/python3.10/site-packages/pytorch_lightning/utilities/device_parser.py:348\u001b[0m, in \u001b[0;36mnum_cuda_devices\u001b[0;34m()\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[1;32m    347\u001b[0m \u001b[39mwith\u001b[39;00m multiprocessing\u001b[39m.\u001b[39mget_context(\u001b[39m\"\u001b[39m\u001b[39mfork\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mPool(\u001b[39m1\u001b[39m) \u001b[39mas\u001b[39;00m pool:\n\u001b[0;32m--> 348\u001b[0m     \u001b[39mreturn\u001b[39;00m pool\u001b[39m.\u001b[39;49mapply(torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mdevice_count)\n",
      "File \u001b[0;32m~/miniconda/envs/pitn2/lib/python3.10/multiprocessing/pool.py:360\u001b[0m, in \u001b[0;36mPool.apply\u001b[0;34m(self, func, args, kwds)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, func, args\u001b[39m=\u001b[39m(), kwds\u001b[39m=\u001b[39m{}):\n\u001b[1;32m    356\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[39m    Equivalent of `func(*args, **kwds)`.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[39m    Pool must be running.\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_async(func, args, kwds)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/miniconda/envs/pitn2/lib/python3.10/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    769\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/pitn2/lib/python3.10/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m~/miniconda/envs/pitn2/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda/envs/pitn2/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "SHOW_WARNINGS = False\n",
    "\n",
    "RUN_NAME = \"linear_interpolation_baseline\"\n",
    "ts = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "# Break ISO format because many programs don't like having colons ':' in a filename.\n",
    "ts = ts.replace(\":\", \"_\")\n",
    "# tmp_res_dir = Path(p.tmp_results_dir) / \"_\".join([ts, RUN_NAME])\n",
    "# tmp_res_dir.mkdir(parents=True)\n",
    "\n",
    "loader_system = INRSystemLoader(accelerator=\"gpu\", devices=1, precision=32)\n",
    "encoder, decoder = loader_system.run(model_weight_f)\n",
    "print(encoder.device, decoder.device)\n",
    "dev = encoder.device\n",
    "# The target vox size isn't used right now, so just make a dummy vox size to satisfy the\n",
    "# function args.\n",
    "dummy_vox_size = torch.as_tensor([1.25, 1.25, 1.25]).reshape(1, -1).to(dev)\n",
    "\n",
    "with warnings.catch_warnings(record=True) as warn_list:\n",
    "    with torch.no_grad():\n",
    "        for subj_dict in test_dataloader:\n",
    "            print(\"Loaded subject\")\n",
    "            subj_id = subj_dict[\"subj_id\"][0]\n",
    "            x = subj_dict[\"lr_dwi\"].to(dev)\n",
    "            x_np = subj_dict[\"lr_dwi\"].cpu().numpy()[0]\n",
    "            x_np = einops.rearrange(x_np, \"c i j k -> i j k c\")\n",
    "            lr_mask = subj_dict[\"lr_mask\"].bool().to(dev)\n",
    "            lr_fodf = subj_dict[\"lr_fodf\"].to(dev)\n",
    "            x = x * lr_mask\n",
    "            x_affine = subj_dict[\"affine_lrvox2acpc\"][0].cpu().numpy().astype(np.double)\n",
    "            x_coords = subj_dict[\"lr_extent_acpc\"].to(dev)\n",
    "\n",
    "            fivett_f = pitn.data.datasets.HCPfODFINRDataset.get_fodf_subj_dict(\n",
    "                subj_id, hcp_low_res_fodf_dir\n",
    "            )[\"fivett\"]\n",
    "            fivett = nib.load(fivett_f)\n",
    "            seeds = dipy.tracking.utils.seeds_from_mask(\n",
    "                fivett.get_fdata().astype(int)[..., 0] == 1,\n",
    "                affine=fivett.affine,\n",
    "                density=seed_density,\n",
    "            )\n",
    "            print(\"Created seeds\")\n",
    "            print(seeds.shape)\n",
    "\n",
    "            pmf_gen = INRPredictionPmfGen(\n",
    "                encoder,\n",
    "                decoder,\n",
    "                subj_dwi_data=x,\n",
    "                subj_dwi_coord_grid=x_coords,\n",
    "                sphere=sampling_sphere,\n",
    "                affine_lrvox2world=x_affine,\n",
    "                query_vox_size=dummy_vox_size,\n",
    "            )\n",
    "            odf_samples_vol = pmf_gen.batched_sphere_sample(lr_fodf)\n",
    "            odf_samples_vol = odf_samples_vol[0].cpu().numpy()\n",
    "            odf_samples_vol = np.moveaxis(odf_samples_vol, 1, -1)\n",
    "            print(\"Sampled odf coefficients in data.\")\n",
    "            gfa_x = dipy.reconst.odf.gfa(odf_samples_vol)\n",
    "            print(\"Created gen. fractional anisotropy image.\")\n",
    "            stopping_criterion = (\n",
    "                dipy.tracking.stopping_criterion.ThresholdStoppingCriterion(\n",
    "                    gfa_x, gfa_stopping_threshold\n",
    "                )\n",
    "            )\n",
    "            direction_getter = dipy.direction.ClosestPeakDirectionGetter(\n",
    "                pmf_gen,\n",
    "                max_angle=max_angle,\n",
    "                sphere=sampling_sphere,\n",
    "                pmf_threshold=pmf_threshold,\n",
    "            )\n",
    "            streamline_gen = dipy.tracking.local_tracking.LocalTracking(\n",
    "                direction_getter,\n",
    "                stopping_criterion=stopping_criterion,\n",
    "                seeds=seeds,\n",
    "                affine=x_affine,\n",
    "                step_size=step_size,\n",
    "                maxlen=maxlen,\n",
    "                max_cross=max_cross,\n",
    "            )\n",
    "            streamlines = dipy.tracking.streamline.Streamlines(streamline_gen)\n",
    "            tractogram = dipy.io.stateful_tractogram.StatefulTractogram(\n",
    "                streamlines,\n",
    "                nib.Nifti1Image(x_np, affine=x_affine),\n",
    "                dipy.io.stateful_tractogram.Space.RASMM,\n",
    "            )\n",
    "\n",
    "            # dipy.io.streamline.save_tck(tractogram, f\"test_streamline_subj-{subj_id}.tck\")\n",
    "            dipy.io.streamline.save_tck(\n",
    "                tractogram, f\"test_streamline_trained-INR_subj-{subj_id}.tck\"\n",
    "            )\n",
    "            break\n",
    "            # if int(subj_id) in p.viz_subjs:\n",
    "            #     print(\"Creating prediction viz\")\n",
    "            #     with mpl.rc_context({\"font.size\": 6.0}):\n",
    "            #         fig = plt.figure(dpi=175, figsize=(9, 5))\n",
    "            #         fig, _ = pitn.viz.plot_fodf_coeff_slices(\n",
    "            #             y_pred,\n",
    "            #             y,\n",
    "            #             y_pred - y,\n",
    "            #             fig=fig,\n",
    "            #             fodf_vol_labels=(\"Predicted\", \"Target\", \"Pred - GT\"),\n",
    "            #             imshow_kwargs={\"interpolation\": \"antialiased\", \"cmap\": \"gray\"},\n",
    "            #         )\n",
    "            #         fig_fname = f\"subj_{subj_id}_{RUN_NAME}_viz.png\"\n",
    "            #         fig.savefig(tmp_res_dir / fig_fname)\n",
    "            #         plt.close(fig)\n",
    "\n",
    "# pd.DataFrame.from_dict(lin_results).to_csv(tmp_res_dir / f\"run_results_{RUN_NAME}.csv\")\n",
    "# shutil.copytree(tmp_res_dir, Path(p.results_dir) / tmp_res_dir.name)\n",
    "\n",
    "if SHOW_WARNINGS:\n",
    "    print(\"=\" * 10)\n",
    "    print(\"Warnings caught:\")\n",
    "    ws = \"\\n\".join(\n",
    "        [\n",
    "            warnings.formatwarning(\n",
    "                w.message, w.category, w.filename, w.lineno, w.file, w.line\n",
    "            )\n",
    "            for w in warn_list\n",
    "        ]\n",
    "    )\n",
    "    ws = \"\\n\".join(filter(lambda s: bool(s.strip()), ws.splitlines()))\n",
    "    print(ws, flush=True)\n",
    "    print(\"=\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dc2615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ed7d20-b8d7-4f48-b665-06bdc1bba8b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-01T19:53:30.999675Z",
     "iopub.status.busy": "2022-12-01T19:53:30.999068Z",
     "iopub.status.idle": "2022-12-01T19:53:31.056877Z",
     "shell.execute_reply": "2022-12-01T19:53:31.056087Z",
     "shell.execute_reply.started": "2022-12-01T19:53:30.999626Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(lin_results)\n",
    "df.mse_mean.median()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "comment_magics": true,
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:23:14) [GCC 10.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "f41faa2479836806c9664d670a156675ad0f09912fd4b0aed749f41e3cac86f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
