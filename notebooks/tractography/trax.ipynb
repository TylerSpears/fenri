{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ae4cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import collections\n",
    "import functools\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "import datetime\n",
    "\n",
    "# Change default behavior of jax GPU memory allocation.\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "# os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \".20\"\n",
    "# Disable cuda blocking **for debugging only!**\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# visualization libraries\n",
    "%matplotlib inline\n",
    "from pprint import pprint\n",
    "\n",
    "import dipy\n",
    "import dipy.denoise\n",
    "import dipy.io\n",
    "import dipy.io.streamline\n",
    "import dipy.reconst\n",
    "import dipy.reconst.csdeconv\n",
    "import dipy.reconst.shm\n",
    "import dipy.viz\n",
    "import einops\n",
    "import functorch\n",
    "import jax\n",
    "import jax.config\n",
    "import jax.dlpack\n",
    "import jax.numpy as jnp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import monai\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import skimage\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from icecream import ic\n",
    "from jax import lax\n",
    "\n",
    "import pitn\n",
    "\n",
    "# Disable flags for debugging.\n",
    "# jax.config.update(\"jax_disable_jit\", True)\n",
    "# jax.config.update(\"jax_debug_nans\", True)\n",
    "# jax.config.update(\"jax_debug_infs\", True)\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "# jax.config.update(\"jax_default_matmul_precision\", 32)\n",
    "\n",
    "\n",
    "plt.rcParams.update({\"figure.autolayout\": True})\n",
    "plt.rcParams.update({\"figure.facecolor\": [1.0, 1.0, 1.0, 1.0]})\n",
    "plt.rcParams.update({\"image.cmap\": \"gray\"})\n",
    "plt.rcParams.update({\"image.interpolation\": \"antialiased\"})\n",
    "\n",
    "# Set print options for ndarrays/tensors.\n",
    "np.set_printoptions(suppress=True, threshold=100, linewidth=88)\n",
    "torch.set_printoptions(sci_mode=False, threshold=100, linewidth=88)\n",
    "\n",
    "# MODEL_SELECTION = \"inr\"\n",
    "MODEL_SELECTION = \"tri-linear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cae81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch setup\n",
    "# allow for CUDA usage, if available\n",
    "if torch.cuda.is_available():\n",
    "    # Pick only one device for the default, may use multiple GPUs for training later.\n",
    "    dev_idx = 0 if MODEL_SELECTION.casefold() == \"inr\" else 1\n",
    "    # dev_idx = 1\n",
    "    device = torch.device(f\"cuda:{dev_idx}\")\n",
    "    print(\"CUDA Device IDX \", dev_idx)\n",
    "    torch.cuda.set_device(device)\n",
    "    print(\"CUDA Current Device \", torch.cuda.current_device())\n",
    "    print(\"CUDA Device properties: \", torch.cuda.get_device_properties(device))\n",
    "    # The flag below controls whether to allow TF32 on matmul. This flag defaults to False\n",
    "    # in PyTorch 1.12 and later.\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    # See\n",
    "    # <https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices>\n",
    "    # for details.\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "    # Activate cudnn benchmarking to optimize convolution algorithm speed.\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        print(\"CuDNN convolution optimization enabled.\")\n",
    "        # The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# keep device as the cpu\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa466444",
   "metadata": {},
   "outputs": [],
   "source": [
    "hcp_full_res_data_dir = Path(\"/data/srv/data/pitn/hcp\")\n",
    "hcp_full_res_fodf_dir = Path(\"/data/srv/outputs/pitn/hcp/full-res/fodf\")\n",
    "hcp_low_res_data_dir = Path(\"/data/srv/outputs/pitn/hcp/downsample/scale-2.00mm/vol\")\n",
    "hcp_low_res_fodf_dir = Path(\"/data/srv/outputs/pitn/hcp/downsample/scale-2.00mm/fodf\")\n",
    "fibercup_fodf_dir = Path(\"/data/srv/outputs/fibercup/fiberfox_replication/B1-3/fodf\")\n",
    "tmp_results_dir = Path(\"/tmp\") / Path(\"/data/srv/outputs/pitn/results/tmp\")\n",
    "\n",
    "assert hcp_full_res_data_dir.exists()\n",
    "assert hcp_full_res_fodf_dir.exists()\n",
    "assert hcp_low_res_data_dir.exists()\n",
    "assert hcp_low_res_fodf_dir.exists()\n",
    "assert fibercup_fodf_dir.exists()\n",
    "assert tmp_results_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b595aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "# Break ISO format because many programs don't like having colons ':' in a filename.\n",
    "ts = ts.replace(\":\", \"_\")\n",
    "tmp_res_dir = Path(tmp_results_dir) / f\"{ts}__tractography_{MODEL_SELECTION}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f1f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_weights_f = (\n",
    "    Path(\"/data/srv/outputs/pitn/results\")\n",
    "    / \"tmp\"\n",
    "    / \"2023-02-22T01_29_25\"\n",
    "    / \"state_dict_epoch_249_step_50000.pt\"\n",
    ")\n",
    "assert network_weights_f.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4458c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "max_sh_order = 8\n",
    "\n",
    "# Seed creation.\n",
    "peaks_per_seed_vox = 2\n",
    "# Total seeds per voxel will be `seeds_per_vox_axis`^3\n",
    "seeds_per_vox_axis = 3\n",
    "# seed_batch_size = 20000\n",
    "seed_batch_size = 20\n",
    "\n",
    "# Threshold parameter for peak finding in the seed voxels.\n",
    "# Element-wise filtering of sphere samples.\n",
    "fodf_sample_min_val = 0.005\n",
    "fodf_sample_min_quantile_thresh = 0.001\n",
    "dipy_relative_peak_threshold = 0.2\n",
    "dipy_min_separation_angle = 20\n",
    "\n",
    "# RK4 estimation\n",
    "step_size = 0.4\n",
    "alpha_exponential_moving_avg = 1.0\n",
    "tracking_fodf_sample_min_val = 0.05\n",
    "# Seems that MRtrix's Newton optimization also has a tolerance value of 0.001, max\n",
    "# iterations of 100.\n",
    "# <https://github.com/MRtrix3/mrtrix3/blob/f633dfd7e9080f71877ea6a4619dabdde99a0fb6/src/dwi/tractography/SIFT2/coeff_optimiser.cpp#L369>\n",
    "grad_descent_kwargs = dict(\n",
    "    tol=1e-3,\n",
    "    stepsize=1.0,\n",
    "    maxiter=100,\n",
    "    # acceleration=True,\n",
    "    acceleration=False,\n",
    "    implicit_diff=True,\n",
    "    # implicit_diff=False,\n",
    "    jit=True,\n",
    "    # unroll=True,\n",
    ")\n",
    "\n",
    "# Stopping & invalidation criteria.\n",
    "min_streamline_len = 10\n",
    "max_streamline_len = 300\n",
    "fa_min_threshold = 0.05\n",
    "max_angular_thresh_rad = torch.pi / 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da6373d",
   "metadata": {},
   "source": [
    "## Seed-Based Tractography Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068ca0c2",
   "metadata": {},
   "source": [
    "### Data & Parameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f656535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HCP or fibercup\n",
    "dataset_selection = \"HCP\"\n",
    "SUBJECT_ID = \"191336\"\n",
    "SEED_MASK_FNAME = \"postproc_5tt_parcellation.nii.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4156b8d8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# HCP Subject scan.\n",
    "sample_fod_f = (\n",
    "    hcp_low_res_fodf_dir / SUBJECT_ID / \"T1w\" / \"postproc_wm_msmt_csd_fod.nii.gz\"\n",
    ")\n",
    "fod_coeff_im = nib.load(sample_fod_f)\n",
    "fod_coeff_im = nib.as_closest_canonical(fod_coeff_im)\n",
    "print(\"Original shape\", fod_coeff_im.shape)\n",
    "print(\"Original affine\", fod_coeff_im.affine)\n",
    "\n",
    "sample_dwi_f = hcp_low_res_data_dir / SUBJECT_ID / \"T1w\" / \"Diffusion\" / \"data.nii.gz\"\n",
    "dwi_im = nib.load(sample_dwi_f)\n",
    "dwi_im = nib.as_closest_canonical(dwi_im)\n",
    "\n",
    "mask_f = sample_fod_f.parent / \"postproc_nodif_brain_mask.nii.gz\"\n",
    "mask_im = nib.load(mask_f)\n",
    "mask_im = nib.as_closest_canonical(mask_im)\n",
    "\n",
    "fa_f = sample_fod_f.parent / \"dti-fa.nii.gz\"\n",
    "fa_im = nib.load(fa_f)\n",
    "fa_im = nib.as_closest_canonical(fa_im)\n",
    "\n",
    "seed_roi_mask_f = sample_fod_f.parent / SEED_MASK_FNAME\n",
    "seed_roi_mask_im = nib.load(seed_roi_mask_f)\n",
    "seed_roi_mask_im = nib.as_closest_canonical(seed_roi_mask_im)\n",
    "seed_roi_mask_im = seed_roi_mask_im.slicer[..., 2]\n",
    "seed_roi_mask_data = seed_roi_mask_im.get_fdata().astype(bool)\n",
    "# Reduce white matter seed area with binary erosion to focus seeding towards the\n",
    "# center of the white matter tracts.\n",
    "seed_roi_mask_data = skimage.morphology.binary_erosion(\n",
    "    seed_roi_mask_data, skimage.morphology.octahedron(2)\n",
    ")\n",
    "# Randomly sub-select seed voxels.\n",
    "random_select_mask = np.random.uniform(0, 1, size=seed_roi_mask_data.shape)\n",
    "random_select_mask = random_select_mask < 0.05\n",
    "seed_roi_mask_data = seed_roi_mask_data * random_select_mask\n",
    "print(\"Num voxels in seed mask: \", seed_roi_mask_data.sum())\n",
    "seed_roi_mask_im = nib.Nifti1Image(\n",
    "    seed_roi_mask_data,\n",
    "    affine=seed_roi_mask_im.affine,\n",
    "    header=seed_roi_mask_im.header,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d66ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-orient volumes from RAS to SAR (xyz -> zyx)\n",
    "nib_affine_vox2ras_mm = fod_coeff_im.affine\n",
    "affine_ras_vox2ras_mm = torch.from_numpy(nib_affine_vox2ras_mm).to(device)\n",
    "ornt_ras = nib.orientations.io_orientation(nib_affine_vox2ras_mm)\n",
    "ornt_sar = nib.orientations.axcodes2ornt((\"S\", \"A\", \"R\"))\n",
    "ornt_ras2sar = nib.orientations.ornt_transform(ornt_ras, ornt_sar)\n",
    "# We also need an affine that maps from SAR -> RAS\n",
    "affine_sar2ras = nib.orientations.inv_ornt_aff(\n",
    "    ornt_ras2sar, tuple(fod_coeff_im.shape[:-1])\n",
    ")\n",
    "affine_sar2ras = torch.from_numpy(affine_sar2ras).to(affine_ras_vox2ras_mm)\n",
    "affine_ras2sar = torch.linalg.inv(affine_sar2ras)\n",
    "\n",
    "# This essentially just flips the translation vector in the affine matrix. It may be\n",
    "# \"RAS\" relative to the object/volume itself, but it is \"SAR\" relative to the original\n",
    "# ordering of the dimensions in the data.\n",
    "affine_sar_vox2sar_mm = affine_ras2sar @ (affine_ras_vox2ras_mm @ affine_sar2ras)\n",
    "\n",
    "# Swap spatial dimensions, assign a new vox->world affine space.\n",
    "sar_fod = einops.rearrange(fod_coeff_im.get_fdata(), \"x y z coeffs -> z y x coeffs\")\n",
    "fod_coeff_im = nib.Nifti1Image(\n",
    "    sar_fod,\n",
    "    affine=affine_sar_vox2sar_mm.cpu().numpy(),\n",
    "    header=fod_coeff_im.header,\n",
    ")\n",
    "sar_dwi = einops.rearrange(dwi_im.get_fdata(), \"x y z b_grads -> z y x b_grads\")\n",
    "dwi_im = nib.Nifti1Image(\n",
    "    sar_dwi, affine=affine_sar_vox2sar_mm.cpu().numpy(), header=dwi_im.header\n",
    ")\n",
    "\n",
    "sar_mask = einops.rearrange(mask_im.get_fdata().astype(bool), \"x y z -> z y x\")\n",
    "mask_im = nib.Nifti1Image(\n",
    "    sar_mask,\n",
    "    affine=affine_sar_vox2sar_mm.cpu().numpy(),\n",
    "    header=mask_im.header,\n",
    ")\n",
    "sar_fa = einops.rearrange(fa_im.get_fdata(), \"x y z -> z y x\")\n",
    "fa_im = nib.Nifti1Image(\n",
    "    sar_fa,\n",
    "    affine=affine_sar_vox2sar_mm.cpu().numpy(),\n",
    "    header=fa_im.header,\n",
    ")\n",
    "\n",
    "sar_roi_seed_mask = einops.rearrange(\n",
    "    seed_roi_mask_im.get_fdata().astype(bool), \"x y z -> z y x\"\n",
    ")\n",
    "seed_roi_mask_im = nib.Nifti1Image(\n",
    "    sar_roi_seed_mask,\n",
    "    affine=affine_sar_vox2sar_mm.cpu().numpy(),\n",
    "    header=seed_roi_mask_im.header,\n",
    ")\n",
    "\n",
    "print(fod_coeff_im.affine)\n",
    "print(fod_coeff_im.shape)\n",
    "print(mask_im.affine)\n",
    "print(mask_im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef9e5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = fod_coeff_im.get_fdata()\n",
    "coeffs = torch.from_numpy(coeffs).to(device)\n",
    "# Move to channels-first layout.\n",
    "coeffs = einops.rearrange(coeffs, \"z y x coeffs -> coeffs z y x\")\n",
    "fod_coeff_im.uncache()\n",
    "\n",
    "brain_mask = mask_im.get_fdata().astype(bool)\n",
    "brain_mask = torch.from_numpy(brain_mask).to(device)\n",
    "brain_mask = einops.rearrange(brain_mask, \"z y x -> 1 z y x\")\n",
    "mask_im.uncache()\n",
    "\n",
    "dwi = dwi_im.get_fdata()\n",
    "dwi = torch.from_numpy(dwi).to(device)\n",
    "dwi = einops.rearrange(dwi, \"z y x b_grads -> b_grads z y x\")\n",
    "dwi = dwi * brain_mask\n",
    "dwi_im.uncache()\n",
    "\n",
    "\n",
    "fa = torch.from_numpy(fa_im.get_fdata()).to(device)\n",
    "fa = einops.rearrange(fa, \"z y x -> 1 z y x\")\n",
    "fa = fa * brain_mask\n",
    "fa_im.uncache()\n",
    "\n",
    "seed_roi_mask = torch.from_numpy(seed_roi_mask_im.get_fdata().astype(bool)).to(device)\n",
    "seed_roi_mask = einops.rearrange(seed_roi_mask, \"z y x -> 1 z y x\")\n",
    "seed_roi_mask = seed_roi_mask * brain_mask\n",
    "seed_roi_mask_im.uncache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d0eeb5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "seed_mask = torch.zeros_like(brain_mask).bool()\n",
    "\n",
    "selected_seed_vox_name = SEED_MASK_FNAME.replace(\".nii.gz\", \"\")\n",
    "#!DEBUG\n",
    "# seed_roi_mask = 0 * seed_roi_mask\n",
    "# seed_roi_mask[:, 16:17, 38:39, 33:34] = True\n",
    "# roi_shape = seed_roi_mask.shape\n",
    "# s_roi = einops.rearrange(seed_roi_mask, \"1 z y x -> (z y x)\")\n",
    "# mask_idx = torch.where(s_roi)\n",
    "# s_roi = s_roi * 0\n",
    "# s_roi[(mask_idx[0][140:142],)] = True\n",
    "# seed_roi_mask = einops.rearrange(\n",
    "#     s_roi,\n",
    "#     \"(z y x) -> 1 z y x\",\n",
    "#     z=seed_roi_mask.shape[1],\n",
    "#     y=seed_roi_mask.shape[2],\n",
    "# )\n",
    "# #!\n",
    "seed_mask = (1 + seed_mask) * seed_roi_mask\n",
    "\n",
    "\n",
    "print(coeffs.shape)\n",
    "print(brain_mask.shape)\n",
    "print(seed_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c41adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_sphere = dipy.data.get_sphere(\"repulsion724\")\n",
    "\n",
    "seed_theta, seed_phi = pitn.odf.get_torch_sample_sphere_coords(\n",
    "    seed_sphere, coeffs.device, coeffs.dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784a2ee4",
   "metadata": {},
   "source": [
    "### Tractography Reconstruction Loop - Trilinear Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e53994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp is x,y,z tuple of scipy.sparse.lil_arrays\n",
    "# full streamline list is x,y,z tuple of scipy.sparse.csr_arrays\n",
    "# After every seed batch, the remaining temp tracts are row-wise stacked onto the full\n",
    "# streamline list with scipy.sparse.vstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e5805c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "sh_degrees = torch.arange(0, max_sh_order + 1, step=2).to(device)\n",
    "sh_orders = torch.concatenate(\n",
    "    [torch.arange(-n_, n_ + 1).to(device) for n_ in sh_degrees]\n",
    ").to(device)\n",
    "sh_degrees = torch.concatenate(\n",
    "    [(torch.arange(-n_, n_ + 1).to(device) * 0) + n_ for n_ in sh_degrees]\n",
    ").to(device)\n",
    "\n",
    "peak_finder_fn_theta_phi_c2theta_phi = pitn.tract.peak.get_grad_descent_peak_finder_fn(\n",
    "    sh_orders=sh_orders,\n",
    "    sh_degrees=sh_degrees,\n",
    "    degree_max=max_sh_order,\n",
    "    batch_size=seed_batch_size,\n",
    "    min_sphere_val=tracking_fodf_sample_min_val,\n",
    "    **grad_descent_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd80148",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Tri-linear interpolation functions.\n",
    "def _fn_linear_interp_zyx_tangent_t2theta_phi(\n",
    "    target_coords_mm_zyx: torch.Tensor,\n",
    "    init_direction_theta_phi: Optional[torch.Tensor],\n",
    "    fodf_coeffs_brain_vol: torch.Tensor,\n",
    "    affine_vox2mm: torch.Tensor,\n",
    "    fn_peak_finder,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # Initial interpolation of fodf coefficients at the target points.\n",
    "    pred_sample_fodf_coeffs = pitn.odf.sample_odf_coeffs_lin_interp(\n",
    "        target_coords_mm_zyx,\n",
    "        fodf_coeff_vol=fodf_coeffs_brain_vol,\n",
    "        affine_vox2mm=affine_vox2mm,\n",
    "    )\n",
    "\n",
    "    # The previous outgoing direction is not really the true \"incoming\" direction in\n",
    "    # the new voxel, but it is located on the opposite hemisphere in the new voxel.\n",
    "    # However, the peak finding locates the peak nearest the given initialization\n",
    "    # direction, so it would just be two consecutive mirrorings on the sphere, which\n",
    "    # is obviously identity.\n",
    "    outgoing_theta, outgoing_phi = (\n",
    "        init_direction_theta_phi[..., 0],\n",
    "        init_direction_theta_phi[..., 1],\n",
    "    )\n",
    "    init_direction_theta_phi = (outgoing_theta, outgoing_phi)\n",
    "    result_direction_theta_phi = fn_peak_finder(\n",
    "        pred_sample_fodf_coeffs, init_direction_theta_phi\n",
    "    )\n",
    "\n",
    "    return result_direction_theta_phi\n",
    "\n",
    "\n",
    "fn_linear_interp_zyx_tangent_t2theta_phi = partial(\n",
    "    _fn_linear_interp_zyx_tangent_t2theta_phi,\n",
    "    fodf_coeffs_brain_vol=coeffs,\n",
    "    affine_vox2mm=affine_sar_vox2sar_mm,\n",
    "    fn_peak_finder=peak_finder_fn_theta_phi_c2theta_phi,\n",
    ")\n",
    "\n",
    "\n",
    "def _fn_linear_interp_spatial_fodf_sample(\n",
    "    coords_zyx: torch.Tensor,\n",
    "    directions_theta_phi: torch.Tensor,\n",
    "    fodf_coeffs_brain_vol: torch.Tensor,\n",
    "    affine_vox2mm: torch.Tensor,\n",
    "    batch_size: int,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # Initial interpolation of fodf coefficients at the target points.\n",
    "    pred_sample_fodf_coeffs = pitn.odf.sample_odf_coeffs_lin_interp(\n",
    "        coords_zyx,\n",
    "        fodf_coeff_vol=fodf_coeffs_brain_vol,\n",
    "        affine_vox2mm=affine_vox2mm,\n",
    "    )\n",
    "    theta = directions_theta_phi[..., 0]\n",
    "    phi = directions_theta_phi[..., 1]\n",
    "    Y_basis = pitn.tract.peak.sh_basis_mrtrix3(\n",
    "        theta=theta, phi=phi, batch_size=batch_size\n",
    "    )\n",
    "    Y_basis = einops.rearrange(Y_basis, \"b sh_idx -> b sh_idx 1\")\n",
    "    pred_sample_fodf_coeffs = einops.rearrange(\n",
    "        pred_sample_fodf_coeffs, \"b sh_idx -> b 1 sh_idx\"\n",
    "    )\n",
    "    samples = torch.bmm(pred_sample_fodf_coeffs, Y_basis)\n",
    "    samples.squeeze_()\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "fn_linear_interp_spatial_fodf_sample = partial(\n",
    "    _fn_linear_interp_spatial_fodf_sample,\n",
    "    fodf_coeffs_brain_vol=coeffs,\n",
    "    affine_vox2mm=affine_sar_vox2sar_mm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118a7181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INR Interpolator\n",
    "# Encoding model\n",
    "class INREncoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        interior_channels: int,\n",
    "        out_channels: int,\n",
    "        n_res_units: int,\n",
    "        n_dense_units: int,\n",
    "        activate_fn,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.init_kwargs = dict(\n",
    "            in_channels=in_channels,\n",
    "            interior_channels=interior_channels,\n",
    "            out_channels=out_channels,\n",
    "            n_res_units=n_res_units,\n",
    "            n_dense_units=n_dense_units,\n",
    "            activate_fn=activate_fn,\n",
    "        )\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.interior_channels = interior_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        if isinstance(activate_fn, str):\n",
    "            activate_fn = pitn.utils.torch_lookups.activation[activate_fn]\n",
    "\n",
    "        self._activation_fn_init = activate_fn\n",
    "        self.activate_fn = activate_fn()\n",
    "\n",
    "        # Pad to maintain the same input shape.\n",
    "        self.pre_conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(\n",
    "                self.in_channels,\n",
    "                self.in_channels,\n",
    "                kernel_size=1,\n",
    "                padding=\"same\",\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "            self.activate_fn,\n",
    "            torch.nn.Conv3d(\n",
    "                self.in_channels,\n",
    "                self.interior_channels,\n",
    "                kernel_size=3,\n",
    "                padding=\"same\",\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Construct the densely-connected cascading layers.\n",
    "        # Create n_dense_units number of dense units.\n",
    "        top_level_units = list()\n",
    "        for _ in range(n_dense_units):\n",
    "            # Create n_res_units number of residual units for every dense unit.\n",
    "            res_layers = list()\n",
    "            for _ in range(n_res_units):\n",
    "                res_layers.append(\n",
    "                    pitn.nn.layers.ResBlock3dNoBN(\n",
    "                        self.interior_channels,\n",
    "                        kernel_size=3,\n",
    "                        activate_fn=activate_fn,\n",
    "                        padding=\"same\",\n",
    "                        padding_mode=\"reflect\",\n",
    "                    )\n",
    "                )\n",
    "            top_level_units.append(\n",
    "                pitn.nn.layers.DenseCascadeBlock3d(self.interior_channels, *res_layers)\n",
    "            )\n",
    "\n",
    "        # Wrap everything into a densely-connected cascade.\n",
    "        self.cascade = pitn.nn.layers.DenseCascadeBlock3d(\n",
    "            self.interior_channels, *top_level_units\n",
    "        )\n",
    "\n",
    "        self.post_conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv3d(\n",
    "                self.interior_channels,\n",
    "                self.interior_channels,\n",
    "                kernel_size=5,\n",
    "                padding=\"same\",\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "            self.activate_fn,\n",
    "            torch.nn.Conv3d(\n",
    "                self.interior_channels,\n",
    "                self.out_channels,\n",
    "                kernel_size=3,\n",
    "                padding=\"same\",\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "            self.activate_fn,\n",
    "            torch.nn.ReplicationPad3d((1, 0, 1, 0, 1, 0)),\n",
    "            torch.nn.AvgPool3d(kernel_size=2, stride=1),\n",
    "            torch.nn.Conv3d(\n",
    "                self.out_channels,\n",
    "                self.out_channels,\n",
    "                kernel_size=1,\n",
    "                padding=\"same\",\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "        )\n",
    "        # self.post_conv = torch.nn.Conv3d(\n",
    "        #     self.interior_channels,\n",
    "        #     self.out_channels,\n",
    "        #     kernel_size=3,\n",
    "        #     padding=\"same\",\n",
    "        #     padding_mode=\"reflect\",\n",
    "        # )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        y = self.pre_conv(x)\n",
    "        y = self.activate_fn(y)\n",
    "        y = self.cascade(y)\n",
    "        y = self.activate_fn(y)\n",
    "        y = self.post_conv(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class ReducedDecoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        context_v_features: int,\n",
    "        out_features: int,\n",
    "        m_encode_num_freqs: int,\n",
    "        sigma_encode_scale: float,\n",
    "        in_features=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.init_kwargs = dict(\n",
    "            context_v_features=context_v_features,\n",
    "            out_features=out_features,\n",
    "            m_encode_num_freqs=m_encode_num_freqs,\n",
    "            sigma_encode_scale=sigma_encode_scale,\n",
    "            in_features=in_features,\n",
    "        )\n",
    "\n",
    "        # Determine the number of input features needed for the MLP.\n",
    "        # The order for concatenation is\n",
    "        # 1) ctx feats over the low-res input space, unfolded over a 3x3x3 window\n",
    "        # ~~2) target voxel shape~~\n",
    "        # 3) absolute coords of this forward pass' prediction target\n",
    "        # 4) absolute coords of the high-res target voxel\n",
    "        # ~~5) relative coords between high-res target coords and this forward pass'\n",
    "        #    prediction target, normalized by low-res voxel shape~~\n",
    "        # 6) encoding of relative coords\n",
    "        self.context_v_features = context_v_features\n",
    "        self.ndim = 3\n",
    "        self.m_encode_num_freqs = m_encode_num_freqs\n",
    "        self.sigma_encode_scale = torch.as_tensor(sigma_encode_scale)\n",
    "        self.n_encode_features = self.ndim * 2 * self.m_encode_num_freqs\n",
    "        self.n_coord_features = 2 * self.ndim + self.n_encode_features\n",
    "        self.internal_features = self.context_v_features + self.n_coord_features\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # \"Swish\" function, recommended in MeshFreeFlowNet\n",
    "        activate_cls = torch.nn.SiLU\n",
    "        self.activate_fn = activate_cls(inplace=True)\n",
    "        # Optional resizing linear layer, if the input size should be different than\n",
    "        # the hidden layer size.\n",
    "        if self.in_features is not None:\n",
    "            self.lin_pre = torch.nn.Linear(self.in_features, self.context_v_features)\n",
    "            self.norm_pre = None\n",
    "        else:\n",
    "            self.lin_pre = None\n",
    "            self.norm_pre = None\n",
    "        self.norm_pre = None\n",
    "\n",
    "        # Internal hidden layers are two res MLPs.\n",
    "        self.internal_res_repr = torch.nn.ModuleList(\n",
    "            [\n",
    "                pitn.nn.inr.SkipMLPBlock(\n",
    "                    n_context_features=self.context_v_features,\n",
    "                    n_coord_features=self.n_coord_features,\n",
    "                    n_dense_layers=3,\n",
    "                    activate_fn=activate_cls,\n",
    "                )\n",
    "                for _ in range(2)\n",
    "            ]\n",
    "        )\n",
    "        self.lin_post = torch.nn.Linear(self.context_v_features, self.out_features)\n",
    "\n",
    "    def encode_relative_coord(self, coords):\n",
    "        c = einops.rearrange(coords, \"b d x y z -> (b x y z) d\")\n",
    "        sigma = self.sigma_encode_scale.expand_as(c).to(c)[..., None]\n",
    "        encode_pos = pitn.nn.inr.fourier_position_encoding(\n",
    "            c, sigma_scale=sigma, m_num_freqs=self.m_encode_num_freqs\n",
    "        )\n",
    "\n",
    "        encode_pos = einops.rearrange(\n",
    "            encode_pos,\n",
    "            \"(b x y z) d -> b d x y z\",\n",
    "            x=coords.shape[2],\n",
    "            y=coords.shape[3],\n",
    "            z=coords.shape[4],\n",
    "        )\n",
    "        return encode_pos\n",
    "\n",
    "    def sub_grid_forward(\n",
    "        self,\n",
    "        context_val,\n",
    "        context_coord,\n",
    "        query_coord,\n",
    "        context_vox_size,\n",
    "        # query_vox_size,\n",
    "        return_rel_context_coord=False,\n",
    "    ):\n",
    "        # Take relative coordinate difference between the current context\n",
    "        # coord and the query coord.\n",
    "        rel_context_coord = query_coord - context_coord\n",
    "        # Also normalize to [0, 1) by subtracting the lower bound of differences\n",
    "        # (- voxel size) and dividing by 2xupper bound (2 x voxel size).\n",
    "        rel_norm_context_coord = (rel_context_coord - -context_vox_size) / (\n",
    "            2 * context_vox_size\n",
    "        )\n",
    "        rel_norm_context_coord.round_(decimals=5)\n",
    "        assert (rel_norm_context_coord >= 0).all() and (\n",
    "            rel_norm_context_coord <= 1.0\n",
    "        ).all()\n",
    "        encoded_rel_norm_context_coord = self.encode_relative_coord(\n",
    "            rel_norm_context_coord\n",
    "        )\n",
    "\n",
    "        # Perform forward pass of the MLP.\n",
    "        if self.norm_pre is not None:\n",
    "            context_val = self.norm_pre(context_val)\n",
    "        context_feats = einops.rearrange(context_val, \"b c x y z -> (b x y z) c\")\n",
    "\n",
    "        # q_vox_size = query_vox_size.expand_as(rel_norm_context_coord)\n",
    "        coord_feats = (\n",
    "            # q_vox_size,\n",
    "            context_coord,\n",
    "            query_coord,\n",
    "            # rel_norm_context_coord,\n",
    "            encoded_rel_norm_context_coord,\n",
    "        )\n",
    "        coord_feats = torch.cat(coord_feats, dim=1)\n",
    "        spatial_layout = {\n",
    "            \"b\": coord_feats.shape[0],\n",
    "            \"x\": coord_feats.shape[2],\n",
    "            \"y\": coord_feats.shape[3],\n",
    "            \"z\": coord_feats.shape[4],\n",
    "        }\n",
    "\n",
    "        coord_feats = einops.rearrange(coord_feats, \"b c x y z -> (b x y z) c\")\n",
    "        x_coord = coord_feats\n",
    "        sub_grid_pred = context_feats\n",
    "\n",
    "        if self.lin_pre is not None:\n",
    "            sub_grid_pred = self.lin_pre(sub_grid_pred)\n",
    "            sub_grid_pred = self.activate_fn(sub_grid_pred)\n",
    "\n",
    "        for l in self.internal_res_repr:\n",
    "            sub_grid_pred, x_coord = l(sub_grid_pred, x_coord)\n",
    "        sub_grid_pred = self.lin_post(sub_grid_pred)\n",
    "        sub_grid_pred = einops.rearrange(\n",
    "            sub_grid_pred, \"(b x y z) c -> b c x y z\", **spatial_layout\n",
    "        )\n",
    "        if return_rel_context_coord:\n",
    "            ret = (sub_grid_pred, rel_context_coord)\n",
    "        else:\n",
    "            ret = sub_grid_pred\n",
    "        return ret\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        context_v,\n",
    "        context_spatial_extent,\n",
    "        affine_context_vox2mm,\n",
    "        # query_vox_size,\n",
    "        query_coord,\n",
    "    ) -> torch.Tensor:\n",
    "        # if query_vox_size.ndim == 2:\n",
    "        #     query_vox_size = query_vox_size[:, :, None, None, None]\n",
    "        context_vox_size = torch.abs(\n",
    "            context_spatial_extent[..., 1, 1, 1] - context_spatial_extent[..., 0, 0, 0]\n",
    "        )\n",
    "        context_vox_size = context_vox_size[:, :, None, None, None]\n",
    "\n",
    "        batch_size = query_coord.shape[0]\n",
    "\n",
    "        query_coord_in_context_fov = query_coord - torch.amin(\n",
    "            context_spatial_extent, (2, 3, 4), keepdim=True\n",
    "        )\n",
    "        query_bottom_back_left_corner_coord = (\n",
    "            query_coord_in_context_fov - (query_coord_in_context_fov % context_vox_size)\n",
    "        ) + torch.amin(context_spatial_extent, (2, 3, 4), keepdim=True)\n",
    "        context_vox_bottom_back_left_corner = pitn.affine.coord_transform_3d(\n",
    "            query_bottom_back_left_corner_coord.movedim(1, -1),\n",
    "            torch.linalg.inv(affine_context_vox2mm),\n",
    "        )\n",
    "        context_vox_bottom_back_left_corner = (\n",
    "            context_vox_bottom_back_left_corner.movedim(-1, 1)\n",
    "        )\n",
    "        batch_vox_idx = einops.repeat(\n",
    "            torch.arange(\n",
    "                batch_size,\n",
    "                dtype=context_vox_bottom_back_left_corner.dtype,\n",
    "                device=context_vox_bottom_back_left_corner.device,\n",
    "            ),\n",
    "            \"idx_b -> idx_b 1 i j k\",\n",
    "            idx_b=batch_size,\n",
    "            i=query_coord.shape[2],\n",
    "            j=query_coord.shape[3],\n",
    "            k=query_coord.shape[4],\n",
    "        )\n",
    "        #     (context_vox_bottom_back_left_corner.shape[0], 1)\n",
    "        #     + tuple(context_vox_bottom_back_left_corner.shape[2:])\n",
    "        # )\n",
    "        context_vox_bottom_back_left_corner = torch.cat(\n",
    "            [batch_vox_idx, context_vox_bottom_back_left_corner], dim=1\n",
    "        )\n",
    "        context_vox_bottom_back_left_corner = (\n",
    "            context_vox_bottom_back_left_corner.floor().long()\n",
    "        )\n",
    "        # Slice with a range to keep the \"1\" dimension in place.\n",
    "        batch_vox_idx = context_vox_bottom_back_left_corner[:, 0:1]\n",
    "\n",
    "        y_weighted_accumulate = None\n",
    "        # Build the low-res representation one sub-window voxel index at a time.\n",
    "        # The indicators specify if the current voxel index that surrounds the\n",
    "        # query coordinate should be \"off the center voxel\" or not. If not, then\n",
    "        # the center voxel (read: no voxel offset from the center) is selected\n",
    "        # (for that dimension).\n",
    "        for (\n",
    "            corner_offset_i,\n",
    "            corner_offset_j,\n",
    "            corner_offset_k,\n",
    "        ) in itertools.product((0, 1), (0, 1), (0, 1)):\n",
    "            # Rebuild indexing tuple for each element of the sub-window\n",
    "            sub_window_offset_ijk = query_bottom_back_left_corner_coord.new_tensor(\n",
    "                [corner_offset_i, corner_offset_j, corner_offset_k]\n",
    "            ).reshape(1, -1, 1, 1, 1)\n",
    "            corner_offset_mm = sub_window_offset_ijk * context_vox_size\n",
    "\n",
    "            i_idx = context_vox_bottom_back_left_corner[:, 1:2] + corner_offset_i\n",
    "            j_idx = context_vox_bottom_back_left_corner[:, 2:3] + corner_offset_j\n",
    "            k_idx = context_vox_bottom_back_left_corner[:, 3:4] + corner_offset_k\n",
    "            context_val = context_v[\n",
    "                batch_vox_idx.flatten(),\n",
    "                :,\n",
    "                i_idx.flatten(),\n",
    "                j_idx.flatten(),\n",
    "                k_idx.flatten(),\n",
    "            ]\n",
    "            context_val = einops.rearrange(\n",
    "                context_val,\n",
    "                \"(b x y z) c -> b c x y z\",\n",
    "                b=batch_size,\n",
    "                x=query_coord.shape[2],\n",
    "                y=query_coord.shape[3],\n",
    "                z=query_coord.shape[4],\n",
    "            )\n",
    "            context_coord = query_bottom_back_left_corner_coord + corner_offset_mm\n",
    "\n",
    "            sub_grid_pred_ijk = self.sub_grid_forward(\n",
    "                context_val=context_val,\n",
    "                context_coord=context_coord,\n",
    "                query_coord=query_coord,\n",
    "                context_vox_size=context_vox_size,\n",
    "                # query_vox_size=query_vox_size,\n",
    "                return_rel_context_coord=False,\n",
    "            )\n",
    "            # Initialize the accumulated prediction after finding the\n",
    "            # output size; easier than trying to pre-compute it.\n",
    "            if y_weighted_accumulate is None:\n",
    "                y_weighted_accumulate = torch.zeros_like(sub_grid_pred_ijk)\n",
    "\n",
    "            sub_window_offset_ijk_compliment = torch.abs(1 - sub_window_offset_ijk)\n",
    "            sub_window_context_coord_compliment = (\n",
    "                query_bottom_back_left_corner_coord\n",
    "                + (sub_window_offset_ijk_compliment * context_vox_size)\n",
    "            )\n",
    "            w_sub_window_cube = torch.abs(\n",
    "                sub_window_context_coord_compliment - query_coord\n",
    "            )\n",
    "            w_sub_window = einops.reduce(\n",
    "                w_sub_window_cube, \"b side_len i j k -> b 1 i j k\", reduction=\"prod\"\n",
    "            ) / einops.reduce(\n",
    "                context_vox_size, \"b size 1 1 1 -> b 1 1 1 1\", reduction=\"prod\"\n",
    "            )\n",
    "\n",
    "            # Weigh this cell's prediction by the inverse of the distance\n",
    "            # from the cell physical coordinate to the true target\n",
    "            # physical coordinate. Normalize the weight by the inverse\n",
    "            # \"sum of the inverse distances\" found before.\n",
    "\n",
    "            # Accumulate weighted cell predictions to eventually create\n",
    "            # the final prediction.\n",
    "            y_weighted_accumulate += w_sub_window * sub_grid_pred_ijk\n",
    "            # del sub_grid_pred_ijk\n",
    "\n",
    "        y = y_weighted_accumulate\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class INR_Interpolator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dwi_brain_vol,\n",
    "        brain_mask_vol,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        affine_vox2mm,\n",
    "        fn_peak_finder,\n",
    "    ):\n",
    "\n",
    "        self.affine_vox2mm = affine_vox2mm.to(torch.float32)\n",
    "        # Network was trained with xyz orientations, but there are zyx orientations in\n",
    "        # the tractography code. So, all coordinates and images need to be rearranged\n",
    "        # and flipped, then again for the output\n",
    "        self.affine_vox2mm[:3, 3] = torch.flip(self.affine_vox2mm[:3, 3], dims=(-1,))\n",
    "        self.decoder = decoder\n",
    "        self.fn_peak_finder = fn_peak_finder\n",
    "\n",
    "        if dwi_brain_vol.ndim == 4:\n",
    "            dwi_brain_vol = dwi_brain_vol[None]\n",
    "        dwi_brain_vol = dwi_brain_vol.to(torch.float32)\n",
    "        if brain_mask_vol.ndim == 4:\n",
    "            brain_mask_vol = brain_mask_vol[None]\n",
    "        dwi_brain_vol = einops.rearrange(dwi_brain_vol, \"b c z y x -> b c x y z\")\n",
    "        brain_mask_vol = einops.rearrange(brain_mask_vol, \"b c z y x -> b c x y z\")\n",
    "        with torch.no_grad():\n",
    "            self.encoded_ctx = encoder(dwi_brain_vol)\n",
    "            self.encoded_ctx = self.encoded_ctx * brain_mask_vol\n",
    "            self.ctx_spatial_extent = pitn.data.datasets._get_extent_world(\n",
    "                brain_mask_vol[:, 0], self.affine_vox2mm\n",
    "            )\n",
    "            self.ctx_spatial_extent = self.ctx_spatial_extent[None].to(torch.float32)\n",
    "\n",
    "    def spatial_fodf_sample(\n",
    "        self,\n",
    "        coords_mm_zyx: torch.Tensor,\n",
    "        directions_theta_phi: Optional[torch.Tensor],\n",
    "        batch_size: int,\n",
    "    ) -> torch.Tensor:\n",
    "        theta = directions_theta_phi[..., 0]\n",
    "        phi = directions_theta_phi[..., 1]\n",
    "        Y_basis = pitn.tract.peak.sh_basis_mrtrix3(\n",
    "            theta=theta, phi=phi, batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        # Interpolation of fodf coefficients at the target points.\n",
    "        with torch.no_grad():\n",
    "            volumetric_target_coords = einops.rearrange(\n",
    "                coords_mm_zyx, \"b c -> 1 c 1 b 1\"\n",
    "            ).to(torch.float32)\n",
    "            volumetric_target_coords = torch.flip(volumetric_target_coords, (1,))\n",
    "            pred_sample_fodf_coeffs = self.decoder(\n",
    "                context_v=self.encoded_ctx,\n",
    "                context_spatial_extent=self.ctx_spatial_extent,\n",
    "                affine_context_vox2mm=self.affine_vox2mm,\n",
    "                query_coord=volumetric_target_coords,\n",
    "            )\n",
    "        pred_sample_fodf_coeffs = einops.rearrange(\n",
    "            pred_sample_fodf_coeffs, \"1 coeff 1 b 1 -> b coeff\"\n",
    "        ).to(coords_mm_zyx)\n",
    "\n",
    "        Y_basis = einops.rearrange(Y_basis, \"b sh_idx -> b sh_idx 1\")\n",
    "        pred_sample_fodf_coeffs = einops.rearrange(\n",
    "            pred_sample_fodf_coeffs, \"b sh_idx -> b 1 sh_idx\"\n",
    "        )\n",
    "        samples = torch.bmm(pred_sample_fodf_coeffs, Y_basis)\n",
    "        samples.squeeze_()\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        target_coords_mm_zyx: torch.Tensor,\n",
    "        init_direction_theta_phi: Optional[torch.Tensor],\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Initial interpolation of fodf coefficients at the target points.\n",
    "        with torch.no_grad():\n",
    "            volumetric_target_coords = einops.rearrange(\n",
    "                target_coords_mm_zyx, \"b c -> 1 c 1 b 1\"\n",
    "            ).to(torch.float32)\n",
    "            volumetric_target_coords = torch.flip(volumetric_target_coords, (1,))\n",
    "            pred_sample_fodf_coeffs = self.decoder(\n",
    "                context_v=self.encoded_ctx,\n",
    "                context_spatial_extent=self.ctx_spatial_extent,\n",
    "                affine_context_vox2mm=self.affine_vox2mm,\n",
    "                query_coord=volumetric_target_coords,\n",
    "            )\n",
    "        pred_sample_fodf_coeffs = einops.rearrange(\n",
    "            pred_sample_fodf_coeffs, \"1 coeff 1 b 1 -> b coeff\"\n",
    "        ).to(target_coords_mm_zyx)\n",
    "\n",
    "        # The previous outgoing direction is not really the true \"incoming\" direction in\n",
    "        # the new voxel, but it is located on the opposite hemisphere in the new voxel.\n",
    "        # However, the peak finding locates the peak nearest the given initialization\n",
    "        # direction, so it would just be two consecutive mirrorings on the sphere, which\n",
    "        # is obviously identity.\n",
    "        outgoing_theta, outgoing_phi = (\n",
    "            init_direction_theta_phi[..., 0],\n",
    "            init_direction_theta_phi[..., 1],\n",
    "        )\n",
    "        init_direction_theta_phi = (outgoing_theta, outgoing_phi)\n",
    "        result_direction_theta_phi = self.fn_peak_finder(\n",
    "            pred_sample_fodf_coeffs, init_direction_theta_phi\n",
    "        )\n",
    "        #!DEBUG\n",
    "        # bugged_idx = (35, 728, 4570)\n",
    "        # result_direction_theta_phi = self.fn_peak_finder(\n",
    "        #     pred_sample_fodf_coeffs[\n",
    "        #         (torch.tensor(bugged_idx).to(pred_sample_fodf_coeffs).long(),)\n",
    "        #     ],\n",
    "        #     (\n",
    "        #         init_direction_theta_phi[0][\n",
    "        #             (torch.tensor(bugged_idx).to(pred_sample_fodf_coeffs).long(),)\n",
    "        #         ],\n",
    "        #         init_direction_theta_phi[1][\n",
    "        #             (torch.tensor(bugged_idx).to(pred_sample_fodf_coeffs).long(),)\n",
    "        #         ],\n",
    "        #     ),\n",
    "        # )\n",
    "        #!\n",
    "        return result_direction_theta_phi\n",
    "\n",
    "\n",
    "#!\n",
    "encoder_init_kwargs = dict(\n",
    "    in_channels=189,\n",
    "    interior_channels=80,\n",
    "    out_channels=128,\n",
    "    n_res_units=3,\n",
    "    n_dense_units=3,\n",
    "    activate_fn=\"relu\",\n",
    ")\n",
    "decoder_init_kwargs = dict(\n",
    "    context_v_features=128,\n",
    "    in_features=encoder_init_kwargs[\"out_channels\"],\n",
    "    out_features=45,\n",
    "    m_encode_num_freqs=36,\n",
    "    sigma_encode_scale=3.0,\n",
    ")\n",
    "inr_system_state_dict = torch.load(network_weights_f)\n",
    "\n",
    "encoder = INREncoder(**encoder_init_kwargs)\n",
    "encoder.load_state_dict(inr_system_state_dict[\"encoder\"])\n",
    "encoder = encoder.to(device).eval()\n",
    "\n",
    "decoder = ReducedDecoder(**decoder_init_kwargs)\n",
    "decoder.load_state_dict(inr_system_state_dict[\"decoder\"])\n",
    "decoder = decoder.to(device).eval()\n",
    "del inr_system_state_dict\n",
    "fn_inr_interp_zyx_tangent_t2theta_phi = INR_Interpolator(\n",
    "    dwi_brain_vol=dwi,\n",
    "    brain_mask_vol=brain_mask,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    affine_vox2mm=affine_sar_vox2sar_mm,\n",
    "    fn_peak_finder=peak_finder_fn_theta_phi_c2theta_phi,\n",
    ")\n",
    "del encoder\n",
    "#!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "298bb95a",
   "metadata": {},
   "source": [
    "### Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fee064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initial seeds and tangent/direction vectors.\n",
    "init_unique_seeds = pitn.tract.seed.seeds_from_mask(\n",
    "    seed_mask,\n",
    "    seeds_per_vox_axis=seeds_per_vox_axis,\n",
    "    affine_vox2mm=affine_sar_vox2sar_mm,\n",
    ")\n",
    "\n",
    "fn_zyx2theta_phi_seed_expansion = (\n",
    "    fn_inr_interp_zyx_tangent_t2theta_phi\n",
    "    if MODEL_SELECTION.casefold() == \"inr\"\n",
    "    else fn_linear_interp_zyx_tangent_t2theta_phi\n",
    ")\n",
    "\n",
    "seed_sampler = pitn.tract.seed.SequentialSeedDirectionSampler(\n",
    "    max_batch_size=seed_batch_size,\n",
    "    max_peaks_per_voxel=peaks_per_seed_vox,\n",
    "    unique_seed_coords_zyx_mm=init_unique_seeds,\n",
    "    tracking_step_size=step_size,\n",
    "    fodf_coeffs_brain_vol=coeffs,\n",
    "    affine_vox2mm=affine_sar_vox2sar_mm,\n",
    "    fn_zyx_direction_t2theta_phi=fn_zyx2theta_phi_seed_expansion,\n",
    "    # dipy peak finder kwargs\n",
    "    seed_sphere_theta=seed_theta,\n",
    "    seed_sphere_phi=seed_phi,\n",
    "    fodf_sample_min_val=fodf_sample_min_val,\n",
    "    fodf_sample_min_quantile_thresh=fodf_sample_min_quantile_thresh,\n",
    "    relative_peak_threshold=dipy_relative_peak_threshold,\n",
    "    min_separation_angle=dipy_min_separation_angle,\n",
    ")\n",
    "\n",
    "seeds_t_neg1_to_0, tangent_t0_zyx = seed_sampler.sample_direction_seeds_sequential(\n",
    "    0, seed_batch_size\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "430f149b",
   "metadata": {},
   "source": [
    "### Primary Tractography Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac314972",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! debug\n",
    "# Create initial seeds and tangent/direction vectors.\n",
    "init_unique_seeds = pitn.tract.seed.seeds_from_mask(\n",
    "    seed_mask,\n",
    "    seeds_per_vox_axis=seeds_per_vox_axis,\n",
    "    affine_vox2mm=affine_sar_vox2sar_mm,\n",
    ")\n",
    "\n",
    "fn_zyx2theta_phi_seed_expansion = (\n",
    "    fn_inr_interp_zyx_tangent_t2theta_phi\n",
    "    if MODEL_SELECTION.casefold() == \"inr\"\n",
    "    else fn_linear_interp_zyx_tangent_t2theta_phi\n",
    ")\n",
    "\n",
    "seed_sampler = pitn.tract.seed.SequentialSeedDirectionSampler(\n",
    "    max_batch_size=seed_batch_size,\n",
    "    max_peaks_per_voxel=peaks_per_seed_vox,\n",
    "    unique_seed_coords_zyx_mm=init_unique_seeds[0:10],\n",
    "    tracking_step_size=step_size,\n",
    "    fodf_coeffs_brain_vol=coeffs,\n",
    "    affine_vox2mm=affine_sar_vox2sar_mm,\n",
    "    fn_zyx_direction_t2theta_phi=fn_zyx2theta_phi_seed_expansion,\n",
    "    # dipy peak finder kwargs\n",
    "    seed_sphere_theta=seed_theta,\n",
    "    seed_sphere_phi=seed_phi,\n",
    "    fodf_sample_min_val=fodf_sample_min_val,\n",
    "    fodf_sample_min_quantile_thresh=fodf_sample_min_quantile_thresh,\n",
    "    relative_peak_threshold=dipy_relative_peak_threshold,\n",
    "    min_separation_angle=dipy_min_separation_angle,\n",
    ")\n",
    "\n",
    "seeds_t_neg1_to_0, tangent_t0_zyx = seed_sampler.sample_direction_seeds_sequential(\n",
    "    0, seed_batch_size\n",
    ")\n",
    "#! debug\n",
    "\n",
    "# Prep objects & initialize all state objects to t=0.\n",
    "\n",
    "fn_direction_estimate = (\n",
    "    fn_inr_interp_zyx_tangent_t2theta_phi\n",
    "    if MODEL_SELECTION.casefold() == \"inr\"\n",
    "    else fn_linear_interp_zyx_tangent_t2theta_phi\n",
    ")\n",
    "\n",
    "fn_fod_ampl_estimate = (\n",
    "    fn_inr_interp_zyx_tangent_t2theta_phi.spatial_fodf_sample\n",
    "    if MODEL_SELECTION.casefold() == \"inr\"\n",
    "    else fn_linear_interp_spatial_fodf_sample\n",
    ")\n",
    "\n",
    "all_tracts = list()\n",
    "streamlines = list()\n",
    "\n",
    "max_steps = math.ceil(max_streamline_len / step_size)\n",
    "batch_size = tangent_t0_zyx.shape[0]\n",
    "streamline_buffer = (\n",
    "    torch.ones(\n",
    "        max_steps, batch_size, 3, device=seeds_t_neg1_to_0.device, dtype=torch.float32\n",
    "    )\n",
    "    * torch.nan\n",
    ")\n",
    "\n",
    "v_t = torch.zeros(batch_size, dtype=torch.long, device=tangent_t0_zyx.device)\n",
    "streamline_buffer.index_put_((v_t,), seeds_t_neg1_to_0[0].to(streamline_buffer))\n",
    "v_t += 1\n",
    "streamline_buffer.index_put_((v_t,), seeds_t_neg1_to_0[1].to(streamline_buffer))\n",
    "\n",
    "# t_max = 1e8\n",
    "t_max = 1e6\n",
    "\n",
    "full_streamline_status = (\n",
    "    torch.ones(batch_size, dtype=torch.int8, device=seeds_t_neg1_to_0.device)\n",
    "    * pitn.tract.stopping.CONTINUE\n",
    ")\n",
    "# At least one step has been made.\n",
    "full_streamline_len = torch.zeros_like(full_streamline_status).float() + step_size\n",
    "full_points_t = streamline_buffer[1]\n",
    "full_tangent_t_theta_phi = torch.stack(\n",
    "    pitn.tract.local.zyx2unit_sphere_theta_phi(tangent_t0_zyx), dim=-1\n",
    ")\n",
    "full_tangent_t_zyx = tangent_t0_zyx\n",
    "full_points_tp1 = torch.zeros_like(full_points_t) * torch.nan\n",
    "\n",
    "sampler_empty = False\n",
    "curr_sampler_idx = batch_size\n",
    "\n",
    "seeds_completed = 0\n",
    "\n",
    "while (not sampler_empty) or pitn.tract.stopping.to_continue_mask(\n",
    "    full_streamline_status\n",
    ").any():\n",
    "\n",
    "    to_continue = pitn.tract.stopping.to_continue_mask(full_streamline_status)\n",
    "\n",
    "    points_t = full_points_t[to_continue]\n",
    "    tangent_t_theta_phi = full_tangent_t_theta_phi[to_continue]\n",
    "    tangent_t_zyx = full_tangent_t_zyx[to_continue]\n",
    "    streamline_len = full_streamline_len[to_continue]\n",
    "    status_t = full_streamline_status[to_continue]\n",
    "\n",
    "    tangent_tp1_zyx = pitn.tract.local.gen_tract_step_rk4(\n",
    "        points_t,\n",
    "        init_direction_theta_phi=tangent_t_theta_phi,\n",
    "        fn_zyx_direction_t2theta_phi=fn_direction_estimate,\n",
    "        step_size=step_size,\n",
    "    )\n",
    "    tangent_tp1_theta_phi = torch.stack(\n",
    "        pitn.tract.local.zyx2unit_sphere_theta_phi(tangent_tp1_zyx), -1\n",
    "    )\n",
    "\n",
    "    fodf_sample_point_t_direction_tp1 = fn_fod_ampl_estimate(\n",
    "        points_t,\n",
    "        directions_theta_phi=tangent_tp1_theta_phi,\n",
    "        batch_size=seed_batch_size,\n",
    "    )\n",
    "\n",
    "    ema_tangent_tp1_zyx = (\n",
    "        alpha_exponential_moving_avg * tangent_tp1_zyx\n",
    "        + (1 - alpha_exponential_moving_avg) * tangent_t_zyx\n",
    "    )\n",
    "    ema_tangent_tp1_zyx = (\n",
    "        step_size\n",
    "        * ema_tangent_tp1_zyx\n",
    "        / torch.linalg.vector_norm(ema_tangent_tp1_zyx, ord=2, dim=-1, keepdim=True)\n",
    "    )\n",
    "\n",
    "    points_tp1 = points_t + ema_tangent_tp1_zyx\n",
    "\n",
    "    tangent_tp1_zyx = ema_tangent_tp1_zyx\n",
    "\n",
    "    # Update state variables based upon new streamline statuses.\n",
    "    tmp_len = streamline_len + step_size\n",
    "    statuses_tp1 = list()\n",
    "    statuses_tp1.append(\n",
    "        pitn.tract.stopping.scalar_vol_threshold(\n",
    "            status_t,\n",
    "            sample_coords_mm_zyx=points_tp1,\n",
    "            scalar_min_threshold=fa_min_threshold,\n",
    "            vol=fa,\n",
    "            affine_vox2mm=affine_sar_vox2sar_mm,\n",
    "        )\n",
    "    )\n",
    "    statuses_tp1.append(\n",
    "        pitn.tract.stopping.angular_threshold(\n",
    "            status_t, points_t, points_tp1, max_angular_thresh_rad\n",
    "        )\n",
    "    )\n",
    "    statuses_tp1.append(\n",
    "        pitn.tract.stopping.streamline_len_mm(\n",
    "            status_t,\n",
    "            tmp_len,\n",
    "            min_len=min_streamline_len,\n",
    "            max_len=max_streamline_len,\n",
    "        )\n",
    "    )\n",
    "    statuses_tp1.append(\n",
    "        pitn.tract.stopping.scalar_vec_threshold(\n",
    "            status_t,\n",
    "            fodf_sample_point_t_direction_tp1,\n",
    "            scalar_min_threshold=tracking_fodf_sample_min_val,\n",
    "        )\n",
    "    )\n",
    "    status_tp1 = pitn.tract.stopping.merge_status(status_t, *statuses_tp1)\n",
    "    status_tp1 = pitn.tract.stopping.merge_status(\n",
    "        status_tp1,\n",
    "        pitn.tract.stopping.streamline_len_mm(\n",
    "            status_tp1,\n",
    "            tmp_len,\n",
    "            min_len=min_streamline_len,\n",
    "            max_len=max_streamline_len,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    full_streamline_status_tp1 = full_streamline_status.masked_scatter(\n",
    "        to_continue, status_tp1\n",
    "    )\n",
    "\n",
    "    # Remove any stopped tracks from the tp1 variables. We want the number of\n",
    "    # elements in the \"x_tp1\" variables to equal the number of \"True\" values in\n",
    "    # the \"to_continue_tp1\" array! Otherwise, the masked_scatter() will not put\n",
    "    # the masked values where we want them.\n",
    "    tp1_props_filter_mask = pitn.tract.stopping.to_continue_mask(status_tp1)\n",
    "    points_tp1 = points_tp1[tp1_props_filter_mask]\n",
    "    tangent_tp1_theta_phi = tangent_tp1_theta_phi[tp1_props_filter_mask]\n",
    "    tangent_tp1_zyx = tangent_tp1_zyx[tp1_props_filter_mask]\n",
    "    streamline_len_tp1 = tmp_len\n",
    "    streamline_len_tp1 = streamline_len_tp1[tp1_props_filter_mask]\n",
    "    status_tp1 = status_tp1[tp1_props_filter_mask]\n",
    "\n",
    "    to_continue_tp1 = pitn.tract.stopping.to_continue_mask(full_streamline_status_tp1)\n",
    "\n",
    "    assert points_tp1.shape[0] == to_continue_tp1.sum()\n",
    "\n",
    "    full_points_tp1 = (full_points_tp1 * torch.nan).masked_scatter(\n",
    "        to_continue_tp1[..., None], points_tp1.to(full_points_tp1)\n",
    "    )\n",
    "    streamline_buffer.index_put_((v_t,), full_points_tp1.to(streamline_buffer))\n",
    "\n",
    "    # t <- t + 1\n",
    "    v_t[to_continue_tp1] += 1\n",
    "    if v_t.max() > t_max:\n",
    "        break\n",
    "\n",
    "    full_points_t = full_points_tp1\n",
    "    full_tangent_t_theta_phi = (full_tangent_t_theta_phi * torch.nan).masked_scatter(\n",
    "        to_continue_tp1[..., None], tangent_tp1_theta_phi\n",
    "    )\n",
    "    full_tangent_t_zyx = (full_tangent_t_zyx * torch.nan).masked_scatter(\n",
    "        to_continue_tp1[..., None], tangent_tp1_zyx\n",
    "    )\n",
    "    full_streamline_len.masked_scatter_(\n",
    "        to_continue_tp1,\n",
    "        streamline_len_tp1,\n",
    "    )\n",
    "    full_streamline_status_change = full_streamline_status != full_streamline_status_tp1\n",
    "    full_streamline_status = full_streamline_status_tp1\n",
    "\n",
    "    # If any streamlines have stopped when they previously were not stopped, then store\n",
    "    # those streamlines and sample more seeds to take their place.\n",
    "    # to_continue_tp1 indicates stopped streamlines, while the streamline status\n",
    "    # change indicates whether those buffer slots have been empty for more than one\n",
    "    # iteration.\n",
    "    if (~to_continue_tp1 & full_streamline_status_change).any():\n",
    "        to_free_mask = ~to_continue_tp1 & full_streamline_status_change\n",
    "        n_free = to_free_mask.sum().cpu().int().item()\n",
    "\n",
    "        # Only store the valid/stopped streamlines, but empty out all the `to_free`\n",
    "        # streamlines.\n",
    "        to_store_valid = to_free_mask & (\n",
    "            full_streamline_status != pitn.tract.stopping.INVALID\n",
    "        )\n",
    "        n_to_store = to_store_valid.sum().cpu().int().item()\n",
    "\n",
    "        if n_to_store > 0:\n",
    "            seeds_completed += n_to_store\n",
    "            if (seeds_completed % 100) == 0:\n",
    "                print(seeds_completed, end=\"...\", flush=True)\n",
    "\n",
    "            stopped_streamlines = torch.tensor_split(\n",
    "                streamline_buffer[:, to_store_valid].cpu(), n_to_store, dim=1\n",
    "            )\n",
    "            streamlines.extend(stopped_streamlines)\n",
    "\n",
    "        full_streamline_len.masked_fill_(to_free_mask, 0.0)\n",
    "        full_points_t.masked_fill_(to_free_mask[..., None], torch.nan)\n",
    "        full_tangent_t_theta_phi.masked_fill_(to_free_mask[..., None], torch.nan)\n",
    "        full_tangent_t_zyx.masked_fill_(to_free_mask[..., None], torch.nan)\n",
    "        full_streamline_status.masked_fill_(to_free_mask, pitn.tract.stopping.STOP)\n",
    "        v_t.masked_fill_(to_free_mask, 0)\n",
    "\n",
    "        # Only initialize new seeds if there are any to be sampled.\n",
    "        if not sampler_empty:\n",
    "            try:\n",
    "                (\n",
    "                    new_seeds_tneg1_to_t0,\n",
    "                    new_tangent_t0_zyx,\n",
    "                ) = seed_sampler.sample_direction_seeds_sequential(\n",
    "                    curr_sampler_idx, curr_sampler_idx + n_free\n",
    "                )\n",
    "            except IndexError:\n",
    "                sampler_empty = True\n",
    "            else:\n",
    "                n_new_seeds = new_tangent_t0_zyx.shape[0]\n",
    "                curr_sampler_idx += n_new_seeds\n",
    "                # It may be the case that the number of new seeds is less than the\n",
    "                # number of available slots.\n",
    "                to_refill_mask = to_free_mask.clone()\n",
    "                to_refill_mask[torch.argwhere(to_refill_mask)[n_new_seeds:]] = False\n",
    "                streamline_buffer[0, to_refill_mask] = new_seeds_tneg1_to_t0[0].to(\n",
    "                    streamline_buffer\n",
    "                )\n",
    "                streamline_buffer[1, to_refill_mask] = new_seeds_tneg1_to_t0[1].to(\n",
    "                    streamline_buffer\n",
    "                )\n",
    "                full_streamline_status.masked_fill_(\n",
    "                    to_refill_mask, pitn.tract.stopping.CONTINUE\n",
    "                )\n",
    "                full_points_t[to_refill_mask] = new_seeds_tneg1_to_t0[1].to(\n",
    "                    full_points_t\n",
    "                )\n",
    "                full_tangent_t_zyx[to_refill_mask] = new_tangent_t0_zyx\n",
    "                full_tangent_t_theta_phi[to_refill_mask] = torch.stack(\n",
    "                    pitn.tract.local.zyx2unit_sphere_theta_phi(new_tangent_t0_zyx), -1\n",
    "                )\n",
    "                v_t.masked_fill_(to_refill_mask, 1)\n",
    "                full_streamline_len.masked_fill_(to_refill_mask, step_size)\n",
    "\n",
    "# Collect all valid streamlines and cut them at the stopping point.\n",
    "streamlines = torch.stack(streamlines, 1).squeeze(2)\n",
    "remove_streamline_mask = torch.isnan(streamlines).all(dim=1).any(dim=1)\n",
    "keep_streamline_mask = ~remove_streamline_mask\n",
    "streams = streamlines[keep_streamline_mask].detach().cpu().numpy()\n",
    "tract_end_idx = np.argwhere(np.isnan(streams).any(2))[:, 1]\n",
    "batch_stream_list = np.split(streams, streams.shape[0], axis=0)\n",
    "all_tracts = list()\n",
    "for s in batch_stream_list:\n",
    "    s = s.squeeze()\n",
    "    if np.isnan(s).any():\n",
    "        end_idx = np.argwhere(np.isnan(s).any(-1)).min()\n",
    "        all_tracts.append(s[:end_idx])\n",
    "    else:\n",
    "        all_tracts.append(s)\n",
    "\n",
    "print(\"\", end=\"\", flush=True)\n",
    "\n",
    "tracts = all_tracts\n",
    "\n",
    "# Create tractogram and save.\n",
    "sar_tracts = dipy.io.dpy.Streamlines(tracts)\n",
    "sar_tracto = dipy.io.streamline.Tractogram(\n",
    "    sar_tracts, affine_to_rasmm=affine_sar2ras.cpu().numpy()\n",
    ")\n",
    "tracto = sar_tracto.to_world()\n",
    "# Get the header from an \"un-re-oriented\" fod volume and give to the tractogram.\n",
    "\n",
    "ref_header = nib.as_closest_canonical(nib.load(sample_fod_f)).header\n",
    "tracto = dipy.io.streamline.StatefulTractogram(\n",
    "    tracto.streamlines,\n",
    "    space=dipy.io.stateful_tractogram.Space.RASMM,\n",
    "    reference=ref_header,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8934b5ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5edb8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fiber_fname = (\n",
    "    f\"{SUBJECT_ID}_{dataset_selection}_\"\n",
    "    + f\"{selected_seed_vox_name}_{MODEL_SELECTION}_test_trax.tck\"\n",
    ")\n",
    "tmp_res_dir.mkdir(parents=True)\n",
    "fiber_fname = str(tmp_res_dir / fiber_fname)\n",
    "# fiber_fname = f\"/tmp/fibercup_single_vox_seed_test_trax.tck\"\n",
    "print(\"Saving tractogram\", flush=True)\n",
    "dipy.io.streamline.save_tck(tracto, fiber_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e37e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ax in range(3):\n",
    "#     sts_on_ax = [s[:, ax] for s in tracto.streamlines]\n",
    "#     plt.figure(dpi=120)\n",
    "#     for i, s in enumerate(sts_on_ax):\n",
    "#         plt.plot(s, label=i, lw=0.4, alpha=0.7)\n",
    "#     plt.ylabel((\"x\", \"y\", \"z\")[ax])\n",
    "#     # plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91578555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465616a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(streamlines[3, :, 2].cpu().numpy(), label=\"x\")\n",
    "# plt.plot(streamlines[3, :, 1].cpu().numpy(), label=\"y\")\n",
    "# plt.plot(streamlines[3, :, 0].cpu().numpy(), label=\"z\")\n",
    "\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bb6d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# im = nib.Nifti1Image(\n",
    "#     gfa[0].cpu().swapdims(0, 2).numpy(), affine_ras_vox2ras_mm.cpu().numpy(), ref_header\n",
    "# )\n",
    "\n",
    "# nib.save(im, str(sample_fod_f.parent / \"gfa.nii.gz\"))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "comment_magics": true,
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "pitn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a27b66b33b39a2ceaf791f3e24555d5a3d820ed1de6ee5d2f5c031617fe18d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
