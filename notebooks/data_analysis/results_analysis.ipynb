{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PITN Model Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automatically re-import project-specific modules.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# imports\n",
    "import collections\n",
    "import functools\n",
    "import io\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "import copy\n",
    "import pdb\n",
    "import inspect\n",
    "import random\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import typing\n",
    "import zipfile\n",
    "import ast\n",
    "\n",
    "import dipy\n",
    "import dipy.core\n",
    "import dipy.reconst\n",
    "import dipy.reconst.dti\n",
    "import dipy.segment.mask\n",
    "import dotenv\n",
    "\n",
    "# visualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patheffects\n",
    "import mpl_toolkits\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data management libraries.\n",
    "import nibabel as nib\n",
    "import nibabel.processing\n",
    "import natsort\n",
    "from natsort import natsorted\n",
    "from pprint import pprint as ppr\n",
    "from box import Box\n",
    "import yaml\n",
    "\n",
    "# Computation & ML libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchio\n",
    "import pytorch_lightning as pl\n",
    "import monai\n",
    "import einops\n",
    "import torchinfo\n",
    "\n",
    "import skimage\n",
    "import skimage.feature\n",
    "import skimage.filters\n",
    "import skimage.measure\n",
    "import scipy\n",
    "\n",
    "import pitn\n",
    "\n",
    "plt.rcParams.update({\"figure.autolayout\": True})\n",
    "plt.rcParams.update({\"figure.facecolor\": [1.0, 1.0, 1.0, 1.0]})\n",
    "\n",
    "# Set print options for ndarrays/tensors.\n",
    "np.set_printoptions(suppress=True, threshold=100, linewidth=88)\n",
    "torch.set_printoptions(sci_mode=False, threshold=100, linewidth=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update notebook's environment variables with direnv.\n",
    "# This requires the python-dotenv package, and direnv be installed on the system\n",
    "# This will not work on Windows.\n",
    "# NOTE: This is kind of hacky, and not necessarily safe. Be careful...\n",
    "# Libraries needed on the python side:\n",
    "# - os\n",
    "# - subprocess\n",
    "# - io\n",
    "# - dotenv\n",
    "\n",
    "# Form command to be run in direnv's context. This command will print out\n",
    "# all environment variables defined in the subprocess/sub-shell.\n",
    "command = f\"direnv exec {os.getcwd()} /usr/bin/env\"\n",
    "# Run command in a new subprocess.\n",
    "proc = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, cwd=os.getcwd())\n",
    "# Store and format the subprocess' output.\n",
    "proc_out = proc.communicate()[0].strip().decode(\"utf-8\")\n",
    "# Use python-dotenv to load the environment variables by using the output of\n",
    "# 'direnv exec ...' as a 'dummy' .env file.\n",
    "dotenv.load_dotenv(stream=io.StringIO(proc_out), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch setup\n",
    "# Will need CUDA for finding eigendecomposition of these large volumes.\n",
    "# allow for CUDA usage, if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    # Activate cudnn benchmarking to optimize convolution algorithm speed.\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        print(\"CuDNN convolution optimization enabled.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# keep device as the cpu\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up directories\n",
    "data_dir = pathlib.Path(os.environ[\"DATA_DIR\"])\n",
    "assert data_dir.exists()\n",
    "write_data_dir = pathlib.Path(os.environ[\"WRITE_DATA_DIR\"])\n",
    "assert write_data_dir.exists()\n",
    "results_dir = pathlib.Path(os.environ[\"RESULTS_DIR\"])\n",
    "assert results_dir.exists()\n",
    "tmp_results_dir = pathlib.Path(os.environ[\"TMP_RESULTS_DIR\"])\n",
    "assert tmp_results_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameter setup.\n",
    "\n",
    "params = Box(default_box=True)\n",
    "\n",
    "# 6 channels for the 6 DTI components\n",
    "params.n_channels = 6\n",
    "params.n_subjs = 48\n",
    "params.lr_vox_size = 2.5\n",
    "params.fr_vox_size = 1.25\n",
    "\n",
    "# Data params\n",
    "params.data.fr_dir = data_dir / f\"scale-{params.fr_vox_size:.2f}mm\"\n",
    "params.data.lr_dir = data_dir / f\"scale-{params.lr_vox_size:.2f}mm\"\n",
    "params.data.dti_fname_pattern = r\"sub-*dti.nii.gz\"\n",
    "params.data.mask_fname_pattern = r\"dti/sub-*mask.nii.gz\"\n",
    "\n",
    "# The data were downsampled artificially by this factor.\n",
    "params.data.downsampled_by_factor = params.lr_vox_size / params.fr_vox_size\n",
    "params.data.downsampled_by_factor = (\n",
    "    int(params.data.downsampled_by_factor)\n",
    "    if int(params.data.downsampled_by_factor) == params.data.downsampled_by_factor\n",
    "    else params.data.downsampled_by_factor\n",
    ")\n",
    "\n",
    "params.data.eigval_clip_cutoff = 0.00332008\n",
    "\n",
    "# Needed for downsampling shape correction.\n",
    "params.train.in_patch_size = (24, 24, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constant for our particular paper template, which is ~1.85 inch margins.\n",
    "FIG_WIDTH_INCHES = 347 * (8.5 / 614)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Preprocess Ground Truth & Predicted DTIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find data directories for each subject.\n",
    "subj_dirs: Box = Box()\n",
    "\n",
    "selected_ids = [\n",
    "    \"397154\",\n",
    "    \"224022\",\n",
    "    \"140117\",\n",
    "    \"751348\",\n",
    "    \"894774\",\n",
    "    \"156637\",\n",
    "    \"227432\",\n",
    "    \"303624\",\n",
    "    \"185947\",\n",
    "    \"810439\",\n",
    "    \"753251\",\n",
    "    \"644246\",\n",
    "    \"141422\",\n",
    "    \"135528\",\n",
    "    \"103010\",\n",
    "    \"700634\",\n",
    "    \"406432\",\n",
    "    \"803240\",\n",
    "    \"815247\",\n",
    "    \"167238\",\n",
    "    \"100408\",\n",
    "    \"792867\",\n",
    "    \"157437\",\n",
    "    \"164030\",\n",
    "    \"103515\",\n",
    "    \"118730\",\n",
    "    \"198047\",\n",
    "    \"189450\",\n",
    "    \"203923\",\n",
    "    \"108828\",\n",
    "    \"124220\",\n",
    "    \"386250\",\n",
    "    \"118124\",\n",
    "    \"701535\",\n",
    "    \"679770\",\n",
    "    \"382242\",\n",
    "    \"231928\",\n",
    "    \"196952\",  # Hold-out subject; for visualization, ensure never in the train or val sets\n",
    "    \"567961\",\n",
    "    \"910241\",\n",
    "    \"175035\",\n",
    "    \"567759\",\n",
    "    \"978578\",\n",
    "    \"150019\",\n",
    "    \"690152\",\n",
    "    \"297655\",\n",
    "    \"307127\",\n",
    "    \"634748\",\n",
    "]\n",
    "HOLDOUT_SUBJ_ID = \"196952\"\n",
    "selected_subjs = selected_ids\n",
    "selected_subjs = natsorted(selected_subjs)\n",
    "\n",
    "for subj_id in selected_subjs:\n",
    "    subj_dirs[subj_id] = Box()\n",
    "    subj_dirs[subj_id].fr = pitn.utils.system.get_file_glob_unique(\n",
    "        params.data.fr_dir, f\"*{subj_id}*\"\n",
    "    )\n",
    "    subj_dirs[subj_id].lr = pitn.utils.system.get_file_glob_unique(\n",
    "        params.data.lr_dir, f\"*{subj_id}*\"\n",
    "    )\n",
    "    assert subj_dirs[subj_id].fr.exists()\n",
    "    assert subj_dirs[subj_id].lr.exists()\n",
    "ppr(subj_dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This dict will contain all ground truth data.\n",
    "subj_gt: Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prep for Dataset loading.\n",
    "\n",
    "# Data reader object for NIFTI files.\n",
    "nib_reader = monai.data.NibabelReader(as_closest_canonical=True)\n",
    "\n",
    "# HR -> LR patch coordinate conversion function.\n",
    "fr2lr_patch_coords_fn = {\n",
    "    \"lr_dti\": functools.partial(\n",
    "        pitn.coords.transform.int_downscale_patch_idx,\n",
    "        downscale_factor=params.data.downsampled_by_factor,\n",
    "        downscale_patch_shape=params.train.in_patch_size,\n",
    "    )\n",
    "}\n",
    "fr2lr_patch_coords_fn[\"lr_log_euclid\"] = fr2lr_patch_coords_fn[\"lr_dti\"]\n",
    "fr2lr_patch_coords_fn[\"lr_mask\"] = fr2lr_patch_coords_fn[\"lr_dti\"]\n",
    "\n",
    "# Kwargs for the patches dataset (the _VolPatchDataset class) of the HR volumes.\n",
    "patch_kwargs = dict(\n",
    "    patch_shape=tuple(\n",
    "        np.floor(\n",
    "            np.asarray(params.train.in_patch_size) * params.data.downsampled_by_factor\n",
    "        ).astype(int)\n",
    "    ),\n",
    "    stride=1,\n",
    "    meta_keys_to_patch_index={\"dti\", \"log_euclid\", \"mask\"},\n",
    "    mask_name=\"mask\",\n",
    ")\n",
    "\n",
    "\n",
    "# Coefficients to the log-euclidean lower triangle/6D vector that properly scales\n",
    "# the Euclidean distance under the log-euclidean metrics.\n",
    "mat_norm_coeffs = torch.ones(6)\n",
    "mat_norm_coeffs[torch.as_tensor([1, 3, 4])] = np.sqrt(2)\n",
    "mat_norm_coeffs = mat_norm_coeffs.reshape(-1, 1, 1, 1)\n",
    "\n",
    "\n",
    "def fix_downsample_shape_errors(\n",
    "    fr_vol: torch.Tensor, fr_affine: torch.Tensor, target_spatial_shape: tuple\n",
    "):\n",
    "    \"\"\"Small utility to fix shape differences between LR and FR data.\"\"\"\n",
    "    target_shape = np.asarray(target_spatial_shape)\n",
    "    if fr_vol.shape[1:] != tuple(target_shape):\n",
    "        # Use torchio objects because they fix the affine matrix, too.\n",
    "        # Flip before transform to pad on the right/top/furthest side of the dimension\n",
    "        # first, before the left/bottom/closest.\n",
    "        flip_vol = fr_vol.flip([1, 2, 3])\n",
    "        im = torchio.ScalarImage(tensor=flip_vol, affine=fr_affine)\n",
    "        transform = torchio.transforms.CropOrPad(target_spatial_shape, 0, copy=False)\n",
    "        im = transform(im)\n",
    "        result_vol = im[\"data\"]\n",
    "        # Unflip.\n",
    "        result_vol = result_vol.flip([1, 2, 3])\n",
    "        result_aff = im[\"affine\"]\n",
    "    else:\n",
    "        result_vol = fr_vol\n",
    "        result_aff = fr_affine\n",
    "\n",
    "    return result_vol, result_aff\n",
    "\n",
    "\n",
    "def orient_to_viz(vol, affine):\n",
    "\n",
    "    if torch.is_tensor(vol):\n",
    "        v = vol.detach().cpu().numpy()\n",
    "    else:\n",
    "        v = vol\n",
    "    v = np.rot90(np.rot90(v, k=1, axes=(1, 3)), k=2, axes=(2, 3))\n",
    "    if torch.is_tensor(vol):\n",
    "        v = torch.from_numpy(np.copy(v)).to(vol)\n",
    "\n",
    "    # Adjust the affine matrix.\n",
    "    full_rot_aff = np.zeros_like(affine)\n",
    "    full_rot_aff[-1, -1] = 1.0\n",
    "    # 90 degree rot around the second axis.\n",
    "    q1 = nib.quaternions.angle_axis2quat(np.pi / 2, [0, 1, 0])\n",
    "    # 180 degree rot around the first axis.\n",
    "    q2 = nib.quaternions.angle_axis2quat(np.pi, [1, 0, 0])\n",
    "    new_q = nib.quaternions.mult(q1, q2)\n",
    "    rot_aff = nib.quaternions.quat2mat(new_q)\n",
    "    full_rot_aff[:-1, :-1] = rot_aff\n",
    "    new_aff = full_rot_aff @ affine\n",
    "\n",
    "    return v, new_aff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subj_gt = Box(default_box=True)\n",
    "\n",
    "meta_keys_to_keep = {\"affine\", \"original_affine\"}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for subj_id, subj_dir in subj_dirs.items():\n",
    "\n",
    "        data = dict()\n",
    "        data[\"subj_id\"] = subj_id\n",
    "        fr_subj_dir = subj_dirs[subj_id][\"fr\"]\n",
    "        lr_subj_dir = subj_dirs[subj_id][\"lr\"]\n",
    "        data[\"fr_subj_dir\"] = fr_subj_dir\n",
    "        data[\"lr_subj_dir\"] = lr_subj_dir\n",
    "\n",
    "        ####### Low-resolution DTIs/volumes\n",
    "        lr_dti_f = pitn.utils.system.get_file_glob_unique(\n",
    "            lr_subj_dir, params.data.dti_fname_pattern\n",
    "        )\n",
    "        im = nib_reader.read(lr_dti_f)\n",
    "        lr_dti, meta = nib_reader.get_data(im)\n",
    "        meta = {k: meta[k] for k in meta_keys_to_keep}\n",
    "        lr_dti = torch.from_numpy(lr_dti)\n",
    "        lr_dti, meta[\"affine\"] = orient_to_viz(lr_dti, meta[\"affine\"])\n",
    "        data[\"lr_dti\"] = lr_dti\n",
    "        data[\"lr_dti_meta_dict\"] = meta\n",
    "\n",
    "        # May need to handle shape errors when re-upscaling back from LR to HR.\n",
    "        lr_dti_shape = np.asarray(lr_dti.shape[1:])\n",
    "        target_fr_shape = np.floor(\n",
    "            lr_dti_shape * params.data.downsampled_by_factor\n",
    "        ).astype(int)\n",
    "\n",
    "        ####### Full-resolution images/volumes.\n",
    "        # DTI.\n",
    "        dti_f = pitn.utils.system.get_file_glob_unique(\n",
    "            fr_subj_dir, params.data.dti_fname_pattern\n",
    "        )\n",
    "        im = nib_reader.read(dti_f)\n",
    "        dti, meta = nib_reader.get_data(im)\n",
    "        dti = torch.from_numpy(dti)\n",
    "        meta = {k: meta[k] for k in meta_keys_to_keep}\n",
    "        dti, meta[\"affine\"] = fix_downsample_shape_errors(\n",
    "            dti, meta[\"affine\"], target_fr_shape\n",
    "        )\n",
    "        dti, meta[\"affine\"] = orient_to_viz(dti, meta[\"affine\"])\n",
    "        data[\"dti\"] = dti\n",
    "        data[\"dti_meta_dict\"] = meta\n",
    "\n",
    "        # Diffusion mask.\n",
    "        mask_f = pitn.utils.system.get_file_glob_unique(\n",
    "            fr_subj_dir, params.data.mask_fname_pattern\n",
    "        )\n",
    "        im = nib_reader.read(mask_f)\n",
    "        mask, meta = nib_reader.get_data(im)\n",
    "        meta = {k: meta[k] for k in meta_keys_to_keep}\n",
    "        mask = torch.from_numpy(mask)\n",
    "        # Add channel dim if not available.\n",
    "        if mask.ndim == 3:\n",
    "            mask = mask[None]\n",
    "        mask, meta[\"affine\"] = fix_downsample_shape_errors(\n",
    "            mask, meta[\"affine\"], target_fr_shape\n",
    "        )\n",
    "        mask, meta[\"affine\"] = orient_to_viz(mask, meta[\"affine\"])\n",
    "        mask = mask.bool()\n",
    "        data[\"mask\"] = mask\n",
    "        data[\"mask_meta_dict\"] = meta\n",
    "\n",
    "        # Construct a quick and cheap mask for the LR DTI\n",
    "        cheap_lr_mask = F.interpolate(\n",
    "            data[\"mask\"][None].float(),\n",
    "            size=data[\"lr_dti\"][0].shape,\n",
    "            mode=\"nearest\",\n",
    "        )[0]\n",
    "        data[\"lr_mask\"] = cheap_lr_mask.bool()\n",
    "\n",
    "        # Consider this as the \"noise correction\" step to have more informative, consistent\n",
    "        # results with minimal biasing. Otherwise, outliers (which are clearly errors) can\n",
    "        # change loss and performance metrics by orders of magnitude for no good reason!\n",
    "        if \"eigval_clip_cutoff\" in params.data and params.data.eigval_clip_cutoff:\n",
    "            correct_dti = pitn.data.outliers.clip_dti_eigvals(\n",
    "                data[\"dti\"].to(device),\n",
    "                tensor_components_dim=0,\n",
    "                eigval_max=params.data.eigval_clip_cutoff,\n",
    "            ).to(data[\"dti\"])\n",
    "            correct_dti = correct_dti * data[\"mask\"]\n",
    "            ####\n",
    "            # sae_fr = (\n",
    "            #     F.l1_loss(data[\"dti\"], correct_dti, reduction=\"none\") * data[\"mask\"]\n",
    "            # )\n",
    "            # sae_fr = sae_fr.view(6, -1).sum(1)\n",
    "            # mae_fr = sae_fr / torch.count_nonzero(data[\"mask\"])\n",
    "            # print(f\"---Subj {subj_id}---\")\n",
    "            # print(\n",
    "            #     \"MAE of FR DTI after eigenvalue clipping:\\n\",\n",
    "            #     mae_fr.tolist(),\n",
    "            # )\n",
    "            ####\n",
    "            data[\"dti\"] = correct_dti\n",
    "\n",
    "            correct_lr_dti = pitn.data.outliers.clip_dti_eigvals(\n",
    "                data[\"lr_dti\"].to(device),\n",
    "                tensor_components_dim=0,\n",
    "                eigval_max=params.data.eigval_clip_cutoff,\n",
    "            ).to(data[\"lr_dti\"])\n",
    "            lr_mask = data[\"lr_mask\"]\n",
    "            correct_lr_dti = correct_lr_dti * lr_mask\n",
    "            ####\n",
    "            # sae_lr = (\n",
    "            #     F.l1_loss(data[\"lr_dti\"], correct_lr_dti, reduction=\"none\") * lr_mask\n",
    "            # )\n",
    "            # sae_lr = sae_lr.view(6, -1).sum(1)\n",
    "            # mae_lr = sae_lr / torch.count_nonzero(lr_mask)\n",
    "            # print(\n",
    "            #     \"MAE of LR DTI after eigenvalue clipping:\\n\",\n",
    "            #     mae_lr.tolist(),\n",
    "            # )\n",
    "            ####\n",
    "            data[\"lr_dti\"] = correct_lr_dti\n",
    "\n",
    "        ####### Log-euclid pre-computed volumes\n",
    "\n",
    "        # LR log-euclid volume.\n",
    "        lr_log_euclid = pitn.eig.tril_vec2sym_mat(data[\"lr_dti\"], tril_dim=0)\n",
    "        lr_log_euclid = pitn.riemann.log_euclid.log_map(lr_log_euclid)\n",
    "        lr_log_euclid = pitn.eig.sym_mat2tril_vec(lr_log_euclid, tril_dim=0)\n",
    "        lr_log_euclid = lr_log_euclid * mat_norm_coeffs\n",
    "        data[\"lr_log_euclid\"] = lr_log_euclid\n",
    "\n",
    "        log_euclid = pitn.eig.tril_vec2sym_mat(data[\"dti\"], tril_dim=0)\n",
    "        log_euclid = pitn.riemann.log_euclid.log_map(log_euclid)\n",
    "        log_euclid = pitn.eig.sym_mat2tril_vec(log_euclid, tril_dim=0)\n",
    "        log_euclid = log_euclid * mat_norm_coeffs\n",
    "        data[\"log_euclid\"] = log_euclid\n",
    "\n",
    "        # Pre-compute FA maps\n",
    "        fa = pitn.metrics.fast_fa(\n",
    "            data[\"dti\"][None].to(device), foreground_mask=data[\"mask\"][None].to(device)\n",
    "        )\n",
    "        fa = fa.to(data[\"dti\"])[0]\n",
    "        data[\"fa\"] = fa\n",
    "\n",
    "        # Finalize this subject.\n",
    "        subj_gt[subj_id] = data\n",
    "        print(\"Loaded Subject \", subj_id)\n",
    "        print(\"=\" * 20)\n",
    "\n",
    "print(\"===Data Loaded===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Aggregate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only choose subjects in the training and validation datasets.\n",
    "\n",
    "subj_agg_stats = Box(default_box=True)\n",
    "subj_agg_stats.dti.min = torch.zeros(params.n_channels).to(subj_gt[selected_ids[0]].dti)\n",
    "subj_agg_stats.dti.max = torch.zeros(params.n_channels).to(subj_agg_stats.dti.min)\n",
    "\n",
    "subj_agg_stats.log_euclid.min = subj_agg_stats.dti.min\n",
    "subj_agg_stats.log_euclid.max = subj_agg_stats.dti.max\n",
    "\n",
    "for s in subj_gt.values():\n",
    "    fr_mask = s.mask\n",
    "    dti = torch.masked_select(s.dti, fr_mask)\n",
    "    subj_agg_stats.dti.min = torch.minimum(\n",
    "        subj_agg_stats.dti.min, dti.view(params.n_channels, -1).min(-1).values\n",
    "    )\n",
    "    subj_agg_stats.dti.max = torch.maximum(\n",
    "        subj_agg_stats.dti.max, dti.view(params.n_channels, -1).max(-1).values\n",
    "    )\n",
    "\n",
    "    lr_dti = s.lr_dti\n",
    "    lr_mask = s.lr_mask\n",
    "    lr_dti = torch.masked_select(lr_dti, lr_mask)\n",
    "    subj_agg_stats.dti.min = torch.minimum(\n",
    "        subj_agg_stats.dti.min, lr_dti.view(params.n_channels, -1).min(-1).values\n",
    "    )\n",
    "    subj_agg_stats.dti.max = torch.maximum(\n",
    "        subj_agg_stats.dti.max, lr_dti.view(params.n_channels, -1).max(-1).values\n",
    "    )\n",
    "\n",
    "\n",
    "print(subj_agg_stats.dti.min)\n",
    "print(subj_agg_stats.dti.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate global ranges of data for PSNR calculations.\n",
    "\n",
    "expander = functools.partial(einops.rearrange, pattern=\"c -> 1 c 1 1 1\")\n",
    "# Collect DTI global data features.\n",
    "dti_min = expander(subj_agg_stats.dti.min)\n",
    "dti_max = expander(subj_agg_stats.dti.max)\n",
    "\n",
    "feat_min, feat_max = torch.as_tensor(\n",
    "    [\n",
    "        [0] * 6,\n",
    "        [1] * 6,\n",
    "    ]\n",
    ")\n",
    "feat_min = expander(feat_min)\n",
    "feat_max = expander(feat_max)\n",
    "\n",
    "# PSNR is calculated on the final output tensor components, so no log-euclidean or\n",
    "# scaling will occur here.\n",
    "psnr_range_params = pitn.data.norm.GlobalScaleParams(\n",
    "    feature_min=feat_min, feature_max=feat_max, data_min=dti_min, data_max=dti_max\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select & Describe Models for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_params_from_config(run_config_file: Path) -> dict:\n",
    "    config_f = Path(run_config_file).resolve()\n",
    "    with open(config_f, \"r\") as f:\n",
    "        config_str = f.read()\n",
    "    config = ast.literal_eval(config_str)\n",
    "    # Clean up any recursive weirdness.\n",
    "    config = Box(config).to_dict()\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "baseline_spline_run = \"2022-03-27T05_51_39__test_spline\"\n",
    "baseline_espcn_runs = []\n",
    "baseline_revnet_runs = [\n",
    "    \"2022-03-22T20_40_20__uvers_espcn_revnet_split_1\",\n",
    "    \"2022-03-22T20_40_22__uvers_espcn_revnet_split_2\",\n",
    "    \"2022-03-23T04_03_59__uvers_espcn_revnet_split_3\",\n",
    "]\n",
    "diqt_carn_single_stream_runs = [\n",
    "    \"2022-03-27T13_24_22__uvers_pitn_single_stream_dti_split_1\",\n",
    "    \"2022-03-27T12_46_02__uvers_pitn_single_stream_dti_split_2\",\n",
    "    \"2022-03-27T08_29_27__uvers_pitn_single_stream_dti_split_3\",\n",
    "    \"2022-03-27T13_24_24__uvers_pitn_single_stream_le_split_1\",\n",
    "    \"2022-03-27T20_05_11__uvers_pitn_single_stream_le_split_2\",\n",
    "    \"2022-03-27T17_57_13__uvers_pitn_single_stream_le_split_3\",\n",
    "]\n",
    "diqt_carn_fake_anat_stream_runs = [\n",
    "    \"2022-03-25T17_15_37__uvers_pitn_fake_anat_stream_dti_split_1\",\n",
    "    \"2022-03-25T17_15_39__uvers_pitn_fake_anat_stream_dti_split_2\",\n",
    "    \"2022-03-26T19_11_13__uvers_pitn_fake_anat_stream_dti_split_3\",\n",
    "    \"2022-03-26T18_46_05__uvers_pitn_fake_anat_stream_le_split_1\",\n",
    "    \"2022-03-26T18_46_07__uvers_pitn_fake_anat_stream_le_split_2\",\n",
    "    \"2022-03-27T02_36_43__uvers_pitn_fake_anat_stream_le_split_3\",\n",
    "]\n",
    "diqt_carn_anat_stream_runs = [\n",
    "    \"2022-03-22T21_33_49__uvers_pitn_anat_stream_dti_split_1\",\n",
    "    \"2022-03-24T03_22_55__uvers_pitn_anat_stream_dti_split_2\",\n",
    "    \"2022-03-24T22_47_26__uvers_pitn_anat_stream_dti_split_3\",\n",
    "    \"2022-03-23T17_25_43__uvers_pitn_anat_stream_le_split_1\",\n",
    "    \"2022-03-24T12_56_41__uvers_pitn_anat_stream_le_split_2\",\n",
    "    \"2022-03-25T08_21_45__uvers_pitn_anat_stream_le_split_3\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Run Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_root_dir = results_dir\n",
    "run_table = collections.defaultdict(list)\n",
    "# cols = [\n",
    "#     \"run_name\",\n",
    "#     \"model_name\",\n",
    "#     \"split\",\n",
    "#     \"use_le\",\n",
    "#     \"use_anat\",\n",
    "#     \"streams\",\n",
    "#     \"use_half_precision\",\n",
    "# ]\n",
    "\n",
    "sp_run = baseline_spline_run\n",
    "run_dir = run_root_dir / sp_run\n",
    "run_table[\"run_name\"].append(sp_run)\n",
    "run_table[\"model_name\"].append(\"Cubic Spline\")\n",
    "run_table[\"split\"].append(0)\n",
    "run_table[\"use_le\"].append(True)\n",
    "run_table[\"use_anat\"].append(False)\n",
    "run_table[\"streams\"].append(1)\n",
    "run_table[\"use_half_precision\"].append(False)\n",
    "\n",
    "for run in baseline_revnet_runs:\n",
    "    run_dir = run_root_dir / run\n",
    "    run_table[\"run_name\"].append(run)\n",
    "    run_table[\"model_name\"].append(\"RevNet4\")\n",
    "    sp_idx = run.find(\"split_\") + (len(\"split_\") - 1) + 1\n",
    "    split = int(run[sp_idx])\n",
    "    run_table[\"split\"].append(split)\n",
    "    run_table[\"use_le\"].append(False)\n",
    "    run_table[\"use_anat\"].append(False)\n",
    "    run_table[\"streams\"].append(1)\n",
    "    run_table[\"use_half_precision\"].append(False)\n",
    "\n",
    "for run in diqt_carn_single_stream_runs:\n",
    "    run_dir = run_root_dir / run\n",
    "    run_table[\"run_name\"].append(run)\n",
    "\n",
    "    sp_idx = run.find(\"split_\") + (len(\"split_\") - 1) + 1\n",
    "    split = int(run[sp_idx])\n",
    "    run_table[\"split\"].append(split)\n",
    "\n",
    "    run_table[\"use_le\"].append(\"_le_\" in run)\n",
    "    run_table[\"use_anat\"].append(False)\n",
    "    run_table[\"streams\"].append(1)\n",
    "    run_table[\"use_half_precision\"].append(True)\n",
    "\n",
    "    model_basename = \"CARN\"\n",
    "    if run_table[\"use_le\"][-1]:\n",
    "        model_name = model_basename + \" LE\"\n",
    "    else:\n",
    "        model_name = model_basename + \" DTI\"\n",
    "    run_table[\"model_name\"].append(model_name)\n",
    "\n",
    "for run in diqt_carn_fake_anat_stream_runs:\n",
    "    run_dir = run_root_dir / run\n",
    "    run_table[\"run_name\"].append(run)\n",
    "\n",
    "    sp_idx = run.find(\"split_\") + (len(\"split_\") - 1) + 1\n",
    "    split = int(run[sp_idx])\n",
    "    run_table[\"split\"].append(split)\n",
    "\n",
    "    run_params_f = run_dir / \"run_params.yaml\"\n",
    "    with open(run_params_f, \"r\") as f:\n",
    "        p = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    run_p = Box(p)\n",
    "    run_table[\"use_le\"].append(run_p.use_log_euclid)\n",
    "    run_table[\"use_anat\"].append(False)\n",
    "    run_table[\"streams\"].append(2)\n",
    "    run_table[\"use_half_precision\"].append(run_p.use_half_precision_float)\n",
    "\n",
    "    model_basename = \"CARN Fake Anat\"\n",
    "    if run_p.use_log_euclid:\n",
    "        model_name = model_basename + \" LE\"\n",
    "    else:\n",
    "        model_name = model_basename + \" DTI\"\n",
    "    run_table[\"model_name\"].append(model_name)\n",
    "\n",
    "for run in diqt_carn_anat_stream_runs:\n",
    "    run_dir = run_root_dir / run\n",
    "    run_table[\"run_name\"].append(run)\n",
    "\n",
    "    sp_idx = run.find(\"split_\") + (len(\"split_\") - 1) + 1\n",
    "    split = int(run[sp_idx])\n",
    "    run_table[\"split\"].append(split)\n",
    "\n",
    "    run_params_f = run_dir / \"run_params.yaml\"\n",
    "    with open(run_params_f, \"r\") as f:\n",
    "        p = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    run_p = Box(p)\n",
    "    run_table[\"use_le\"].append(run_p.use_log_euclid)\n",
    "    run_table[\"use_anat\"].append(True)\n",
    "    run_table[\"streams\"].append(2)\n",
    "    run_table[\"use_half_precision\"].append(run_p.use_half_precision_float)\n",
    "\n",
    "    model_basename = \"CARN Anat\"\n",
    "    if run_p.use_log_euclid:\n",
    "        model_name = model_basename + \" LE\"\n",
    "    else:\n",
    "        model_name = model_basename + \" DTI\"\n",
    "    run_table[\"model_name\"].append(model_name)\n",
    "\n",
    "\n",
    "run_table = pd.DataFrame.from_dict(run_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_results = Box()\n",
    "run_root_dir = results_dir\n",
    "\n",
    "# Handle cubic spline baseline results.\n",
    "sp_run = baseline_spline_run\n",
    "print(sp_run)\n",
    "sp_run_dir = run_root_dir / sp_run\n",
    "sp_run_res = Box(default_box=True)\n",
    "sp_run_res.name = sp_run\n",
    "sp_run_res.dir = sp_run_dir.resolve()\n",
    "sp_run_res.split = 0\n",
    "sp_run_res.metrics = pd.read_csv(sp_run_res.dir / \"test_loss.csv\", index_col=False)\n",
    "sp_run_res.model_name = \"Cubic Spline\"\n",
    "sp_run_res.metrics = sp_run_res.metrics.replace(\n",
    "    {\"model\": \"diqt\"}, {\"model\": sp_run_res.model_name}\n",
    ")\n",
    "split_col = pd.DataFrame.from_dict(\n",
    "    {\"split\": list(itertools.repeat(sp_run_res.split, len(sp_run_res.metrics)))}\n",
    ")\n",
    "sp_run_res.metrics = pd.concat([sp_run_res.metrics, split_col], axis=1)\n",
    "sp_run_params_f = sp_run_dir / \"run_params.yaml\"\n",
    "if sp_run_params_f.exists():\n",
    "    with open(sp_run_params_f, \"r\") as f:\n",
    "        p = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    sp_run_res.params = dict(p)\n",
    "else:\n",
    "    sp_run_res.params = None\n",
    "pred_zip = sp_run_dir / \"predicted_dti.zip\"\n",
    "sp_run_res.pred_zip = pred_zip.resolve()\n",
    "pred_le_zip = sp_run_dir / \"predicted_le.zip\"\n",
    "sp_run_res.pred_le_zip = pred_le_zip.resolve()\n",
    "sp_run_res.pred_pre_anat_zip = None\n",
    "\n",
    "run_results[sp_run] = sp_run_res\n",
    "\n",
    "for run in itertools.chain(\n",
    "    baseline_espcn_runs,\n",
    "    baseline_revnet_runs,\n",
    "    diqt_carn_single_stream_runs,\n",
    "    diqt_carn_fake_anat_stream_runs,\n",
    "    diqt_carn_anat_stream_runs,\n",
    "):\n",
    "    print(run)\n",
    "    run_res = Box(default_box=True)\n",
    "    run_dir = run_root_dir / run\n",
    "    run_res.name = run\n",
    "    run_res.dir = run_dir.resolve()\n",
    "    run_res.split = run_table.loc[run_table.run_name == run].split.iloc[0]\n",
    "    run_res.metrics = pd.read_csv(run_res.dir / \"test_loss.csv\", index_col=False)\n",
    "    model_name = run_table.loc[run_table.run_name == run].model_name.iloc[0]\n",
    "    run_res.model_name = model_name\n",
    "\n",
    "    run_res.metrics = run_res.metrics.replace(\n",
    "        {\"model\": \"diqt\"}, {\"model\": run_res.model_name}\n",
    "    )\n",
    "    split_col = pd.DataFrame.from_dict(\n",
    "        {\"split\": list(itertools.repeat(run_res.split, len(run_res.metrics)))}\n",
    "    )\n",
    "    run_res.metrics = pd.concat([run_res.metrics, split_col], axis=1)\n",
    "\n",
    "    run_params_f = run_dir / \"run_params.yaml\"\n",
    "    if run_params_f.exists():\n",
    "        with open(run_params_f, \"r\") as f:\n",
    "            p = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        run_res.params = dict(p)\n",
    "    else:\n",
    "        run_res.params = None\n",
    "\n",
    "    pred_zip = run_dir / \"predicted_dti.zip\"\n",
    "    run_res.pred_zip = pred_zip.resolve()\n",
    "    if (run_dir / \"predicted_pre_anat_dti.zip\").exists():\n",
    "        run_res.pred_pre_anat_zip = run_dir / \"predicted_pre_anat_dti.zip\"\n",
    "    else:\n",
    "        run_res.pred_pre_anat_zip = None\n",
    "\n",
    "    run_results[run] = run_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpack Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make this cell idempotent\n",
    "for run in run_results.values():\n",
    "    unzip_dir = run.pred_zip.parent / \"predicted_dti\"\n",
    "    unzip_dir.mkdir(exist_ok=True)\n",
    "    with zipfile.ZipFile(run.pred_zip, \"r\") as z:\n",
    "        for f in z.namelist():\n",
    "            unzip_pred = unzip_dir / f\n",
    "            if unzip_pred.exists():\n",
    "                continue\n",
    "            z.extract(f, unzip_dir)\n",
    "    run.pred_dir = unzip_dir\n",
    "    pred_subjs = [f.name[: f.name.find(\"_pr\")] for f in run.pred_dir.glob(\"*\")]\n",
    "    pred_vols = {\n",
    "        s: pitn.utils.system.get_file_glob_unique(run.pred_dir, s + \"*\")\n",
    "        for s in pred_subjs\n",
    "    }\n",
    "    run.pred_vols = pred_vols\n",
    "\n",
    "    if run.pred_pre_anat_zip is not None:\n",
    "        unzip_dir = run.pred_pre_anat_zip.parent / \"predicted_pre_anat_dti\"\n",
    "        unzip_dir.mkdir(exist_ok=True)\n",
    "        run.pred_pre_anat_dir = unzip_dir\n",
    "        with zipfile.ZipFile(run.pred_pre_anat_zip, \"r\") as z:\n",
    "            for f in z.namelist():\n",
    "                unzip_pred = unzip_dir / f\n",
    "                if unzip_pred.exists():\n",
    "                    continue\n",
    "                z.extract(f, unzip_dir)\n",
    "\n",
    "        pred_pre_anat_vols = {\n",
    "            s: pitn.utils.system.get_file_glob_unique(run.pred_pre_anat_dir, s + \"*\")\n",
    "            for s in pred_subjs\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        unzip_dir = None\n",
    "        pred_pre_anat_vols = None\n",
    "\n",
    "    if \"pred_le_zip\" in run:\n",
    "        unzip_dir = run.pred_le_zip.parent / \"predicted_le\"\n",
    "        unzip_dir.mkdir(exist_ok=True)\n",
    "        run.pred_le_dir = unzip_dir\n",
    "        with zipfile.ZipFile(run.pred_le_zip, \"r\") as z:\n",
    "            for f in z.namelist():\n",
    "                unzip_pred = unzip_dir / f\n",
    "                if unzip_pred.exists():\n",
    "                    continue\n",
    "                z.extract(f, unzip_dir)\n",
    "\n",
    "        pred_le_vols = {\n",
    "            s: pitn.utils.system.get_file_glob_unique(run.pred_le_dir, s + \"*\")\n",
    "            for s in pred_subjs\n",
    "        }\n",
    "        run.pred_le_vols = pred_le_vols\n",
    "\n",
    "    run.pred_pre_anat_dir = unzip_dir\n",
    "    run.pred_pre_anat_vols = pred_pre_anat_vols\n",
    "    print(\"Finished \", run.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Quantify Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select metrics to show.\n",
    "perf_metrics_with_directions = {\n",
    "    \"rmse\": \"↓\",\n",
    "    # \"nrmse\": \"↓\",\n",
    "    \"rmse_log_euclid\": \"↓\",\n",
    "    # \"nrmse_log_euclid\": \"↓\",\n",
    "    \"scaled_psnr\": \"↑\",\n",
    "    \"ssim_fa\": \"↑\",\n",
    "    # \"rmse_fa\": \"↓\",\n",
    "    # \"nrmse_fa\": \"↓\",\n",
    "}\n",
    "\n",
    "metric_rename_map = {\n",
    "    \"rmse\": \"DTI RMFD\",\n",
    "    \"rmse_log_euclid\": \"LE RMFD\",\n",
    "    \"scaled_psnr\": \"PSNR\",\n",
    "    \"ssim_fa\": \"FA SSIM\",\n",
    "}\n",
    "\n",
    "perf_metrics = tuple(perf_metrics_with_directions.keys())\n",
    "\n",
    "# Merge all results from all runs together.\n",
    "test_results = pd.concat([r.metrics for r in run_results.values()], ignore_index=True)\n",
    "test_results = test_results.loc[np.isin(test_results.metric, perf_metrics)]\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_table = (\n",
    "    test_results.groupby([\"model\", \"metric\"]).mean().drop(columns=[\"subj_id\", \"split\"])\n",
    ")\n",
    "results_table[\"std\"] = test_results.groupby([\"model\", \"metric\"]).std().value\n",
    "results_table = results_table.rename(columns={\"value\": \"mean\"})\n",
    "\n",
    "results_table = results_table.reset_index()\n",
    "rmse_mask = results_table.metric == \"rmse\"\n",
    "\n",
    "# Scale rmse by 100 for readability.\n",
    "# !Scale both the mean and std by 100\n",
    "results_table.loc[rmse_mask, \"mean\"] = (results_table[\"mean\"] * 100)[rmse_mask]\n",
    "results_table.loc[rmse_mask, \"std\"] = (results_table[\"std\"] * 100)[rmse_mask]\n",
    "\n",
    "float_str_format = \"{:.3f}\"\n",
    "results_table[\"display\"] = results_table.apply(\n",
    "    lambda row: \"$\"\n",
    "    + float_str_format.format(np.round(row[\"mean\"], decimals=3))\n",
    "    + r\" \\pm \"\n",
    "    + float_str_format.format(np.round(row[\"std\"], decimals=3))\n",
    "    + \"$\",\n",
    "    axis=\"columns\",\n",
    ")\n",
    "\n",
    "\n",
    "results_table = results_table.pivot(index=\"model\", columns=\"metric\", values=\"display\")\n",
    "# Rename rmse metric after scaling\n",
    "results_table = results_table.rename(\n",
    "    columns={\"rmse\": r\"$\\textrm{RMSE} \\times 10^{-2} \\downarrow$\"}\n",
    ")\n",
    "results_table = results_table.rename(\n",
    "    columns={\n",
    "        \"rmse_log_euclid\": r\"$\\textrm{RMSE LE} \\downarrow$\",\n",
    "        \"scaled_psnr\": r\"$\\textrm{PSNR} \\uparrow$\",\n",
    "        \"ssim_fa\": r\"$\\textrm{SSIM FA} \\uparrow$\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Reorder row/model order\n",
    "results_table = results_table.loc[\n",
    "    [\n",
    "        \"Cubic Spline\",\n",
    "        \"RevNet4\",\n",
    "        \"CARN DTI\",\n",
    "        \"CARN LE\",\n",
    "        \"CARN Fake Anat DTI\",\n",
    "        \"CARN Fake Anat LE\",\n",
    "        \"CARN Anat DTI\",\n",
    "        \"CARN Anat LE\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "# Save out table of results.\n",
    "results_table.to_csv(\"test_results.csv\")\n",
    "with open(\"test_results_table.tex\", \"w\") as f:\n",
    "    latex_tab = results_table.to_latex(escape=False)\n",
    "    f.write(latex_tab)\n",
    "\n",
    "print(latex_tab)\n",
    "results_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Viz and Display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DTI Eigenvalue Validity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Subject-specific non-zero eigenvalues.\n",
    "select_sub_id = HOLDOUT_SUBJ_ID\n",
    "m_names = list(run_table.model_name.unique())\n",
    "\n",
    "for m in m_names:\n",
    "    print(m)\n",
    "    model_hist = list()\n",
    "    rs = list(run_table.loc[run_table.model_name == m].run_name.unique())\n",
    "    for r in rs:\n",
    "        print(r)\n",
    "        run = run_results[r]\n",
    "        pred_dti = torch.from_numpy(nib.load(run.pred_vols[select_sub_id]).get_fdata())\n",
    "        mask = subj_gt[select_sub_id][\"mask\"]\n",
    "\n",
    "        if mask.shape[1:] != pred_dti.shape[1:]:\n",
    "            t = monai.transforms.CenterSpatialCrop(pred_dti.shape[1:])\n",
    "            mask = t(mask)\n",
    "        dti_mat = pitn.eig.tril_vec2sym_mat(pred_dti, tril_dim=0)\n",
    "        dti_mat = dti_mat[mask[0]]\n",
    "        eigvals = pitn.eig.eigvalsh_workaround(dti_mat)\n",
    "        model_hist.append(eigvals.T.detach().cpu().numpy())\n",
    "        # break\n",
    "    model_hist = np.concatenate(model_hist, axis=1)\n",
    "    print(f\"Eigvals < 0: {(model_hist < 0).sum(axis=1)} out of {model_hist.shape[1]}\")\n",
    "    le_0_mask = model_hist < 0\n",
    "    fig, axs = plt.subplots(ncols=3, sharex=True, sharey=True, figsize=(12, 2), dpi=100)\n",
    "    # Lambda 1, 2, then 3\n",
    "    if le_0_mask[0].sum() > 0:\n",
    "        sns.histplot(model_hist[0][le_0_mask[0]], log_scale=False, ax=axs[0])\n",
    "    if le_0_mask[1].sum() > 0:\n",
    "        sns.histplot(\n",
    "            model_hist[1][le_0_mask[1]], log_scale=False, ax=axs[1], color=\"red\"\n",
    "        )\n",
    "    if le_0_mask[2].sum() > 0:\n",
    "        sns.histplot(\n",
    "            model_hist[2][le_0_mask[2]], log_scale=False, ax=axs[2], color=\"green\"\n",
    "        )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Negative eigenvalues distribution for all subjects (each subj counted only once)\n",
    "m_names = list(run_table.model_name.unique())\n",
    "\n",
    "for m in m_names:\n",
    "    print(m)\n",
    "    sub_model_hist = dict(l1=list(), l2=list(), l3=list())\n",
    "    subs_seen = set()\n",
    "    rs = list(run_table.loc[run_table.model_name == m].run_name.unique())\n",
    "    for r in rs:\n",
    "        print(r)\n",
    "        run = run_results[r]\n",
    "        for sub_id in run.pred_vols.keys():\n",
    "            if sub_id in subs_seen:\n",
    "                continue\n",
    "            pred_dti = torch.from_numpy(nib.load(run.pred_vols[sub_id]).get_fdata())\n",
    "            mask = subj_gt[sub_id][\"mask\"]\n",
    "\n",
    "            if mask.shape[1:] != pred_dti.shape[1:]:\n",
    "                t = monai.transforms.CenterSpatialCrop(pred_dti.shape[1:])\n",
    "                mask = t(mask)\n",
    "            dti_mat = pitn.eig.tril_vec2sym_mat(pred_dti, tril_dim=0)\n",
    "            dti_mat = dti_mat[mask[0]]\n",
    "            eigvals = pitn.eig.eigvalsh_workaround(dti_mat)\n",
    "            eigvals = eigvals.T.detach().cpu().numpy()\n",
    "            eigvals_mask = eigvals < 0\n",
    "            # model_hist.append(eigvals.T.detach().cpu().numpy())\n",
    "            sub_model_hist[\"l1\"].append(eigvals[0][eigvals_mask[0]])\n",
    "            sub_model_hist[\"l2\"].append(eigvals[1][eigvals_mask[1]])\n",
    "            sub_model_hist[\"l3\"].append(eigvals[2][eigvals_mask[2]])\n",
    "\n",
    "            subs_seen = subs_seen | {\n",
    "                sub_id,\n",
    "            }\n",
    "\n",
    "    sub_model_hist[\"l1\"] = np.concatenate(sub_model_hist[\"l1\"], axis=0)\n",
    "    sub_model_hist[\"l2\"] = np.concatenate(sub_model_hist[\"l2\"], axis=0)\n",
    "    sub_model_hist[\"l3\"] = np.concatenate(sub_model_hist[\"l3\"], axis=0)\n",
    "\n",
    "    print(\n",
    "        f\"Eigvals < 0: {len(sub_model_hist['l1'])}, {len(sub_model_hist['l2'])}, {len(sub_model_hist['l3'])}\"\n",
    "    )\n",
    "    print(\"Min eigvals \")\n",
    "    print(sub_model_hist[\"l1\"].min() if len(sub_model_hist[\"l1\"]) > 0 else \"NA\")\n",
    "    print(sub_model_hist[\"l2\"].min() if len(sub_model_hist[\"l2\"]) > 0 else \"NA\")\n",
    "    print(sub_model_hist[\"l3\"].min() if len(sub_model_hist[\"l3\"]) > 0 else \"NA\")\n",
    "\n",
    "    with mpl.rc_context({\"font.size\": 6.0}):\n",
    "        fig, axs = plt.subplots(\n",
    "            ncols=3, sharex=True, sharey=True, figsize=(12, 2), dpi=100\n",
    "        )\n",
    "        # Lambda 1, 2, then 3\n",
    "        if len(sub_model_hist[\"l1\"]) > 0:\n",
    "            sns.histplot(sub_model_hist[\"l1\"], ax=axs[0])\n",
    "        if len(sub_model_hist[\"l2\"]) > 0:\n",
    "            sns.histplot(sub_model_hist[\"l2\"], ax=axs[1], color=\"red\")\n",
    "        if len(sub_model_hist[\"l3\"]) > 0:\n",
    "            sns.histplot(sub_model_hist[\"l3\"], ax=axs[2], color=\"green\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### FA Prediction Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Negative eigenvalues distribution for all subjects (each subj counted only once)\n",
    "select_sub_id = HOLDOUT_SUBJ_ID\n",
    "select_sub_ids = {\n",
    "    \"141422\",\n",
    "    \"135528\",\n",
    "    \"103010\",\n",
    "    # HOLDOUT_SUBJ_ID,\n",
    "    # \"567759\",\n",
    "    # \"815247\",\n",
    "}\n",
    "m_names = list(run_table.model_name.unique())\n",
    "with mpl.rc_context({\"font.size\": 6.0}):\n",
    "    fig, axs = plt.subplots(\n",
    "        ncols=len(select_sub_ids),\n",
    "        nrows=len(m_names),\n",
    "        figsize=(9, 2.3 * len(select_sub_ids)),\n",
    "        dpi=180,\n",
    "        sharex=\"all\",\n",
    "        sharey=\"all\",\n",
    "    )\n",
    "    for i_m, m in enumerate(m_names):\n",
    "        print(m)\n",
    "        fa_diff_hist = dict()\n",
    "        subs_seen = set()\n",
    "        rs = list(run_table.loc[run_table.model_name == m].run_name.unique())\n",
    "        for r in rs:\n",
    "            print(r)\n",
    "            run = run_results[r]\n",
    "            for j_sub, sub_id in enumerate(select_sub_ids):\n",
    "                if sub_id in subs_seen:\n",
    "                    continue\n",
    "                if sub_id not in run.pred_vols.keys():\n",
    "                    continue\n",
    "                pred_dti = torch.from_numpy(nib.load(run.pred_vols[sub_id]).get_fdata())\n",
    "                mask = subj_gt[sub_id][\"mask\"]\n",
    "                gt_fa = subj_gt[sub_id][\"fa\"]\n",
    "                if mask.shape[1:] != pred_dti.shape[1:]:\n",
    "                    t = monai.transforms.CenterSpatialCrop(pred_dti.shape[1:])\n",
    "                    mask = t(mask)\n",
    "                    gt_fa = t(gt_fa)\n",
    "\n",
    "                fa = pitn.metrics.fast_fa(pred_dti[None], mask[None])[0]\n",
    "\n",
    "                diff = (fa - gt_fa)[mask].flatten()\n",
    "                diff = diff.detach().cpu().numpy()\n",
    "                sel_diff = diff  # [np.where(np.abs(diff) > 1e-6)]\n",
    "\n",
    "                print(f\"Diff range: {diff.min()} - {diff.max()}\")\n",
    "                print(f\"Skew {scipy.stats.skew(sel_diff)}\")\n",
    "                print(\n",
    "                    f\"Statistically significant skew? {scipy.stats.skewtest(sel_diff)}\"\n",
    "                )\n",
    "                sns.histplot(sel_diff, ax=axs[i_m, j_sub], kde=True, thresh=10)\n",
    "                if j_sub == 0:\n",
    "                    axs[i_m, j_sub].set_ylabel(m)\n",
    "\n",
    "                axs[i_m, j_sub].axvline(\n",
    "                    np.mean(sel_diff), color=\"red\", ls=\"--\", alpha=0.8\n",
    "                )\n",
    "                axs[i_m, j_sub].axvline(\n",
    "                    np.median(sel_diff), color=\"green\", ls=\"-\", alpha=0.8\n",
    "                )\n",
    "\n",
    "                subs_seen = subs_seen | {\n",
    "                    sub_id,\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select metrics to show.\n",
    "perf_metrics_with_directions = {\n",
    "    \"rmse\": \"↓\",\n",
    "    # \"nrmse\": \"↓\",\n",
    "    \"rmse_log_euclid\": \"↓\",\n",
    "    # \"nrmse_log_euclid\": \"↓\",\n",
    "    \"scaled_psnr\": \"↑\",\n",
    "    \"ssim_fa\": \"↑\",\n",
    "    # \"rmse_fa\": \"↓\",\n",
    "    # \"nrmse_fa\": \"↓\",\n",
    "}\n",
    "\n",
    "metric_rename_map = {\n",
    "    \"rmse\": \"DTI RMFD\",\n",
    "    \"rmse_log_euclid\": \"LE RMFD\",\n",
    "    \"scaled_psnr\": \"PSNR\",\n",
    "    \"ssim_fa\": \"FA SSIM\",\n",
    "}\n",
    "\n",
    "perf_metrics = tuple(perf_metrics_with_directions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge all results from all runs together.\n",
    "test_results = pd.concat([r.metrics for r in run_results.values()], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_results.groupby([\"model\", \"metric\", \"split\"]).mean().drop(columns=\"subj_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Main result plot\n",
    "n_splits = len(test_results.split.unique())\n",
    "n_metrics = len(test_results.metric.unique())\n",
    "\n",
    "# models_to_display = list(test_results.model.unique())\n",
    "models_to_display = [\n",
    "    \"Cubic Spline\",\n",
    "    \"RevNet4\",\n",
    "    \"CARN Anat DTI\",\n",
    "    \"CARN Anat LE\",\n",
    "]\n",
    "\n",
    "model_rename_mapping = {\n",
    "    \"Cubic Spline\": \"Cubic Spline\",\n",
    "    \"RevNet4\": \"RevNet4\",\n",
    "    \"CARN Anat DTI\": \"CARN DTI Anat\\n(Ours)\",\n",
    "    \"CARN Anat LE\": \"CARN LE Anat\\n(Ours)\",\n",
    "}\n",
    "n_models = len(models_to_display)\n",
    "\n",
    "with mpl.rc_context({\"font.size\": 9.0}):\n",
    "\n",
    "    ncols = 2\n",
    "    nrows = 2  # np.ceil(n_metrics / ncols).astype(int)\n",
    "    figsize = (n_models * ncols, nrows * 3)\n",
    "    dpi = 160\n",
    "    fig, axs = plt.subplots(\n",
    "        ncols=ncols,\n",
    "        nrows=nrows,\n",
    "        # sharex=True,\n",
    "        figsize=figsize,\n",
    "        dpi=dpi,\n",
    "        # gridspec_kw={\"wspace\": 1.0, \"hspace\": 1.0},\n",
    "    )\n",
    "    axs = axs.flatten()\n",
    "    sns.despine(fig=fig, top=True, right=True)\n",
    "\n",
    "    all_colors = sns.color_palette(\n",
    "        \"tab10\",\n",
    "        n_colors=n_splits + n_models,\n",
    "    )\n",
    "    model_colors = all_colors[:n_models]\n",
    "    run_colors = all_colors[n_models:]\n",
    "    # run_order = list(test_results.run_name.unique())\n",
    "\n",
    "    ax_count = 0\n",
    "    for i, l in enumerate(perf_metrics):\n",
    "\n",
    "        ax = axs[i]\n",
    "        # Convert metric index to ASCII letters...easier than hand-coding the alphabet.\n",
    "        subfig_section = str(chr(97 + i)).upper()\n",
    "        df = test_results.loc[test_results.metric == l].copy()\n",
    "        df = df.loc[np.isin(df.model, models_to_display)]\n",
    "        df.model = df.model.apply(lambda x: model_rename_mapping[x])\n",
    "\n",
    "        # model_rename_mapping[\n",
    "        # test_results.loc[np.isin(test_results.model, curr_comp)]\n",
    "        vplot = sns.violinplot(\n",
    "            x=\"model\",\n",
    "            y=\"value\",\n",
    "            data=df,\n",
    "            ax=ax,\n",
    "            scale=\"count\",\n",
    "            inner=\"box\",\n",
    "            palette=model_colors,\n",
    "            # lw=0.5,\n",
    "        )\n",
    "        ax.grid(axis=\"y\", alpha=0.5)\n",
    "\n",
    "        # points_plot = sns.stripplot(\n",
    "        #     x=\"model\",\n",
    "        #     y=\"value\",\n",
    "        #     hue=\"split\",\n",
    "        #     # hue_order=run_order,\n",
    "        #     jitter=0.17,\n",
    "        #     data=df,\n",
    "        #     ax=ax,\n",
    "        #     palette=run_colors,\n",
    "        #     # palette=plt.get_cmap('gist_rainbow'),\n",
    "        #     # color=\"black\",\n",
    "        #     edgecolor=\"white\",\n",
    "        #     size=2.8,\n",
    "        #     linewidth=0.7,\n",
    "        # )\n",
    "        # points_plot.get_legend().remove()\n",
    "\n",
    "        # Calculate mean performance score.\n",
    "        means = df.groupby([\"model\", \"split\"]).mean().groupby(\"model\").mean()\n",
    "        # Make sure the order follows seaborn's x-axis ordering.\n",
    "        model_order = list(map(lambda ax: ax.get_text(), axs[i].get_xticklabels()))\n",
    "        means = means.reindex(model_order)\n",
    "\n",
    "        #         lines = ax.hlines(\n",
    "        #             y=means.value,\n",
    "        #             xmin=np.arange(0, len(means)) - 0.5 + 0.05,\n",
    "        #             xmax=np.arange(1, len(means) + 1) - 0.5 - 0.05,\n",
    "        #             colors=model_colors,\n",
    "        #             lw=1.5,\n",
    "        #             zorder=1000,\n",
    "        #         )\n",
    "\n",
    "        #         outline_path_effects = [\n",
    "        #             mpl.patheffects.Stroke(linewidth=5, foreground=\"white\", alpha=0.9),\n",
    "        #             mpl.patheffects.Normal(),\n",
    "        #         ]\n",
    "        #         lines.set_path_effects(outline_path_effects)\n",
    "\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=25)\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        ax_format = ax.get_yaxis().get_major_formatter()\n",
    "\n",
    "        #         for m, c in zip(means.value, model_colors):\n",
    "\n",
    "        #             ax.annotate(\n",
    "        #                 f\"{m:.4g}\",\n",
    "        #                 xy=(ax.get_xlim()[0] + (ax.get_xlim()[0] * 0.4), m),\n",
    "        #                 xycoords=\"data\",\n",
    "        #                 color=c,\n",
    "        #                 ha=\"right\",\n",
    "        #                 va=\"center\",\n",
    "        #                 annotation_clip=False,\n",
    "        #                 fontweight=\"bold\",\n",
    "        #                 snap=True,\n",
    "        #                 bbox=dict(\n",
    "        #                     boxstyle=\"square,pad=0.3\", fc=\"white\", lw=0, snap=True, alpha=0.75\n",
    "        #                 ),\n",
    "        #             )\n",
    "        ax.set_title(f\"{metric_rename_map[l]} {perf_metrics_with_directions[l]}\")\n",
    "        ax.set_ylabel(\"\")\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.annotate(\n",
    "            subfig_section,\n",
    "            (-0.3, 1.2),\n",
    "            xycoords=\"axes fraction\",\n",
    "            fontweight=\"bold\",\n",
    "            fontstyle=\"oblique\",\n",
    "            fontsize=\"x-large\",\n",
    "            horizontalalignment=\"left\",\n",
    "            verticalalignment=\"top\",\n",
    "        )\n",
    "\n",
    "    for ax in axs[: (nrows - 1) * ncols]:\n",
    "        # sns.despine(ax=ax, left=True, bottom=True, top=True, right=True, trim=True)\n",
    "        # ax.set_yticks([])\n",
    "        # ax.set_xticks([])\n",
    "        # ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "# plt.suptitle(\"DIQT V-Fro DTI vs\")\n",
    "# plt.savefig(\"uvers_all_runs_violin.pdf\")\n",
    "plt.savefig(\"main_results_violin.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ablation result violin plot\n",
    "n_splits = len(test_results.split.unique())\n",
    "n_metrics = len(test_results.metric.unique())\n",
    "\n",
    "# models_to_display = list(test_results.model.unique())\n",
    "models_to_display = [\n",
    "    \"CARN DTI\",\n",
    "    \"CARN LE\",\n",
    "    \"CARN Fake Anat DTI\",\n",
    "    \"CARN Fake Anat LE\",\n",
    "    \"CARN Anat DTI\",\n",
    "    \"CARN Anat LE\",\n",
    "]\n",
    "\n",
    "model_rename_mapping = {\n",
    "    \"CARN DTI\": \"CARN DTI\",\n",
    "    \"CARN LE\": \"CARN LE\",\n",
    "    \"CARN Fake Anat DTI\": \"CARN DTI\\nw/out Anat\",\n",
    "    \"CARN Fake Anat LE\": \"CARN LE\\nw/out Anat\",\n",
    "    \"CARN Anat DTI\": \"CARN DTI Anat\",\n",
    "    \"CARN Anat LE\": \"CARN LE Anat\",\n",
    "}\n",
    "n_models = len(models_to_display)\n",
    "\n",
    "with mpl.rc_context({\"font.size\": 9.0}):\n",
    "\n",
    "    ncols = 2\n",
    "    nrows = 2  # np.ceil(n_metrics / ncols).astype(int)\n",
    "    figsize = (n_models * ncols, nrows * 3)\n",
    "    dpi = 160\n",
    "    fig, axs = plt.subplots(\n",
    "        ncols=ncols,\n",
    "        nrows=nrows,\n",
    "        # sharex=True,\n",
    "        figsize=figsize,\n",
    "        dpi=dpi,\n",
    "        # gridspec_kw={\"wspace\": 1.0, \"hspace\": 1.0},\n",
    "    )\n",
    "    axs = axs.flatten()\n",
    "    sns.despine(fig=fig, top=True, right=True)\n",
    "\n",
    "    all_colors = sns.color_palette(\n",
    "        \"tab10\",\n",
    "        n_colors=n_splits + n_models,\n",
    "    )\n",
    "    model_colors = all_colors[:n_models]\n",
    "    run_colors = all_colors[n_models:]\n",
    "    # run_order = list(test_results.run_name.unique())\n",
    "\n",
    "    ax_count = 0\n",
    "    for i, l in enumerate(perf_metrics):\n",
    "\n",
    "        ax = axs[i]\n",
    "        # Convert metric index to ASCII letters...easier than hand-coding the alphabet.\n",
    "        subfig_section = str(chr(97 + i)).upper()\n",
    "        df = test_results.loc[test_results.metric == l].copy()\n",
    "        df = df.loc[np.isin(df.model, models_to_display)]\n",
    "        df.model = df.model.apply(lambda x: model_rename_mapping[x])\n",
    "\n",
    "        # model_rename_mapping[\n",
    "        # test_results.loc[np.isin(test_results.model, curr_comp)]\n",
    "        vplot = sns.violinplot(\n",
    "            x=\"model\",\n",
    "            y=\"value\",\n",
    "            data=df,\n",
    "            ax=ax,\n",
    "            scale=\"count\",\n",
    "            inner=\"box\",\n",
    "            palette=model_colors,\n",
    "            # lw=0.5,\n",
    "        )\n",
    "        ax.grid(axis=\"y\", alpha=0.5)\n",
    "\n",
    "        # points_plot = sns.stripplot(\n",
    "        #     x=\"model\",\n",
    "        #     y=\"value\",\n",
    "        #     hue=\"split\",\n",
    "        #     # hue_order=run_order,\n",
    "        #     jitter=0.17,\n",
    "        #     data=df,\n",
    "        #     ax=ax,\n",
    "        #     palette=run_colors,\n",
    "        #     # palette=plt.get_cmap('gist_rainbow'),\n",
    "        #     # color=\"black\",\n",
    "        #     edgecolor=\"white\",\n",
    "        #     size=2.8,\n",
    "        #     linewidth=0.7,\n",
    "        # )\n",
    "        # points_plot.get_legend().remove()\n",
    "\n",
    "        # Calculate mean performance score.\n",
    "        means = df.groupby([\"model\", \"split\"]).mean().groupby(\"model\").mean()\n",
    "        # Make sure the order follows seaborn's x-axis ordering.\n",
    "        model_order = list(map(lambda ax: ax.get_text(), axs[i].get_xticklabels()))\n",
    "        means = means.reindex(model_order)\n",
    "\n",
    "        #         lines = ax.hlines(\n",
    "        #             y=means.value,\n",
    "        #             xmin=np.arange(0, len(means)) - 0.5 + 0.05,\n",
    "        #             xmax=np.arange(1, len(means) + 1) - 0.5 - 0.05,\n",
    "        #             colors=model_colors,\n",
    "        #             lw=1.5,\n",
    "        #             zorder=1000,\n",
    "        #         )\n",
    "\n",
    "        #         outline_path_effects = [\n",
    "        #             mpl.patheffects.Stroke(linewidth=5, foreground=\"white\", alpha=0.9),\n",
    "        #             mpl.patheffects.Normal(),\n",
    "        #         ]\n",
    "        #         lines.set_path_effects(outline_path_effects)\n",
    "\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=25)\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        ax_format = ax.get_yaxis().get_major_formatter()\n",
    "\n",
    "        #         for m, c in zip(means.value, model_colors):\n",
    "\n",
    "        #             ax.annotate(\n",
    "        #                 f\"{m:.4g}\",\n",
    "        #                 xy=(ax.get_xlim()[0] + (ax.get_xlim()[0] * 0.4), m),\n",
    "        #                 xycoords=\"data\",\n",
    "        #                 color=c,\n",
    "        #                 ha=\"right\",\n",
    "        #                 va=\"center\",\n",
    "        #                 annotation_clip=False,\n",
    "        #                 fontweight=\"bold\",\n",
    "        #                 snap=True,\n",
    "        #                 bbox=dict(\n",
    "        #                     boxstyle=\"square,pad=0.3\", fc=\"white\", lw=0, snap=True, alpha=0.75\n",
    "        #                 ),\n",
    "        #             )\n",
    "        ax.set_title(f\"{metric_rename_map[l]} {perf_metrics_with_directions[l]}\")\n",
    "        ax.set_ylabel(\"\")\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.annotate(\n",
    "            subfig_section,\n",
    "            (-0.3, 1.2),\n",
    "            xycoords=\"axes fraction\",\n",
    "            fontweight=\"bold\",\n",
    "            fontstyle=\"oblique\",\n",
    "            fontsize=\"x-large\",\n",
    "            horizontalalignment=\"left\",\n",
    "            verticalalignment=\"top\",\n",
    "        )\n",
    "\n",
    "    for ax in axs[: (nrows - 1) * ncols]:\n",
    "        # sns.despine(ax=ax, left=True, bottom=True, top=True, right=True, trim=True)\n",
    "        # ax.set_yticks([])\n",
    "        # ax.set_xticks([])\n",
    "        # ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "plt.savefig(\"ablation_results_violin.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create violinplot\n",
    "curr_comp = [\n",
    "    \"CARN DTI\",\n",
    "    \"CARN LE\",\n",
    "    # \"CARN Fake Anat DTI\", \"CARN Fake Anat LE\",\n",
    "    \"CARN Anat DTI\",\n",
    "    \"CARN Anat LE\",\n",
    "]\n",
    "tr = test_results.loc[np.isin(test_results.model, curr_comp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_splits = len(test_results.split.unique())\n",
    "n_metrics = len(test_results.metric.unique())\n",
    "n_models = len(test_results.model.unique())\n",
    "\n",
    "with mpl.rc_context({\"font.size\": 10.0}):\n",
    "\n",
    "    ncols = 2\n",
    "    nrows = 2  # np.ceil(n_metrics / ncols).astype(int)\n",
    "    # figsize = (n_models * ncols, nrows * 3)\n",
    "    figsize = (len(curr_comp) * ncols, nrows * 3)\n",
    "\n",
    "    fig, axs = plt.subplots(\n",
    "        ncols=ncols,\n",
    "        nrows=nrows,\n",
    "        # sharex=True,\n",
    "        figsize=figsize,\n",
    "        dpi=160,\n",
    "        # gridspec_kw={\"wspace\": 1.0, \"hspace\": 1.0},\n",
    "    )\n",
    "    axs = axs.flatten()\n",
    "    sns.despine(fig=fig, top=True, right=True)\n",
    "\n",
    "    all_colors = sns.color_palette(\n",
    "        \"tab10\",\n",
    "        n_colors=n_splits + n_models,\n",
    "    )\n",
    "    model_colors = all_colors[:n_models]\n",
    "    run_colors = all_colors[n_models:]\n",
    "    # run_order = list(test_results.run_name.unique())\n",
    "\n",
    "    ax_count = 0\n",
    "    for i, l in enumerate(perf_metrics):\n",
    "\n",
    "        ax = axs[i]\n",
    "        df = tr.loc[tr.metric == l]\n",
    "\n",
    "        vplot = sns.violinplot(\n",
    "            x=\"model\",\n",
    "            y=\"value\",\n",
    "            data=df,\n",
    "            ax=ax,\n",
    "            scale=\"count\",\n",
    "            inner=None,\n",
    "            palette=model_colors,\n",
    "            # lw=0.5,\n",
    "        )\n",
    "        ax.grid(axis=\"y\", alpha=0.5)\n",
    "\n",
    "        points_plot = sns.stripplot(\n",
    "            x=\"model\",\n",
    "            y=\"value\",\n",
    "            hue=\"split\",\n",
    "            # hue_order=run_order,\n",
    "            jitter=0.17,\n",
    "            data=df,\n",
    "            ax=ax,\n",
    "            palette=run_colors,\n",
    "            # palette=plt.get_cmap('gist_rainbow'),\n",
    "            # color=\"black\",\n",
    "            edgecolor=\"white\",\n",
    "            size=2.8,\n",
    "            linewidth=0.7,\n",
    "        )\n",
    "        points_plot.get_legend().remove()\n",
    "\n",
    "        # Calculate mean performance score.\n",
    "        means = df.groupby([\"model\", \"split\"]).mean().groupby(\"model\").mean()\n",
    "        # Make sure the order follows seaborn's x-axis ordering.\n",
    "        model_order = list(map(lambda ax: ax.get_text(), axs[i].get_xticklabels()))\n",
    "        means = means.reindex(model_order)\n",
    "\n",
    "        lines = ax.hlines(\n",
    "            y=means.value,\n",
    "            xmin=np.arange(0, len(means)) - 0.5 + 0.05,\n",
    "            xmax=np.arange(1, len(means) + 1) - 0.5 - 0.05,\n",
    "            colors=model_colors,\n",
    "            lw=1.5,\n",
    "            zorder=1000,\n",
    "        )\n",
    "\n",
    "        outline_path_effects = [\n",
    "            mpl.patheffects.Stroke(linewidth=5, foreground=\"white\", alpha=0.9),\n",
    "            mpl.patheffects.Normal(),\n",
    "        ]\n",
    "        lines.set_path_effects(outline_path_effects)\n",
    "\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=25)\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        ax_format = ax.get_yaxis().get_major_formatter()\n",
    "\n",
    "        for m, c in zip(means.value, model_colors):\n",
    "\n",
    "            ax.annotate(\n",
    "                f\"{m:.4g}\",\n",
    "                xy=(ax.get_xlim()[0] + (ax.get_xlim()[0] * 0.4), m),\n",
    "                xycoords=\"data\",\n",
    "                color=c,\n",
    "                ha=\"right\",\n",
    "                va=\"center\",\n",
    "                annotation_clip=False,\n",
    "                fontweight=\"bold\",\n",
    "                snap=True,\n",
    "                bbox=dict(\n",
    "                    boxstyle=\"square,pad=0.3\", fc=\"white\", lw=0, snap=True, alpha=0.75\n",
    "                ),\n",
    "            )\n",
    "        ax.set_title(f\"{l.replace('_', ' ')} {perf_metrics_with_directions[l]}\")\n",
    "        ax.set_ylabel(\"\")\n",
    "        ax.set_xlabel(\"\")\n",
    "\n",
    "    for ax in axs[: (nrows - 1) * ncols]:\n",
    "        # sns.despine(ax=ax, left=True, bottom=True, top=True, right=True, trim=True)\n",
    "        # ax.set_yticks([])\n",
    "        # ax.set_xticks([])\n",
    "        # ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "# plt.suptitle(\"DIQT V-Fro DTI vs\")\n",
    "# plt.savefig(\"uvers_all_runs_violin.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_results[\n",
    "    \"2022-03-25T08_21_45__uvers_pitn_anat_stream_le_split_3\"\n",
    "].metrics.subj_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runs_to_comp = [\n",
    "    \"2022-03-27T08_29_27__uvers_pitn_single_stream_dti_split_3\",\n",
    "    \"2022-03-24T22_47_26__uvers_pitn_anat_stream_dti_split_3\",\n",
    "    \"2022-03-25T08_21_45__uvers_pitn_anat_stream_le_split_3\",\n",
    "]\n",
    "sub = \"397154\"\n",
    "vols_to_show = list()\n",
    "for r in runs_to_comp:\n",
    "    pred_dti = nib.load(run_results[r].pred_vols[sub]).get_fdata()\n",
    "    vols_to_show.append(pred_dti)\n",
    "\n",
    "fig = plt.figure(dpi=200, figsize=(8, 5))\n",
    "pitn.viz.plot_vol_slices(\n",
    "    *vols_to_show, slice_idx=(0.5, None, None), fig=fig, cmap=\"gray\"\n",
    ")  # , vol_labels=runs_to_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subj_gt[sub].dti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=200, figsize=(5, 5))\n",
    "pitn.viz.plot_vol_slices(\n",
    "    subj_gt[sub].lr_dti.numpy(),\n",
    "    subj_gt[sub].dti.numpy(),\n",
    "    slice_idx=(0.5, None, None),\n",
    "    fig=fig,\n",
    "    cmap=\"gray\",\n",
    ")  # , vol_labels=runs_to_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Comparison Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Comparison Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "select_subj_id = HOLDOUT_SUBJ_ID\n",
    "rot_k = 0\n",
    "\n",
    "row_labels = (\"Color FA\", r\"$D_{x, x}\", r\"D_{y, z}\")\n",
    "col_labels = (\n",
    "    \"Downsample\",\n",
    "    \"Cubic Spline\",\n",
    "    \"RevNet4\",\n",
    "    \"CARN DTI (Ours)\",\n",
    "    \"CARN LE (Ours)\",\n",
    "    \"Ground\\nTruth\",\n",
    ")\n",
    "\n",
    "runs_to_sample = (\n",
    "    \"2022-03-27T05_51_39__test_spline\",\n",
    "    \"2022-03-23T04_03_59__uvers_espcn_revnet_split_3\",\n",
    "    \"2022-03-24T22_47_26__uvers_pitn_anat_stream_dti_split_3\",\n",
    "    \"2022-03-25T08_21_45__uvers_pitn_anat_stream_le_split_3\",\n",
    ")\n",
    "\n",
    "full_vol_cols = list()\n",
    "# Low-res input col\n",
    "lr_col = [\n",
    "    np.rot90(\n",
    "        np.moveaxis(pitn.viz.direction_map(subj_gt[select_subj_id].lr_dti), 0, -1),\n",
    "        k=rot_k,\n",
    "    ),\n",
    "    np.rot90(subj_gt[select_subj_id].lr_dti[0].cpu().detach().numpy(), k=rot_k),\n",
    "    np.rot90(subj_gt[select_subj_id].lr_dti[4].cpu().detach().numpy(), k=rot_k),\n",
    "]\n",
    "\n",
    "# Zoom in with Nearest Neighbor interpolation to fit the size of other images.\n",
    "lr_col = [\n",
    "    scipy.ndimage.zoom(lr_col[0], (2, 2, 2, 1), order=0),\n",
    "    scipy.ndimage.zoom(lr_col[1], (2, 2, 2), order=0),\n",
    "    scipy.ndimage.zoom(lr_col[2], (2, 2, 2), order=0),\n",
    "]\n",
    "\n",
    "full_vol_cols.append(lr_col)\n",
    "\n",
    "for r in runs_to_sample:\n",
    "\n",
    "    pred_dti_f = run_results[r].pred_vols[select_subj_id]\n",
    "    pred_dti = nib.load(pred_dti_f).get_fdata()\n",
    "\n",
    "    if pred_dti.shape[0] != 6:\n",
    "        pred_dti = np.moveaxis(pred_dti, -1, 0)\n",
    "\n",
    "    if pred_dti.shape == (6, 136, 166, 136):\n",
    "        pred_dti = np.pad(pred_dti, ((0, 0), (4, 4), (4, 4), (4, 4)))\n",
    "    pred_dti = np.rot90(pred_dti, axes=(1, 2), k=rot_k)\n",
    "\n",
    "    color_fa = pitn.viz.direction_map(pred_dti)\n",
    "    color_fa = np.moveaxis(color_fa, 0, -1)\n",
    "    dxx = pred_dti[0]\n",
    "    dyz = pred_dti[4]\n",
    "    model_col = [color_fa, dxx, dyz]\n",
    "    full_vol_cols.append(model_col)\n",
    "\n",
    "hr_col = [\n",
    "    np.rot90(\n",
    "        np.moveaxis(pitn.viz.direction_map(subj_gt[select_subj_id].dti), 0, -1), k=rot_k\n",
    "    ),\n",
    "    np.rot90(subj_gt[select_subj_id].dti[0].cpu().detach().numpy(), k=rot_k),\n",
    "    np.rot90(subj_gt[select_subj_id].dti[4].cpu().detach().numpy(), k=rot_k),\n",
    "]\n",
    "full_vol_cols.append(hr_col)\n",
    "\n",
    "# for i in range(len(full_vol_cols)):\n",
    "#     for j in range(len(full_vol_cols[i])):\n",
    "#         v = full_vol_cols[i][j]\n",
    "#         # full_vol_cols[i, j] = np.rot90(np.rot90(v, k=1, axes=(1, 3)), k=2, axes=(2, 3))\n",
    "#         full_vol_cols[i][j] = np.rot90(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[v.shape for x in full_vol_cols for v in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zoom_out_grid_coords = (75, slice(20, 154), slice(20, 124))\n",
    "zoom_in_grid_coords = (75, slice(67, 107), slice(55, 95))\n",
    "\n",
    "with mpl.rc_context(\n",
    "    {\n",
    "        \"font.size\": 6.0,\n",
    "        \"axes.labelpad\": 10.0,\n",
    "        # \"figure.autolayout\": False,\n",
    "        # \"figure.constrained_layout.use\": True,\n",
    "    }\n",
    "):\n",
    "    fig = plt.figure(dpi=250, figsize=(4, 4))\n",
    "\n",
    "    grid = ImageGrid(\n",
    "        fig,\n",
    "        111,  # similar to subplot(111)\n",
    "        aspect=False,\n",
    "        nrows_ncols=(len(row_labels) * 2, len(col_labels)),\n",
    "        axes_pad=0.01,  # pad between axes in inch.\n",
    "    )\n",
    "\n",
    "    for i_c, ax_col_list in enumerate(grid.axes_column):\n",
    "        full_vol_col = full_vol_cols[i_c]\n",
    "        for i_ax, j_r in zip(range(0, len(ax_col_list), 2), range(len(ax_col_list))):\n",
    "            if i_ax >= 2:\n",
    "                cmap = \"gray\"\n",
    "            else:\n",
    "                cmap = None\n",
    "            full_vol_im = full_vol_col[j_r]\n",
    "\n",
    "            zoom_out_ax = ax_col_list[i_ax]\n",
    "\n",
    "            zoom_out_im = full_vol_im[zoom_out_grid_coords]\n",
    "            zoom_out_ax.imshow(zoom_out_im, cmap=cmap)\n",
    "            zoom_out_ax.axis(\"off\")\n",
    "\n",
    "            zoom_in_ax = ax_col_list[i_ax + 1]\n",
    "            zoom_in_im = full_vol_im[zoom_in_grid_coords]\n",
    "\n",
    "            if zoom_in_im.shape[-1] == 3:\n",
    "                zoom_in_im = scipy.ndimage.zoom(zoom_in_im, (2.5, 2.5, 1), order=0)\n",
    "            else:\n",
    "                zoom_in_im = scipy.ndimage.zoom(zoom_in_im, 2.5, order=0)\n",
    "            zoom_in_ax.imshow(zoom_in_im, cmap=cmap)\n",
    "            zoom_in_ax.axis(\"off\")\n",
    "plt.savefig(\"dense_qual_results.pdf\")\n",
    "plt.savefig(\"dense_qual_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compact Nested Comparison Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "select_subj_id = HOLDOUT_SUBJ_ID\n",
    "rot_k = 0\n",
    "\n",
    "row_labels = (\"Color FA\", r\"$D_{x, x}$\", r\"$D_{y, z}$\")\n",
    "col_labels = (\n",
    "    \"Downsample\",\n",
    "    \"Cubic Spline\",\n",
    "    \"RevNet4\",\n",
    "    \"CARN DTI\\n(Ours)\",\n",
    "    \"CARN LE\\n(Ours)\",\n",
    "    \"Ground Truth\",\n",
    ")\n",
    "\n",
    "runs_to_sample = (\n",
    "    \"2022-03-27T05_51_39__test_spline\",\n",
    "    \"2022-03-23T04_03_59__uvers_espcn_revnet_split_3\",\n",
    "    \"2022-03-24T22_47_26__uvers_pitn_anat_stream_dti_split_3\",\n",
    "    \"2022-03-25T08_21_45__uvers_pitn_anat_stream_le_split_3\",\n",
    ")\n",
    "\n",
    "full_vol_cols = list()\n",
    "# Low-res input col\n",
    "lr_col = [\n",
    "    np.rot90(\n",
    "        np.moveaxis(pitn.viz.direction_map(subj_gt[select_subj_id].lr_dti), 0, -1),\n",
    "        k=rot_k,\n",
    "    ),\n",
    "    np.rot90(subj_gt[select_subj_id].lr_dti[0].cpu().detach().numpy(), k=rot_k),\n",
    "    np.rot90(subj_gt[select_subj_id].lr_dti[4].cpu().detach().numpy(), k=rot_k),\n",
    "]\n",
    "\n",
    "# Zoom in with Nearest Neighbor interpolation to fit the size of other images.\n",
    "lr_col = [\n",
    "    scipy.ndimage.zoom(lr_col[0], (2, 2, 2, 1), order=0),\n",
    "    scipy.ndimage.zoom(lr_col[1], (2, 2, 2), order=0),\n",
    "    scipy.ndimage.zoom(lr_col[2], (2, 2, 2), order=0),\n",
    "]\n",
    "\n",
    "full_vol_cols.append(lr_col)\n",
    "\n",
    "for r in runs_to_sample:\n",
    "\n",
    "    pred_dti_f = run_results[r].pred_vols[select_subj_id]\n",
    "    pred_dti = nib.load(pred_dti_f).get_fdata()\n",
    "\n",
    "    if pred_dti.shape[0] != 6:\n",
    "        pred_dti = np.moveaxis(pred_dti, -1, 0)\n",
    "\n",
    "    if pred_dti.shape == (6, 136, 166, 136):\n",
    "        pred_dti = np.pad(pred_dti, ((0, 0), (4, 4), (4, 4), (4, 4)))\n",
    "    pred_dti = np.rot90(pred_dti, axes=(1, 2), k=rot_k)\n",
    "\n",
    "    color_fa = pitn.viz.direction_map(pred_dti)\n",
    "    color_fa = np.moveaxis(color_fa, 0, -1)\n",
    "    dxx = pred_dti[0]\n",
    "    dyz = pred_dti[4]\n",
    "    model_col = [color_fa, dxx, dyz]\n",
    "    full_vol_cols.append(model_col)\n",
    "    print(\"Grabbed run \", r)\n",
    "\n",
    "hr_col = [\n",
    "    np.rot90(\n",
    "        np.moveaxis(pitn.viz.direction_map(subj_gt[select_subj_id].dti), 0, -1), k=rot_k\n",
    "    ),\n",
    "    np.rot90(subj_gt[select_subj_id].dti[0].cpu().detach().numpy(), k=rot_k),\n",
    "    np.rot90(subj_gt[select_subj_id].dti[4].cpu().detach().numpy(), k=rot_k),\n",
    "]\n",
    "full_vol_cols.append(hr_col)\n",
    "\n",
    "# for i in range(len(full_vol_cols)):\n",
    "#     for j in range(len(full_vol_cols[i])):\n",
    "#         v = full_vol_cols[i][j]\n",
    "#         # full_vol_cols[i, j] = np.rot90(np.rot90(v, k=1, axes=(1, 3)), k=2, axes=(2, 3))\n",
    "#         full_vol_cols[i][j] = np.rot90(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[v.shape for x in full_vol_cols for v in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "height_idx = 81\n",
    "zoom_out_grid_coords = (height_idx, slice(20, 154), slice(20, 124))\n",
    "zoom_in_grid_coords = (height_idx, slice(27, 67), slice(45, 85))\n",
    "\n",
    "embed_im_zoom_factor = 2.0\n",
    "\n",
    "row_vranges = list(map(lambda x: list((np.inf, -np.inf)), range(len(row_labels))))\n",
    "# Normalize images within-modality.\n",
    "for i_row in range(len(row_labels)):\n",
    "    for j_col in range(len(col_labels)):\n",
    "        full_vol_im = full_vol_cols[j_col][i_row]\n",
    "        zoom_out_im = full_vol_im[zoom_out_grid_coords]\n",
    "\n",
    "        row_vranges[i_row][0] = min(row_vranges[i_row][0], zoom_out_im.min())\n",
    "        row_vranges[i_row][1] = max(row_vranges[i_row][1], zoom_out_im.max())\n",
    "\n",
    "\n",
    "with mpl.rc_context(\n",
    "    {\n",
    "        \"font.size\": 8.0,\n",
    "        # \"axes.labelpad\": 5.0,\n",
    "        # \"figure.autolayout\": False,\n",
    "        \"figure.constrained_layout.use\": True,\n",
    "    }\n",
    "):\n",
    "    fig = plt.figure(\n",
    "        dpi=300,\n",
    "        figsize=(FIG_WIDTH_INCHES, 3.5)\n",
    "    )\n",
    "\n",
    "    grid = mpl.gridspec.GridSpec(\n",
    "        nrows=len(row_labels),\n",
    "        ncols=len(col_labels),\n",
    "        figure=fig,\n",
    "        # left=0.05,\n",
    "        # right=0.95,\n",
    "        # top=0.95,\n",
    "        # bottom=0.05,\n",
    "        wspace=0.01,\n",
    "        hspace=0.01,\n",
    "    )\n",
    "\n",
    "    for i_row in range(grid.nrows):\n",
    "        for j_col in range(grid.ncols):\n",
    "            ax = fig.add_subplot(grid[i_row, j_col])\n",
    "            if i_row >= 1:\n",
    "                cmap = \"gray\"\n",
    "            else:\n",
    "                cmap = None\n",
    "            # Handle labels & ticks\n",
    "            # Set row & column labels\n",
    "            if ax.get_subplotspec().is_first_col():\n",
    "                ax.set_ylabel(row_labels[i_row])\n",
    "            if ax.get_subplotspec().is_first_row():\n",
    "                ax.set_xlabel(col_labels[j_col])\n",
    "                ax.xaxis.set_label_position(\"top\")\n",
    "            # Empty all ticks and ticklabels.\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_aspect(\"equal\")\n",
    "\n",
    "            # Grab actual volume to display.\n",
    "            full_vol_im = full_vol_cols[j_col][i_row]\n",
    "\n",
    "            # Plot base zoomed-out image.\n",
    "            zoom_out_im = full_vol_im[zoom_out_grid_coords]\n",
    "            im_plot = ax.imshow(\n",
    "                zoom_out_im,\n",
    "                cmap=cmap,\n",
    "                interpolation=\"antialiased\",\n",
    "                origin=\"upper\",\n",
    "                vmin=row_vranges[i_row][0],\n",
    "                vmax=row_vranges[i_row][1],\n",
    "            )\n",
    "\n",
    "            # Plot zoomed-in ROI\n",
    "            zoom_in_im = full_vol_im[zoom_in_grid_coords]\n",
    "            # Zoom in with nearest neighbor interpolation.\n",
    "            if zoom_in_im.shape[-1] == 3:\n",
    "                zoom_in_im = scipy.ndimage.zoom(\n",
    "                    zoom_in_im, (embed_im_zoom_factor, embed_im_zoom_factor, 1), order=0\n",
    "                )\n",
    "            else:\n",
    "                zoom_in_im = scipy.ndimage.zoom(\n",
    "                    zoom_in_im, embed_im_zoom_factor, order=0\n",
    "                )\n",
    "\n",
    "            # Set up zoom-in as an inset axis.\n",
    "            embed_x0 = zoom_out_im.shape[1] - zoom_in_im.shape[1] - 0.5\n",
    "            embed_y0 = zoom_out_im.shape[0] - zoom_in_im.shape[0] - 0.5\n",
    "            # Shift origin point by a fraction of the zoom-in size\n",
    "            embed_x0 += zoom_in_im.shape[1] * 0.4\n",
    "            embed_y0 += zoom_in_im.shape[0] * 0.4\n",
    "\n",
    "            inset_ax = ax.inset_axes(\n",
    "                bounds=[embed_x0, embed_y0, zoom_in_im.shape[1], zoom_in_im.shape[0]],\n",
    "                transform=ax.transData,\n",
    "            )\n",
    "\n",
    "            inset_ax.imshow(\n",
    "                zoom_in_im,\n",
    "                cmap=cmap,\n",
    "                origin=\"upper\",\n",
    "                interpolation=\"antialiased\",\n",
    "                vmin=row_vranges[i_row][0],\n",
    "                vmax=row_vranges[i_row][1],\n",
    "            )\n",
    "\n",
    "            inset_ax.set_xticks([])\n",
    "            inset_ax.set_yticks([])\n",
    "            inset_ax.set_xticklabels([])\n",
    "            inset_ax.set_yticklabels([])\n",
    "            inset_ax.set_aspect(\"equal\")\n",
    "            # Change frame around inset image to yellow.\n",
    "            plt.setp(inset_ax.spines.values(), color=\"yellow\")\n",
    "\n",
    "            # Outline zoomed ROI with yellow on the zoomed-out image.\n",
    "            rect_orig = (\n",
    "                zoom_in_grid_coords[2].start - zoom_out_grid_coords[2].start - 0.5,\n",
    "                zoom_in_grid_coords[1].start - zoom_out_grid_coords[1].start - 0.5,\n",
    "            )\n",
    "            rect_size = (\n",
    "                zoom_in_grid_coords[2].stop - zoom_in_grid_coords[2].start,\n",
    "                zoom_in_grid_coords[1].stop - zoom_in_grid_coords[1].start,\n",
    "            )\n",
    "            rect = mpl.patches.Rectangle(\n",
    "                xy=rect_orig,\n",
    "                width=rect_size[0],\n",
    "                height=rect_size[1],\n",
    "                edgecolor=\"yellow\",\n",
    "                lw=0.75,\n",
    "                fill=False,\n",
    "                transform=ax.transData,\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "plt.savefig(\"qual_results.pdf\")\n",
    "plt.savefig(\"qual_results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "02b6fbf192f5f02a9e4be2e8493e981ac4440e63c59c4307b81de14e51e6a1b0"
  },
  "kernelspec": {
   "display_name": "Python [conda env:miniconda-pitn]",
   "language": "python",
   "name": "conda-env-miniconda-pitn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
